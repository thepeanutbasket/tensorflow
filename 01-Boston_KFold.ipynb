{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보스턴 주택 가격 예측하기\n",
    "\n",
    "- 회귀(regression): 연속적인 값의 예측\n",
    "- 보스턴 주택 가격 데이터셋 -> 1970년대 보스턴 지역의 범죄율, 토지 지역의 비율, 방의 개수 등 총 14개의 변수가 포함됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 참조 및 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.boston_housing import load_data\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KFold는 하나의 데이터셋을 내가 원하는 등분으로 나누어 학습데이터, 검증데이터를 계속해서 바꿔가며 분석을 진행하는 방법이다.\n",
    "- 다시 말해, 1-5까지 5등분을 했으면, 첫 번째에는 1-4까지가 학습데이터, 5가 검증데이터, 두 번째에는 1-3 + 5가 학습데이터, 4가 검증데이터 등등이 된다.\n",
    "> KFold가 필요한 이유? 만약 보스턴 집값 데이터가 집값이 가장 저렴한 집 ~ 가장 비싼 집 순으로 입력되어 있다면 그 데이터셋 그대로 7:3으로 나누었을 때 정확한 분석 결과를 도출하기 어렵기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운받기\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path = 'boston_housing.npz', \n",
    "                                                test_split = 0.33, seed = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 크기 확인하기\n",
    "# 학습데이터와 학습 label의 크기\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 13) (167,)\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터와 검증 label의 크기\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>339 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0    22.5\n",
       "1     8.3\n",
       "2    17.2\n",
       "3    25.0\n",
       "4    28.5\n",
       "..    ...\n",
       "334  21.9\n",
       "335  21.8\n",
       "336  34.9\n",
       "337  25.0\n",
       "338  34.7\n",
       "\n",
       "[339 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주택 가격의 중간 가격($1,000 단위)를 의미함.\n",
    "DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터 확인\n",
    "- 총 13개의 특성을 갖는 데이터셋\n",
    "- 각 특성마다 데이터의 범위(스케일)가 다르다.\n",
    "    - 범죄율 : 0 ~ 1\n",
    "    - 방의 개수 : 3 ~ 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20608</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>5.593</td>\n",
       "      <td>76.5</td>\n",
       "      <td>7.9549</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>372.49</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01951</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>7.104</td>\n",
       "      <td>59.5</td>\n",
       "      <td>9.2229</td>\n",
       "      <td>3.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>393.24</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.67822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>5.362</td>\n",
       "      <td>96.2</td>\n",
       "      <td>2.1036</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>380.79</td>\n",
       "      <td>10.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04294</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>6.249</td>\n",
       "      <td>77.3</td>\n",
       "      <td>3.6150</td>\n",
       "      <td>4.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.07151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>6.121</td>\n",
       "      <td>56.8</td>\n",
       "      <td>3.7476</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>395.15</td>\n",
       "      <td>8.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>12.04820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>5.648</td>\n",
       "      <td>87.6</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>291.55</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.10328</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4530</td>\n",
       "      <td>5.927</td>\n",
       "      <td>47.2</td>\n",
       "      <td>6.9320</td>\n",
       "      <td>8.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>6.982</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5.4917</td>\n",
       "      <td>7.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>390.43</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.21161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>6.137</td>\n",
       "      <td>87.4</td>\n",
       "      <td>2.7147</td>\n",
       "      <td>5.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>394.47</td>\n",
       "      <td>13.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1      2    3       4      5     6       7     8      9   \\\n",
       "0     0.20608  22.0   5.86  0.0  0.4310  5.593  76.5  7.9549   7.0  330.0   \n",
       "1     0.01951  17.5   1.38  0.0  0.4161  7.104  59.5  9.2229   3.0  216.0   \n",
       "2     3.67822   0.0  18.10  0.0  0.7700  5.362  96.2  2.1036  24.0  666.0   \n",
       "3     0.63796   0.0   8.14  0.0  0.5380  6.096  84.5  4.4619   4.0  307.0   \n",
       "4     0.04294  28.0  15.04  0.0  0.4640  6.249  77.3  3.6150   4.0  270.0   \n",
       "..        ...   ...    ...  ...     ...    ...   ...     ...   ...    ...   \n",
       "162   0.07151   0.0   4.49  0.0  0.4490  6.121  56.8  3.7476   3.0  247.0   \n",
       "163  12.04820   0.0  18.10  0.0  0.6140  5.648  87.6  1.9512  24.0  666.0   \n",
       "164   0.10328  25.0   5.13  0.0  0.4530  5.927  47.2  6.9320   8.0  284.0   \n",
       "165   0.10000  34.0   6.09  0.0  0.4330  6.982  17.7  5.4917   7.0  329.0   \n",
       "166   0.21161   0.0   8.56  0.0  0.5200  6.137  87.4  2.7147   5.0  384.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    19.1  372.49  12.50  \n",
       "1    18.6  393.24   8.05  \n",
       "2    20.2  380.79  10.19  \n",
       "3    21.0  380.02  10.26  \n",
       "4    18.2  396.90  10.59  \n",
       "..    ...     ...    ...  \n",
       "162  18.5  395.15   8.44  \n",
       "163  20.2  291.55  14.10  \n",
       "164  19.7  396.90   9.22  \n",
       "165  16.1  390.43   4.86  \n",
       "166  20.9  394.47  13.44  \n",
       "\n",
       "[167 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 표준화 수행\n",
    "- 스케일이 서로 다를 경우 신경망의 성능에 큰 영향을 주기 때문에 표준화를 수행해야 한다.\n",
    "- 표준화 -> 각 값에 대해 특성(DF의 컬럼)의 평균을 때고 표준편차로 나누는 처리\n",
    "- 표준화는 특성의 평균을 0으로, 표준편차를 1로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 구하기\n",
    "mean = np.mean(x_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준편차 구하기\n",
    "std = np.std(x_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 표준화\n",
    "std_x_train = (x_train - mean) / std\n",
    "\n",
    "# 검증 데이터 표준화\n",
    "std_x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.398157</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.114676</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.572882</td>\n",
       "      <td>-0.714780</td>\n",
       "      <td>0.135436</td>\n",
       "      <td>0.258581</td>\n",
       "      <td>-0.637411</td>\n",
       "      <td>-0.789973</td>\n",
       "      <td>0.107780</td>\n",
       "      <td>0.384607</td>\n",
       "      <td>0.703446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.352052</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>0.994262</td>\n",
       "      <td>-0.554323</td>\n",
       "      <td>0.939305</td>\n",
       "      <td>-0.871380</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>-3.607039</td>\n",
       "      <td>1.563816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.403808</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>0.317915</td>\n",
       "      <td>-0.950494</td>\n",
       "      <td>1.027837</td>\n",
       "      <td>-1.081587</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>-0.397411</td>\n",
       "      <td>1.837015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.412229</td>\n",
       "      <td>0.556941</td>\n",
       "      <td>-0.903942</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.869815</td>\n",
       "      <td>0.675368</td>\n",
       "      <td>-0.902158</td>\n",
       "      <td>1.934138</td>\n",
       "      <td>-0.181541</td>\n",
       "      <td>-0.748424</td>\n",
       "      <td>0.607672</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>-0.460025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.422487</td>\n",
       "      <td>2.866843</td>\n",
       "      <td>-0.929962</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.216236</td>\n",
       "      <td>0.815945</td>\n",
       "      <td>-1.451056</td>\n",
       "      <td>0.610605</td>\n",
       "      <td>-0.637411</td>\n",
       "      <td>-0.979912</td>\n",
       "      <td>0.380448</td>\n",
       "      <td>0.462715</td>\n",
       "      <td>-1.298649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>-0.421010</td>\n",
       "      <td>2.866843</td>\n",
       "      <td>-1.119328</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.372951</td>\n",
       "      <td>-0.253290</td>\n",
       "      <td>-1.305864</td>\n",
       "      <td>2.506859</td>\n",
       "      <td>-0.979314</td>\n",
       "      <td>-0.564422</td>\n",
       "      <td>-0.892003</td>\n",
       "      <td>0.420785</td>\n",
       "      <td>-0.858270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>-0.110380</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.218213</td>\n",
       "      <td>-0.744599</td>\n",
       "      <td>-1.011938</td>\n",
       "      <td>0.140008</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>0.421099</td>\n",
       "      <td>-0.334980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-0.422647</td>\n",
       "      <td>2.656852</td>\n",
       "      <td>-1.219071</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.076018</td>\n",
       "      <td>1.047399</td>\n",
       "      <td>-1.879550</td>\n",
       "      <td>0.742025</td>\n",
       "      <td>-0.751379</td>\n",
       "      <td>-0.938363</td>\n",
       "      <td>-0.028554</td>\n",
       "      <td>0.449331</td>\n",
       "      <td>-1.482140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.216240</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.218213</td>\n",
       "      <td>1.099938</td>\n",
       "      <td>0.287710</td>\n",
       "      <td>-0.177775</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>0.445776</td>\n",
       "      <td>-0.798465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>-0.423353</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.623507</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.737845</td>\n",
       "      <td>1.276014</td>\n",
       "      <td>-0.275352</td>\n",
       "      <td>0.541476</td>\n",
       "      <td>-0.865346</td>\n",
       "      <td>-0.997718</td>\n",
       "      <td>-0.255777</td>\n",
       "      <td>0.420158</td>\n",
       "      <td>-1.203505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>339 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.398157 -0.493015 -0.114676 -0.311588 -0.572882 -0.714780  0.135436   \n",
       "1    1.352052 -0.493015  0.970928 -0.311588  0.994262 -0.554323  0.939305   \n",
       "2    0.403808 -0.493015  0.970928 -0.311588  0.317915 -0.950494  1.027837   \n",
       "3   -0.412229  0.556941 -0.903942 -0.311588 -0.869815  0.675368 -0.902158   \n",
       "4   -0.422487  2.866843 -0.929962 -0.311588 -1.216236  0.815945 -1.451056   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "334 -0.421010  2.866843 -1.119328 -0.311588 -1.372951 -0.253290 -1.305864   \n",
       "335 -0.110380 -0.493015  0.970928 -0.311588 -0.218213 -0.744599 -1.011938   \n",
       "336 -0.422647  2.656852 -1.219071 -0.311588 -1.076018  1.047399 -1.879550   \n",
       "337  0.216240 -0.493015  0.970928 -0.311588 -0.218213  1.099938  0.287710   \n",
       "338 -0.423353 -0.493015 -0.623507 -0.311588 -0.737845  1.276014 -0.275352   \n",
       "\n",
       "           7         8         9         10        11        12  \n",
       "0    0.258581 -0.637411 -0.789973  0.107780  0.384607  0.703446  \n",
       "1   -0.871380  1.641939  1.518963  0.834895 -3.607039  1.563816  \n",
       "2   -1.081587  1.641939  1.518963  0.834895 -0.397411  1.837015  \n",
       "3    1.934138 -0.181541 -0.748424  0.607672  0.448912 -0.460025  \n",
       "4    0.610605 -0.637411 -0.979912  0.380448  0.462715 -1.298649  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "334  2.506859 -0.979314 -0.564422 -0.892003  0.420785 -0.858270  \n",
       "335  0.140008  1.641939  1.518963  0.834895  0.421099 -0.334980  \n",
       "336  0.742025 -0.751379 -0.938363 -0.028554  0.449331 -1.482140  \n",
       "337 -0.177775  1.641939  1.518963  0.834895  0.445776 -0.798465  \n",
       "338  0.541476 -0.865346 -0.997718 -0.255777  0.420158 -1.203505  \n",
       "\n",
       "[339 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습데이터 표준화 확인\n",
    "DataFrame(std_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.403305</td>\n",
       "      <td>0.430946</td>\n",
       "      <td>-0.798418</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.051274</td>\n",
       "      <td>-0.984573</td>\n",
       "      <td>0.270004</td>\n",
       "      <td>1.922124</td>\n",
       "      <td>-0.295509</td>\n",
       "      <td>-0.475388</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.207479</td>\n",
       "      <td>-0.052267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.424226</td>\n",
       "      <td>0.241954</td>\n",
       "      <td>-1.446021</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.174171</td>\n",
       "      <td>1.160997</td>\n",
       "      <td>-0.332013</td>\n",
       "      <td>2.508060</td>\n",
       "      <td>-0.751379</td>\n",
       "      <td>-1.152043</td>\n",
       "      <td>0.107780</td>\n",
       "      <td>0.424445</td>\n",
       "      <td>-0.657109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013963</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>1.744841</td>\n",
       "      <td>-1.312586</td>\n",
       "      <td>0.967635</td>\n",
       "      <td>-0.781733</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>0.294266</td>\n",
       "      <td>-0.366241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.354877</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.468834</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.168724</td>\n",
       "      <td>-0.270330</td>\n",
       "      <td>0.553306</td>\n",
       "      <td>0.308026</td>\n",
       "      <td>-0.637411</td>\n",
       "      <td>-0.611906</td>\n",
       "      <td>1.198453</td>\n",
       "      <td>0.286214</td>\n",
       "      <td>-0.356727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.421599</td>\n",
       "      <td>0.682936</td>\n",
       "      <td>0.528591</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.779085</td>\n",
       "      <td>-0.053075</td>\n",
       "      <td>0.298334</td>\n",
       "      <td>-0.083323</td>\n",
       "      <td>-0.637411</td>\n",
       "      <td>-0.831522</td>\n",
       "      <td>-0.073998</td>\n",
       "      <td>0.462715</td>\n",
       "      <td>-0.311873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-0.418395</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.996457</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.902807</td>\n",
       "      <td>-0.234831</td>\n",
       "      <td>-0.427627</td>\n",
       "      <td>-0.022049</td>\n",
       "      <td>-0.751379</td>\n",
       "      <td>-0.968040</td>\n",
       "      <td>0.062336</td>\n",
       "      <td>0.444416</td>\n",
       "      <td>-0.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.924589</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>-0.906475</td>\n",
       "      <td>0.663086</td>\n",
       "      <td>-0.852157</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>-0.638844</td>\n",
       "      <td>0.165204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.414832</td>\n",
       "      <td>0.556941</td>\n",
       "      <td>-0.903942</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.869815</td>\n",
       "      <td>-0.510304</td>\n",
       "      <td>-0.767590</td>\n",
       "      <td>1.449447</td>\n",
       "      <td>-0.181541</td>\n",
       "      <td>-0.748424</td>\n",
       "      <td>0.607672</td>\n",
       "      <td>0.462715</td>\n",
       "      <td>-0.498083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-0.415200</td>\n",
       "      <td>0.934925</td>\n",
       "      <td>-0.765170</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.034777</td>\n",
       "      <td>0.987761</td>\n",
       "      <td>-1.812266</td>\n",
       "      <td>0.783891</td>\n",
       "      <td>-0.295509</td>\n",
       "      <td>-0.481324</td>\n",
       "      <td>-1.028337</td>\n",
       "      <td>0.395063</td>\n",
       "      <td>-1.090692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-0.402685</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.408121</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.317190</td>\n",
       "      <td>-0.212111</td>\n",
       "      <td>0.656003</td>\n",
       "      <td>-0.499347</td>\n",
       "      <td>-0.523444</td>\n",
       "      <td>-0.154868</td>\n",
       "      <td>1.153008</td>\n",
       "      <td>0.437306</td>\n",
       "      <td>0.075497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.403305  0.430946 -0.798418 -0.311588 -1.051274 -0.984573  0.270004   \n",
       "1   -0.424226  0.241954 -1.446021 -0.311588 -1.174171  1.160997 -0.332013   \n",
       "2   -0.013963 -0.493015  0.970928 -0.311588  1.744841 -1.312586  0.967635   \n",
       "3   -0.354877 -0.493015 -0.468834 -0.311588 -0.168724 -0.270330  0.553306   \n",
       "4   -0.421599  0.682936  0.528591 -0.311588 -0.779085 -0.053075  0.298334   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "162 -0.418395 -0.493015 -0.996457 -0.311588 -0.902807 -0.234831 -0.427627   \n",
       "163  0.924589 -0.493015  0.970928 -0.311588  0.458134 -0.906475  0.663086   \n",
       "164 -0.414832  0.556941 -0.903942 -0.311588 -0.869815 -0.510304 -0.767590   \n",
       "165 -0.415200  0.934925 -0.765170 -0.311588 -1.034777  0.987761 -1.812266   \n",
       "166 -0.402685 -0.493015 -0.408121 -0.311588 -0.317190 -0.212111  0.656003   \n",
       "\n",
       "           7         8         9         10        11        12  \n",
       "0    1.922124 -0.295509 -0.475388  0.335004  0.207479 -0.052267  \n",
       "1    2.508060 -0.751379 -1.152043  0.107780  0.424445 -0.657109  \n",
       "2   -0.781733  1.641939  1.518963  0.834895  0.294266 -0.366241  \n",
       "3    0.308026 -0.637411 -0.611906  1.198453  0.286214 -0.356727  \n",
       "4   -0.083323 -0.637411 -0.831522 -0.073998  0.462715 -0.311873  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "162 -0.022049 -0.751379 -0.968040  0.062336  0.444416 -0.604100  \n",
       "163 -0.852157  1.641939  1.518963  0.834895 -0.638844  0.165204  \n",
       "164  1.449447 -0.181541 -0.748424  0.607672  0.462715 -0.498083  \n",
       "165  0.783891 -0.295509 -0.481324 -1.028337  0.395063 -1.090692  \n",
       "166 -0.499347 -0.523444 -0.154868  1.153008  0.437306  0.075497  \n",
       "\n",
       "[167 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 데이터 표준화 확인\n",
    "DataFrame(std_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할할 폴드 수\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 데이터셋을 k만큼 등분\n",
    "# 여기서는 3이므로 훈련 데이터셋(404개)를 3등분하여 \n",
    "# 1개는 검증셋으로, 나머지는 훈련셋으로 활용\n",
    "kfold = KFold(n_splits = k, random_state = 777)\n",
    "\n",
    "# 흠.. 워닝에 shuffle을 True로 해야 random_state가 의미있다고 하는데.. 그럼 굳이 k-fold를 할 필요가 있나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련셋 인덱스: [113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n",
      " 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
      " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
      " 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328\n",
      " 329 330 331 332 333 334 335 336 337 338]\n",
      "검증셋 인덱스: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112]\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 594.4891 - mae: 22.4692 - val_loss: 556.5499 - val_mae: 21.7903\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 565.1404 - mae: 21.8442 - val_loss: 533.8645 - val_mae: 21.2655\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 539.6794 - mae: 21.2664 - val_loss: 511.3365 - val_mae: 20.7410\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 513.5942 - mae: 20.6706 - val_loss: 486.7711 - val_mae: 20.1546\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 484.4302 - mae: 19.9939 - val_loss: 458.6310 - val_mae: 19.4559\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 450.8713 - mae: 19.1721 - val_loss: 425.2877 - val_mae: 18.6068\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 411.0697 - mae: 18.1808 - val_loss: 386.9314 - val_mae: 17.6077\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 366.0854 - mae: 17.0079 - val_loss: 344.3706 - val_mae: 16.4161\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 318.1530 - mae: 15.6501 - val_loss: 298.4217 - val_mae: 15.0306\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 265.7101 - mae: 14.0287 - val_loss: 249.9018 - val_mae: 13.4664\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 217.2987 - mae: 12.3176 - val_loss: 203.2323 - val_mae: 11.8641\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 171.3445 - mae: 10.5808 - val_loss: 163.2685 - val_mae: 10.4437\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 133.7822 - mae: 9.1705 - val_loss: 131.7133 - val_mae: 9.2218\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 107.4946 - mae: 8.2000 - val_loss: 108.0192 - val_mae: 8.3129\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 91.4092 - mae: 7.5234 - val_loss: 90.7397 - val_mae: 7.5716\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 81.3490 - mae: 7.0899 - val_loss: 80.3813 - val_mae: 7.0353\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 73.9860 - mae: 6.7577 - val_loss: 71.4638 - val_mae: 6.5576\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 66.1046 - mae: 6.3417 - val_loss: 64.1143 - val_mae: 6.1452\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 59.3488 - mae: 5.9596 - val_loss: 58.0362 - val_mae: 5.7913\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 53.6322 - mae: 5.6348 - val_loss: 52.8507 - val_mae: 5.4886\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 48.6686 - mae: 5.3415 - val_loss: 48.5507 - val_mae: 5.2108\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 44.4997 - mae: 5.0691 - val_loss: 45.6133 - val_mae: 5.0134\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 40.6843 - mae: 4.8069 - val_loss: 41.4538 - val_mae: 4.7154\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 37.0844 - mae: 4.5530 - val_loss: 37.6775 - val_mae: 4.4553\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 34.0255 - mae: 4.3346 - val_loss: 34.9508 - val_mae: 4.2662\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 31.5855 - mae: 4.1341 - val_loss: 32.5269 - val_mae: 4.0888\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 29.5929 - mae: 3.9663 - val_loss: 30.7323 - val_mae: 3.9775\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 27.8429 - mae: 3.8061 - val_loss: 28.7563 - val_mae: 3.8671\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 26.2093 - mae: 3.6678 - val_loss: 27.5301 - val_mae: 3.7913\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 25.1542 - mae: 3.5529 - val_loss: 26.8450 - val_mae: 3.7218\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 24.0911 - mae: 3.4402 - val_loss: 26.3520 - val_mae: 3.6808\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 23.3216 - mae: 3.3556 - val_loss: 25.8028 - val_mae: 3.6371\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.6331 - mae: 3.2902 - val_loss: 25.4845 - val_mae: 3.6166\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.0947 - mae: 3.2343 - val_loss: 25.2457 - val_mae: 3.6015\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.5411 - mae: 3.1818 - val_loss: 24.9970 - val_mae: 3.5775\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.0512 - mae: 3.1318 - val_loss: 24.4705 - val_mae: 3.5530\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.6086 - mae: 3.1164 - val_loss: 24.0430 - val_mae: 3.5440\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.2172 - mae: 3.0833 - val_loss: 23.8578 - val_mae: 3.5257\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19.8442 - mae: 3.0408 - val_loss: 23.6452 - val_mae: 3.5081\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.5456 - mae: 3.0054 - val_loss: 23.4704 - val_mae: 3.4802\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 19.2700 - mae: 2.9684 - val_loss: 23.3695 - val_mae: 3.4592\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.8581 - mae: 2.9276 - val_loss: 23.0423 - val_mae: 3.4489\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.6861 - mae: 2.9303 - val_loss: 22.7216 - val_mae: 3.4480\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.2944 - mae: 2.9035 - val_loss: 22.6296 - val_mae: 3.4332\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 18.0643 - mae: 2.8851 - val_loss: 22.3690 - val_mae: 3.4282\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.8597 - mae: 2.8618 - val_loss: 22.2142 - val_mae: 3.4079\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.6006 - mae: 2.8370 - val_loss: 21.9023 - val_mae: 3.3980\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.8053 - mae: 2.8962 - val_loss: 21.6506 - val_mae: 3.4328\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2870 - mae: 2.8815 - val_loss: 21.4804 - val_mae: 3.4107\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 17.0527 - mae: 2.8437 - val_loss: 21.4021 - val_mae: 3.3727\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.7169 - mae: 2.8115 - val_loss: 21.3355 - val_mae: 3.3560\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.5584 - mae: 2.7810 - val_loss: 21.2976 - val_mae: 3.3347\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.3096 - mae: 2.7398 - val_loss: 21.1045 - val_mae: 3.3009\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.1528 - mae: 2.7094 - val_loss: 21.0241 - val_mae: 3.2732\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.9121 - mae: 2.6800 - val_loss: 20.7747 - val_mae: 3.2605\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.6857 - mae: 2.6670 - val_loss: 20.5520 - val_mae: 3.2448\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.5854 - mae: 2.6495 - val_loss: 20.4353 - val_mae: 3.2303\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.6016 - mae: 2.6439 - val_loss: 20.2129 - val_mae: 3.2033\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.3994 - mae: 2.6320 - val_loss: 20.1239 - val_mae: 3.1879\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 15.2089 - mae: 2.6314 - val_loss: 19.8475 - val_mae: 3.1866\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.8795 - mae: 2.6252 - val_loss: 19.6690 - val_mae: 3.1767\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.6930 - mae: 2.6070 - val_loss: 19.5034 - val_mae: 3.1613\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.5606 - mae: 2.5910 - val_loss: 19.4046 - val_mae: 3.1508\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.3996 - mae: 2.5765 - val_loss: 19.2982 - val_mae: 3.1436\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.3048 - mae: 2.5780 - val_loss: 19.1641 - val_mae: 3.1365\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.1589 - mae: 2.5685 - val_loss: 19.1047 - val_mae: 3.1253\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.0585 - mae: 2.5442 - val_loss: 19.1031 - val_mae: 3.1168\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.9534 - mae: 2.5233 - val_loss: 19.0049 - val_mae: 3.1128\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.9550 - mae: 2.5530 - val_loss: 18.9387 - val_mae: 3.1660\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.8254 - mae: 2.5688 - val_loss: 18.9161 - val_mae: 3.1603\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.6104 - mae: 2.5344 - val_loss: 18.8764 - val_mae: 3.1445\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.4432 - mae: 2.5097 - val_loss: 18.7923 - val_mae: 3.1337\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.3747 - mae: 2.5036 - val_loss: 18.7013 - val_mae: 3.1287\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.3719 - mae: 2.5156 - val_loss: 18.7796 - val_mae: 3.1399\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.2576 - mae: 2.5199 - val_loss: 18.5452 - val_mae: 3.1271\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0571 - mae: 2.4911 - val_loss: 18.4327 - val_mae: 3.1183\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.9363 - mae: 2.4628 - val_loss: 18.3267 - val_mae: 3.1097\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.8144 - mae: 2.4454 - val_loss: 18.2111 - val_mae: 3.0911\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.6684 - mae: 2.4463 - val_loss: 18.0806 - val_mae: 3.0899\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.6426 - mae: 2.4625 - val_loss: 18.0897 - val_mae: 3.0964\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.5909 - mae: 2.4619 - val_loss: 18.0295 - val_mae: 3.0995\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4907 - mae: 2.4426 - val_loss: 17.9135 - val_mae: 3.0915\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.3115 - mae: 2.4079 - val_loss: 17.8655 - val_mae: 3.0830\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.2020 - mae: 2.3954 - val_loss: 17.7471 - val_mae: 3.0708\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.0881 - mae: 2.3800 - val_loss: 17.5405 - val_mae: 3.0682\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.3096 - mae: 2.4477 - val_loss: 17.2707 - val_mae: 3.1040\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4300 - mae: 2.5006 - val_loss: 17.1357 - val_mae: 3.0940\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.1972 - mae: 2.4786 - val_loss: 16.9829 - val_mae: 3.0774\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8605 - mae: 2.4434 - val_loss: 17.0937 - val_mae: 3.0620\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6302 - mae: 2.4152 - val_loss: 17.4730 - val_mae: 3.0322\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7453 - mae: 2.4005 - val_loss: 17.4936 - val_mae: 3.0269\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6285 - mae: 2.3661 - val_loss: 17.2422 - val_mae: 3.0204\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.4291 - mae: 2.3510 - val_loss: 16.9696 - val_mae: 3.0250\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.2797 - mae: 2.3416 - val_loss: 16.8752 - val_mae: 3.0333\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.2363 - mae: 2.3363 - val_loss: 16.7881 - val_mae: 3.0267\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.0464 - mae: 2.3340 - val_loss: 16.8249 - val_mae: 3.0412\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6722 - mae: 2.4284 - val_loss: 16.8204 - val_mae: 3.0912\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.4245 - mae: 2.4055 - val_loss: 16.6265 - val_mae: 3.0440\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1311 - mae: 2.3590 - val_loss: 16.7168 - val_mae: 3.0176\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.0172 - mae: 2.3281 - val_loss: 16.8793 - val_mae: 3.0137\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.8448 - mae: 2.3214 - val_loss: 16.7562 - val_mae: 3.0195\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.8975 - mae: 2.3199 - val_loss: 16.9317 - val_mae: 3.0175\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.9240 - mae: 2.3213 - val_loss: 16.9632 - val_mae: 3.0214\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.7757 - mae: 2.3046 - val_loss: 16.5473 - val_mae: 2.9996\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.5454 - mae: 2.2931 - val_loss: 16.3445 - val_mae: 3.0257\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.6244 - mae: 2.3166 - val_loss: 16.3219 - val_mae: 3.0223\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.5127 - mae: 2.2982 - val_loss: 16.3511 - val_mae: 3.0016\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.3608 - mae: 2.2929 - val_loss: 16.3451 - val_mae: 3.0139\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3473 - mae: 2.3015 - val_loss: 16.2918 - val_mae: 3.0084\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2595 - mae: 2.2911 - val_loss: 16.1241 - val_mae: 2.9967\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.4510 - mae: 2.3256 - val_loss: 16.4396 - val_mae: 3.0217\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3710 - mae: 2.3117 - val_loss: 16.3009 - val_mae: 2.9997\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.1975 - mae: 2.2805 - val_loss: 16.2768 - val_mae: 2.9924\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.1128 - mae: 2.2674 - val_loss: 16.0680 - val_mae: 2.9798\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.9525 - mae: 2.2423 - val_loss: 16.0904 - val_mae: 2.9829\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.8669 - mae: 2.2364 - val_loss: 15.9656 - val_mae: 2.9837\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7866 - mae: 2.2363 - val_loss: 15.9557 - val_mae: 2.9884\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.8244 - mae: 2.2540 - val_loss: 15.9352 - val_mae: 2.9865\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7637 - mae: 2.2426 - val_loss: 15.8654 - val_mae: 2.9701\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.6835 - mae: 2.2138 - val_loss: 15.8781 - val_mae: 2.9661\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.5520 - mae: 2.2058 - val_loss: 15.7695 - val_mae: 2.9774\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5039 - mae: 2.2297 - val_loss: 15.7104 - val_mae: 3.0275\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7572 - mae: 2.2671 - val_loss: 15.6438 - val_mae: 3.0120\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.4533 - mae: 2.2342 - val_loss: 15.6786 - val_mae: 2.9818\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.3932 - mae: 2.2163 - val_loss: 15.7780 - val_mae: 2.9823\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.3846 - mae: 2.2113 - val_loss: 15.7500 - val_mae: 2.9795\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2568 - mae: 2.2022 - val_loss: 15.7350 - val_mae: 2.9923\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.2210 - mae: 2.1985 - val_loss: 15.8951 - val_mae: 3.0098\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.1877 - mae: 2.2018 - val_loss: 15.8903 - val_mae: 3.0069\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.1643 - mae: 2.1844 - val_loss: 15.9153 - val_mae: 2.9961\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.1078 - mae: 2.1751 - val_loss: 15.8837 - val_mae: 2.9925\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0361 - mae: 2.1715 - val_loss: 15.8197 - val_mae: 2.9997\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.0493 - mae: 2.1788 - val_loss: 15.7937 - val_mae: 3.0024\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.1074 - mae: 2.1922 - val_loss: 15.6969 - val_mae: 3.0010\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9794 - mae: 2.1869 - val_loss: 15.8228 - val_mae: 3.0098\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.9078 - mae: 2.1640 - val_loss: 15.8586 - val_mae: 3.0032\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.9778 - mae: 2.1690 - val_loss: 15.6962 - val_mae: 2.9938\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.8517 - mae: 2.1623 - val_loss: 15.5551 - val_mae: 3.0064\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0898 - mae: 2.2095 - val_loss: 15.6982 - val_mae: 3.0411\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.9908 - mae: 2.1928 - val_loss: 15.6507 - val_mae: 2.9908\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7844 - mae: 2.1635 - val_loss: 15.6033 - val_mae: 2.9735\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.7256 - mae: 2.1354 - val_loss: 15.4822 - val_mae: 2.9650\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7333 - mae: 2.1366 - val_loss: 15.4534 - val_mae: 2.9852\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.6312 - mae: 2.1497 - val_loss: 15.5556 - val_mae: 3.0268\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6734 - mae: 2.1721 - val_loss: 15.6363 - val_mae: 3.0329\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5481 - mae: 2.1481 - val_loss: 15.6127 - val_mae: 3.0066\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7405 - mae: 2.1362 - val_loss: 15.8314 - val_mae: 3.0263\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7334 - mae: 2.1581 - val_loss: 15.6926 - val_mae: 3.0396\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4607 - mae: 2.1273 - val_loss: 15.8185 - val_mae: 3.0237\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.5713 - mae: 2.1129 - val_loss: 16.1591 - val_mae: 3.0092\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6231 - mae: 2.1150 - val_loss: 15.9502 - val_mae: 2.9981\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3681 - mae: 2.0904 - val_loss: 15.8000 - val_mae: 3.0138\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3142 - mae: 2.1093 - val_loss: 15.8153 - val_mae: 3.0260\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.2754 - mae: 2.1031 - val_loss: 15.7704 - val_mae: 3.0169\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.2781 - mae: 2.0875 - val_loss: 15.7231 - val_mae: 2.9978\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1814 - mae: 2.0770 - val_loss: 15.5043 - val_mae: 2.9807\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1361 - mae: 2.0750 - val_loss: 15.4386 - val_mae: 2.9896\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1286 - mae: 2.0749 - val_loss: 15.3920 - val_mae: 2.9834\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0720 - mae: 2.0692 - val_loss: 15.5339 - val_mae: 2.9741\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1708 - mae: 2.0855 - val_loss: 15.4836 - val_mae: 2.9731\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4580 - mae: 2.1287 - val_loss: 15.3768 - val_mae: 3.0126\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3821 - mae: 2.1023 - val_loss: 15.2628 - val_mae: 2.9976\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0530 - mae: 2.0702 - val_loss: 15.3035 - val_mae: 2.9919\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9624 - mae: 2.0830 - val_loss: 15.3640 - val_mae: 2.9946\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8393 - mae: 2.0646 - val_loss: 15.3530 - val_mae: 2.9892\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0687 - mae: 2.0733 - val_loss: 15.3684 - val_mae: 2.9813\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0178 - mae: 2.0582 - val_loss: 15.1754 - val_mae: 2.9719\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7990 - mae: 2.0470 - val_loss: 15.3007 - val_mae: 2.9736\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7650 - mae: 2.0453 - val_loss: 15.2369 - val_mae: 2.9653\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7817 - mae: 2.0460 - val_loss: 15.2320 - val_mae: 2.9834\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7509 - mae: 2.0491 - val_loss: 15.2551 - val_mae: 2.9816\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6549 - mae: 2.0266 - val_loss: 15.2036 - val_mae: 2.9699\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5915 - mae: 2.0152 - val_loss: 15.2536 - val_mae: 2.9647\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6257 - mae: 2.0161 - val_loss: 15.4468 - val_mae: 2.9997\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5838 - mae: 2.0371 - val_loss: 15.3486 - val_mae: 2.9713\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5364 - mae: 2.0208 - val_loss: 15.3178 - val_mae: 2.9551\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5661 - mae: 2.0165 - val_loss: 15.3347 - val_mae: 2.9533\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5434 - mae: 2.0178 - val_loss: 15.3249 - val_mae: 2.9552\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4751 - mae: 2.0123 - val_loss: 15.2384 - val_mae: 2.9525\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4053 - mae: 2.0017 - val_loss: 15.3520 - val_mae: 2.9915\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5775 - mae: 2.0266 - val_loss: 15.3624 - val_mae: 3.0059\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4208 - mae: 2.0094 - val_loss: 15.0755 - val_mae: 2.9620\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2761 - mae: 1.9923 - val_loss: 15.0612 - val_mae: 2.9509\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3792 - mae: 2.0081 - val_loss: 15.1938 - val_mae: 2.9617\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2984 - mae: 1.9896 - val_loss: 14.9698 - val_mae: 2.9442\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2729 - mae: 1.9950 - val_loss: 15.0118 - val_mae: 2.9498\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1225 - mae: 1.9819 - val_loss: 15.1487 - val_mae: 2.9624\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1146 - mae: 1.9645 - val_loss: 15.1921 - val_mae: 2.9691\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0938 - mae: 1.9621 - val_loss: 15.2634 - val_mae: 2.9797\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1205 - mae: 1.9744 - val_loss: 15.3071 - val_mae: 2.9852\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0666 - mae: 1.9738 - val_loss: 15.1936 - val_mae: 2.9761\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0345 - mae: 1.9676 - val_loss: 15.2077 - val_mae: 2.9765\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0031 - mae: 1.9668 - val_loss: 15.1985 - val_mae: 2.9712\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9442 - mae: 1.9554 - val_loss: 15.1471 - val_mae: 2.9659\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9170 - mae: 1.9474 - val_loss: 15.1339 - val_mae: 2.9652\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.8871 - mae: 1.9362 - val_loss: 15.1335 - val_mae: 2.9681\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9628 - mae: 1.9415 - val_loss: 15.2139 - val_mae: 2.9768\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0352 - mae: 1.9445 - val_loss: 15.1207 - val_mae: 2.9510\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.9156 - mae: 1.9275 - val_loss: 15.0289 - val_mae: 2.9494\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.8332 - mae: 1.9250 - val_loss: 15.0796 - val_mae: 2.9527\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.8121 - mae: 1.9203 - val_loss: 15.1402 - val_mae: 2.9605\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7666 - mae: 1.9132 - val_loss: 15.0825 - val_mae: 2.9530\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7262 - mae: 1.9173 - val_loss: 15.0787 - val_mae: 2.9534\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.8150 - mae: 1.9376 - val_loss: 15.1391 - val_mae: 2.9813\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9108 - mae: 1.9359 - val_loss: 14.8464 - val_mae: 2.9439\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8379 - mae: 1.9431 - val_loss: 14.7171 - val_mae: 2.9311\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8369 - mae: 1.9432 - val_loss: 14.6983 - val_mae: 2.9293\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7062 - mae: 1.9135 - val_loss: 14.7822 - val_mae: 2.9373\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6919 - mae: 1.8977 - val_loss: 14.7525 - val_mae: 2.9393\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7729 - mae: 1.9327 - val_loss: 14.7835 - val_mae: 2.9514\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7427 - mae: 1.9351 - val_loss: 14.6641 - val_mae: 2.9384\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5364 - mae: 1.8815 - val_loss: 14.6900 - val_mae: 2.9407\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6170 - mae: 1.8851 - val_loss: 14.7800 - val_mae: 2.9531\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.7907 - mae: 1.9091 - val_loss: 14.5968 - val_mae: 2.9259\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6423 - mae: 1.8935 - val_loss: 14.7035 - val_mae: 2.9393\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.6330 - mae: 1.9015 - val_loss: 14.6217 - val_mae: 2.9254\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5909 - mae: 1.9214 - val_loss: 14.6066 - val_mae: 2.9304\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5135 - mae: 1.9037 - val_loss: 14.4083 - val_mae: 2.9117\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5051 - mae: 1.9289 - val_loss: 14.6245 - val_mae: 2.9317\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4673 - mae: 1.8967 - val_loss: 14.6166 - val_mae: 2.8957\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5616 - mae: 1.8917 - val_loss: 14.6389 - val_mae: 2.9058\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3186 - mae: 1.8460 - val_loss: 14.7805 - val_mae: 2.9240\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2816 - mae: 1.8492 - val_loss: 14.9299 - val_mae: 2.9501\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2107 - mae: 1.8371 - val_loss: 14.9370 - val_mae: 2.9573\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2306 - mae: 1.8432 - val_loss: 15.0515 - val_mae: 2.9807\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2103 - mae: 1.8411 - val_loss: 14.9278 - val_mae: 2.9655\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1977 - mae: 1.8364 - val_loss: 14.8020 - val_mae: 2.9419\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1203 - mae: 1.8252 - val_loss: 14.8096 - val_mae: 2.9403\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1772 - mae: 1.8307 - val_loss: 14.8426 - val_mae: 2.9396\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1120 - mae: 1.8182 - val_loss: 14.8529 - val_mae: 2.9453\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0501 - mae: 1.8161 - val_loss: 14.8661 - val_mae: 2.9491\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0309 - mae: 1.8136 - val_loss: 14.7543 - val_mae: 2.9430\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0261 - mae: 1.8175 - val_loss: 14.5869 - val_mae: 2.9268\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0036 - mae: 1.8141 - val_loss: 14.5967 - val_mae: 2.9273\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9435 - mae: 1.8020 - val_loss: 14.6758 - val_mae: 2.9329\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8808 - mae: 1.7886 - val_loss: 14.7418 - val_mae: 2.9417\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1642 - mae: 1.8171 - val_loss: 15.1723 - val_mae: 2.9583\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3870 - mae: 1.8377 - val_loss: 15.1252 - val_mae: 2.9727\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 6.1004 - mae: 1.8599 - val_loss: 16.4204 - val_mae: 3.0858\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.5012 - mae: 1.9208 - val_loss: 16.0713 - val_mae: 3.0809\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 0s 743us/step - loss: 6.1229 - mae: 1.8273 - val_loss: 15.6517 - val_mae: 3.0480\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.0301 - mae: 1.8125 - val_loss: 15.4626 - val_mae: 3.0289\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9996 - mae: 1.8126 - val_loss: 15.2029 - val_mae: 2.9888\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9158 - mae: 1.7956 - val_loss: 15.0903 - val_mae: 2.9756\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7905 - mae: 1.7891 - val_loss: 15.1507 - val_mae: 2.9878\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7148 - mae: 1.7800 - val_loss: 14.8398 - val_mae: 2.9580\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8154 - mae: 1.7936 - val_loss: 14.6459 - val_mae: 2.9342\n",
      "Epoch 248/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7868 - mae: 1.7977 - val_loss: 14.8204 - val_mae: 2.9691\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7395 - mae: 1.7844 - val_loss: 14.8154 - val_mae: 2.9589\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6295 - mae: 1.7534 - val_loss: 14.7938 - val_mae: 2.9411\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6770 - mae: 1.7730 - val_loss: 14.7819 - val_mae: 2.9435\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5880 - mae: 1.7579 - val_loss: 14.8920 - val_mae: 2.9636\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5535 - mae: 1.7497 - val_loss: 14.8494 - val_mae: 2.9496\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5611 - mae: 1.7537 - val_loss: 14.8266 - val_mae: 2.9428\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5346 - mae: 1.7446 - val_loss: 14.8763 - val_mae: 2.9484\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5608 - mae: 1.7680 - val_loss: 15.7657 - val_mae: 3.0438\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8533 - mae: 1.8137 - val_loss: 15.5741 - val_mae: 3.0351\n",
      "Epoch 258/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6347 - mae: 1.7663 - val_loss: 15.3446 - val_mae: 3.0111\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7887 - mae: 1.7687 - val_loss: 15.2465 - val_mae: 2.9824\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5963 - mae: 1.7574 - val_loss: 15.4614 - val_mae: 3.0077\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.5884 - mae: 1.7787 - val_loss: 15.4653 - val_mae: 3.0190\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0383 - mae: 1.8135 - val_loss: 15.4290 - val_mae: 2.9843\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1252 - mae: 1.7905 - val_loss: 15.4196 - val_mae: 3.0108\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6797 - mae: 1.7410 - val_loss: 15.4693 - val_mae: 3.0194\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4579 - mae: 1.7353 - val_loss: 15.2361 - val_mae: 2.9973\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3677 - mae: 1.7215 - val_loss: 15.1354 - val_mae: 2.9897\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4869 - mae: 1.7302 - val_loss: 15.3187 - val_mae: 3.0117\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4164 - mae: 1.7260 - val_loss: 14.9690 - val_mae: 2.9760\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2770 - mae: 1.7090 - val_loss: 14.8088 - val_mae: 2.9488\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3267 - mae: 1.7219 - val_loss: 14.6244 - val_mae: 2.9309\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2989 - mae: 1.7194 - val_loss: 14.6437 - val_mae: 2.9277\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3973 - mae: 1.7376 - val_loss: 15.0382 - val_mae: 2.9334\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8089 - mae: 1.7912 - val_loss: 14.9072 - val_mae: 2.9252\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4013 - mae: 1.7205 - val_loss: 14.8450 - val_mae: 2.9301\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1525 - mae: 1.6905 - val_loss: 14.9453 - val_mae: 2.9486\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1556 - mae: 1.6955 - val_loss: 14.8842 - val_mae: 2.9497\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.1350 - mae: 1.6994 - val_loss: 14.9895 - val_mae: 2.9849\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1465 - mae: 1.6975 - val_loss: 14.8453 - val_mae: 2.9476\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0492 - mae: 1.6780 - val_loss: 14.9728 - val_mae: 2.9616\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1833 - mae: 1.7115 - val_loss: 14.9841 - val_mae: 2.9703\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.1395 - mae: 1.6989 - val_loss: 14.7625 - val_mae: 2.9389\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.3066 - mae: 1.7346 - val_loss: 14.9194 - val_mae: 2.9639\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 0s 829us/step - loss: 5.3631 - mae: 1.7703 - val_loss: 15.1530 - val_mae: 3.0023\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.9730 - mae: 1.8401 - val_loss: 14.7476 - val_mae: 2.9495\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.4462 - mae: 1.7409 - val_loss: 14.7553 - val_mae: 2.9550\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.2319 - mae: 1.7291 - val_loss: 14.9915 - val_mae: 2.9750\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.0326 - mae: 1.6813 - val_loss: 15.3110 - val_mae: 2.9882\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9639 - mae: 1.6606 - val_loss: 15.1420 - val_mae: 2.9697\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9546 - mae: 1.6461 - val_loss: 14.9710 - val_mae: 2.9536\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9325 - mae: 1.6369 - val_loss: 14.9455 - val_mae: 2.9524\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.9625 - mae: 1.6933 - val_loss: 15.0496 - val_mae: 2.9667\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8439 - mae: 1.6500 - val_loss: 14.8939 - val_mae: 2.9467\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.7268 - mae: 1.6155 - val_loss: 14.9443 - val_mae: 2.9538\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6821 - mae: 1.6068 - val_loss: 14.8984 - val_mae: 2.9490\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6972 - mae: 1.6039 - val_loss: 15.0021 - val_mae: 2.9689\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 4.6897 - mae: 1.5976 - val_loss: 14.8975 - val_mae: 2.9440\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6345 - mae: 1.5872 - val_loss: 14.8983 - val_mae: 2.9406\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.5819 - mae: 1.5795 - val_loss: 14.8238 - val_mae: 2.9339\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.6028 - mae: 1.5819 - val_loss: 14.9703 - val_mae: 2.9735\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 4.8389 - mae: 1.6431 - val_loss: 15.0476 - val_mae: 3.0041\n",
      "6/6 [==============================] - 0s 665us/step - loss: 10.8925 - mae: 2.2742\n",
      "훈련셋 인덱스: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
      " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
      " 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328\n",
      " 329 330 331 332 333 334 335 336 337 338]\n",
      "검증셋 인덱스: [113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n",
      " 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n",
      " 221 222 223 224 225]\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 592.7663 - mae: 22.4076 - val_loss: 578.9495 - val_mae: 22.1957\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 576.6802 - mae: 22.0422 - val_loss: 562.2266 - val_mae: 21.8205\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 561.2428 - mae: 21.6819 - val_loss: 544.2802 - val_mae: 21.4090\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 543.9420 - mae: 21.2715 - val_loss: 523.6735 - val_mae: 20.9187\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 523.3931 - mae: 20.7548 - val_loss: 497.7466 - val_mae: 20.2975\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 497.8706 - mae: 20.1063 - val_loss: 465.6791 - val_mae: 19.5129\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 465.7760 - mae: 19.2754 - val_loss: 426.5310 - val_mae: 18.5228\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 427.9813 - mae: 18.2466 - val_loss: 380.9961 - val_mae: 17.3076\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 383.8730 - mae: 16.9731 - val_loss: 330.0593 - val_mae: 15.8941\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 335.9065 - mae: 15.5628 - val_loss: 275.7797 - val_mae: 14.2768\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 285.4474 - mae: 14.0486 - val_loss: 223.3307 - val_mae: 12.5035\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 235.7533 - mae: 12.4979 - val_loss: 175.8014 - val_mae: 10.8540\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 188.7994 - mae: 10.9882 - val_loss: 133.1943 - val_mae: 9.2598\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 148.5684 - mae: 9.5008 - val_loss: 104.3983 - val_mae: 8.0870\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 118.8273 - mae: 8.3523 - val_loss: 86.3442 - val_mae: 7.3087\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 97.2998 - mae: 7.4568 - val_loss: 77.1273 - val_mae: 6.8326\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 83.7138 - mae: 6.7883 - val_loss: 70.7087 - val_mae: 6.4690\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 74.6479 - mae: 6.2754 - val_loss: 63.7219 - val_mae: 6.0866\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 65.2691 - mae: 5.7786 - val_loss: 55.4835 - val_mae: 5.6069\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.4398 - mae: 5.3659 - val_loss: 50.2132 - val_mae: 5.2914\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.6568 - mae: 5.0849 - val_loss: 46.2233 - val_mae: 5.0293\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 47.3570 - mae: 4.8999 - val_loss: 43.0570 - val_mae: 4.8056\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 44.1222 - mae: 4.7426 - val_loss: 40.1201 - val_mae: 4.6199\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 41.2417 - mae: 4.5997 - val_loss: 37.5556 - val_mae: 4.4711\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 38.8192 - mae: 4.4488 - val_loss: 35.1116 - val_mae: 4.3004\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 36.7899 - mae: 4.3086 - val_loss: 33.4307 - val_mae: 4.1827\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 34.7731 - mae: 4.1806 - val_loss: 32.0611 - val_mae: 4.0815\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 33.2990 - mae: 4.0874 - val_loss: 30.9151 - val_mae: 3.9922\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 32.1160 - mae: 4.0051 - val_loss: 29.8411 - val_mae: 3.8953\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 30.8925 - mae: 3.9258 - val_loss: 28.9007 - val_mae: 3.8222\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 29.7571 - mae: 3.8730 - val_loss: 28.1260 - val_mae: 3.7670\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 28.8959 - mae: 3.8144 - val_loss: 27.5410 - val_mae: 3.7177\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 28.1240 - mae: 3.7622 - val_loss: 26.9095 - val_mae: 3.6675\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 27.3829 - mae: 3.7187 - val_loss: 26.3546 - val_mae: 3.6194\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 26.4686 - mae: 3.6687 - val_loss: 25.8709 - val_mae: 3.5952\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 25.7299 - mae: 3.6313 - val_loss: 25.4019 - val_mae: 3.5650\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 25.1211 - mae: 3.5846 - val_loss: 25.0070 - val_mae: 3.5377\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 24.5853 - mae: 3.5558 - val_loss: 24.7429 - val_mae: 3.5403\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 24.0818 - mae: 3.5316 - val_loss: 24.4052 - val_mae: 3.5116\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.5135 - mae: 3.5024 - val_loss: 24.0049 - val_mae: 3.4526\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 22.9371 - mae: 3.4889 - val_loss: 23.8735 - val_mae: 3.4270\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 22.5372 - mae: 3.4893 - val_loss: 23.7051 - val_mae: 3.4191\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 22.0675 - mae: 3.4589 - val_loss: 23.3396 - val_mae: 3.3841\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 21.6129 - mae: 3.4155 - val_loss: 23.0236 - val_mae: 3.3360\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 21.1792 - mae: 3.3612 - val_loss: 22.8099 - val_mae: 3.2983\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 20.8720 - mae: 3.3229 - val_loss: 22.4736 - val_mae: 3.2798\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.6670 - mae: 3.2744 - val_loss: 22.2406 - val_mae: 3.2671\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.2675 - mae: 3.2279 - val_loss: 22.0493 - val_mae: 3.2542\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.8515 - mae: 3.2152 - val_loss: 21.9330 - val_mae: 3.2564\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.5325 - mae: 3.2120 - val_loss: 21.8948 - val_mae: 3.2334\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.1285 - mae: 3.1967 - val_loss: 21.8080 - val_mae: 3.2211\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.9134 - mae: 3.1784 - val_loss: 21.8035 - val_mae: 3.1926\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.6090 - mae: 3.1613 - val_loss: 21.6524 - val_mae: 3.1937\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.3554 - mae: 3.1440 - val_loss: 21.5677 - val_mae: 3.1785\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.1206 - mae: 3.1256 - val_loss: 21.4527 - val_mae: 3.1655\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.8828 - mae: 3.1012 - val_loss: 21.2610 - val_mae: 3.1424\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6142 - mae: 3.0693 - val_loss: 21.0564 - val_mae: 3.1268\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2984 - mae: 3.0486 - val_loss: 21.0543 - val_mae: 3.1280\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0769 - mae: 3.0408 - val_loss: 21.0672 - val_mae: 3.1270\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.8745 - mae: 3.0322 - val_loss: 20.9870 - val_mae: 3.1109\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.6925 - mae: 3.0185 - val_loss: 20.9356 - val_mae: 3.0932\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.4579 - mae: 3.0043 - val_loss: 20.6948 - val_mae: 3.0936\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.4736 - mae: 3.0142 - val_loss: 20.6587 - val_mae: 3.1045\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.2274 - mae: 2.9912 - val_loss: 20.5032 - val_mae: 3.0671\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.0862 - mae: 2.9793 - val_loss: 20.6970 - val_mae: 3.0721\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.7563 - mae: 2.9485 - val_loss: 20.3735 - val_mae: 3.0321\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.5946 - mae: 2.9163 - val_loss: 20.1051 - val_mae: 3.0132\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.4746 - mae: 2.8916 - val_loss: 19.7973 - val_mae: 2.9893\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.4379 - mae: 2.8644 - val_loss: 19.7091 - val_mae: 2.9669\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.2548 - mae: 2.8478 - val_loss: 19.5858 - val_mae: 2.9418\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.0584 - mae: 2.8250 - val_loss: 19.5235 - val_mae: 2.9225\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.9318 - mae: 2.8081 - val_loss: 19.4556 - val_mae: 2.9097\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.7884 - mae: 2.7958 - val_loss: 19.4554 - val_mae: 2.8974\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.6247 - mae: 2.7894 - val_loss: 19.5770 - val_mae: 2.9028\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.4797 - mae: 2.7763 - val_loss: 19.6391 - val_mae: 2.9103\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.3970 - mae: 2.7852 - val_loss: 19.6048 - val_mae: 2.9068\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.1923 - mae: 2.7796 - val_loss: 19.5847 - val_mae: 2.8985\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.1623 - mae: 2.7660 - val_loss: 19.4957 - val_mae: 2.8830\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.8963 - mae: 2.7211 - val_loss: 19.1165 - val_mae: 2.8420\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.8147 - mae: 2.7128 - val_loss: 18.9641 - val_mae: 2.8311\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.7534 - mae: 2.7023 - val_loss: 18.9125 - val_mae: 2.8213\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.6006 - mae: 2.6908 - val_loss: 18.9885 - val_mae: 2.8238\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.4444 - mae: 2.6937 - val_loss: 19.2210 - val_mae: 2.8616\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.3582 - mae: 2.6998 - val_loss: 19.3156 - val_mae: 2.8415\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.1681 - mae: 2.6711 - val_loss: 19.2174 - val_mae: 2.8121\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0762 - mae: 2.6453 - val_loss: 19.0321 - val_mae: 2.7884\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0091 - mae: 2.6292 - val_loss: 18.9483 - val_mae: 2.7772\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.8908 - mae: 2.6157 - val_loss: 18.8747 - val_mae: 2.7667\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7697 - mae: 2.5982 - val_loss: 18.9237 - val_mae: 2.7616\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7053 - mae: 2.5832 - val_loss: 18.7013 - val_mae: 2.7294\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.9311 - mae: 2.5888 - val_loss: 18.4802 - val_mae: 2.6918\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7399 - mae: 2.5660 - val_loss: 18.4898 - val_mae: 2.7030\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.5999 - mae: 2.5972 - val_loss: 18.6722 - val_mae: 2.7522\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.2293 - mae: 2.5821 - val_loss: 18.7007 - val_mae: 2.7561\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.2025 - mae: 2.5739 - val_loss: 18.6810 - val_mae: 2.7553\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9990 - mae: 2.5564 - val_loss: 18.7185 - val_mae: 2.7667\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9131 - mae: 2.5671 - val_loss: 18.7557 - val_mae: 2.7843\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8451 - mae: 2.5633 - val_loss: 18.5796 - val_mae: 2.7663\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.8374 - mae: 2.5570 - val_loss: 18.4900 - val_mae: 2.7686\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7335 - mae: 2.5438 - val_loss: 18.4210 - val_mae: 2.7497\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6167 - mae: 2.5272 - val_loss: 18.4444 - val_mae: 2.7643\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5612 - mae: 2.5247 - val_loss: 18.4593 - val_mae: 2.7721\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5180 - mae: 2.5078 - val_loss: 18.4340 - val_mae: 2.7574\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5098 - mae: 2.5409 - val_loss: 18.9794 - val_mae: 2.8290\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.4572 - mae: 2.5312 - val_loss: 18.8902 - val_mae: 2.7650\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2465 - mae: 2.4998 - val_loss: 18.8192 - val_mae: 2.7083\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1075 - mae: 2.4577 - val_loss: 18.5695 - val_mae: 2.6664\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0579 - mae: 2.4409 - val_loss: 18.4411 - val_mae: 2.6694\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.9816 - mae: 2.4270 - val_loss: 18.5096 - val_mae: 2.6808\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8573 - mae: 2.4215 - val_loss: 18.7174 - val_mae: 2.7079\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8059 - mae: 2.4433 - val_loss: 19.0704 - val_mae: 2.7789\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.9099 - mae: 2.4769 - val_loss: 19.0100 - val_mae: 2.7911\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.7600 - mae: 2.4573 - val_loss: 18.6023 - val_mae: 2.7392\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.6795 - mae: 2.4208 - val_loss: 18.1966 - val_mae: 2.6749\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4980 - mae: 2.3870 - val_loss: 18.1879 - val_mae: 2.6759\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4258 - mae: 2.3779 - val_loss: 18.4585 - val_mae: 2.6949\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4895 - mae: 2.3625 - val_loss: 18.3435 - val_mae: 2.6645\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3623 - mae: 2.3774 - val_loss: 18.2315 - val_mae: 2.6767\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.5037 - mae: 2.3965 - val_loss: 18.2137 - val_mae: 2.6676\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3818 - mae: 2.3754 - val_loss: 18.2847 - val_mae: 2.6862\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.3964 - mae: 2.4473 - val_loss: 20.2038 - val_mae: 2.9021\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - ETA: 0s - loss: 9.3327 - mae: 2.440 - 0s 2ms/step - loss: 10.6792 - mae: 2.4874 - val_loss: 20.0209 - val_mae: 2.8554\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4166 - mae: 2.4378 - val_loss: 19.2916 - val_mae: 2.7410\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.0154 - mae: 2.3612 - val_loss: 18.8376 - val_mae: 2.6813\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.0469 - mae: 2.3337 - val_loss: 18.5714 - val_mae: 2.6335\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.9787 - mae: 2.3045 - val_loss: 18.4646 - val_mae: 2.6179\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7904 - mae: 2.2931 - val_loss: 18.3069 - val_mae: 2.6121\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.6674 - mae: 2.3032 - val_loss: 18.4460 - val_mae: 2.6347\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5826 - mae: 2.2999 - val_loss: 18.4017 - val_mae: 2.6479\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5656 - mae: 2.2951 - val_loss: 18.2308 - val_mae: 2.6386\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5611 - mae: 2.3024 - val_loss: 18.0994 - val_mae: 2.6422\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4859 - mae: 2.2792 - val_loss: 18.0386 - val_mae: 2.6222\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4215 - mae: 2.2700 - val_loss: 18.4317 - val_mae: 2.6870\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4658 - mae: 2.2730 - val_loss: 18.3254 - val_mae: 2.6737\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4957 - mae: 2.2710 - val_loss: 17.8092 - val_mae: 2.6141\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3299 - mae: 2.2619 - val_loss: 18.3613 - val_mae: 2.6849\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4195 - mae: 2.2831 - val_loss: 18.5998 - val_mae: 2.6977\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2247 - mae: 2.2486 - val_loss: 17.9683 - val_mae: 2.5980\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0869 - mae: 2.1904 - val_loss: 17.7050 - val_mae: 2.5671\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3135 - mae: 2.1970 - val_loss: 17.7377 - val_mae: 2.5692\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1301 - mae: 2.1744 - val_loss: 17.4675 - val_mae: 2.5358\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1502 - mae: 2.1948 - val_loss: 17.6804 - val_mae: 2.5952\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1407 - mae: 2.2176 - val_loss: 17.8632 - val_mae: 2.6016\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0945 - mae: 2.1780 - val_loss: 18.5229 - val_mae: 2.7130\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1199 - mae: 2.1595 - val_loss: 18.2227 - val_mae: 2.6359\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8935 - mae: 2.1393 - val_loss: 18.0428 - val_mae: 2.5823\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7996 - mae: 2.1480 - val_loss: 18.0482 - val_mae: 2.5765\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7253 - mae: 2.1567 - val_loss: 18.0454 - val_mae: 2.5716\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6857 - mae: 2.1434 - val_loss: 18.1489 - val_mae: 2.5890\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5992 - mae: 2.1337 - val_loss: 17.9795 - val_mae: 2.5627\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6561 - mae: 2.1319 - val_loss: 18.0016 - val_mae: 2.5741\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5652 - mae: 2.1138 - val_loss: 18.2178 - val_mae: 2.5997\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5549 - mae: 2.1115 - val_loss: 18.1798 - val_mae: 2.5663\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4633 - mae: 2.1058 - val_loss: 18.0050 - val_mae: 2.5612\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.4828 - mae: 2.1264 - val_loss: 17.9024 - val_mae: 2.5540\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4263 - mae: 2.0944 - val_loss: 17.8737 - val_mae: 2.5531\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3902 - mae: 2.0776 - val_loss: 17.8630 - val_mae: 2.5506\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - ETA: 0s - loss: 8.8732 - mae: 2.174 - 0s 2ms/step - loss: 8.3336 - mae: 2.0714 - val_loss: 18.1265 - val_mae: 2.5935\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4150 - mae: 2.1116 - val_loss: 18.7724 - val_mae: 2.6683\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4302 - mae: 2.1385 - val_loss: 18.5436 - val_mae: 2.6401\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.3177 - mae: 2.1002 - val_loss: 18.4053 - val_mae: 2.6309\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5722 - mae: 2.1589 - val_loss: 19.8877 - val_mae: 2.8029\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7002 - mae: 2.1380 - val_loss: 18.1137 - val_mae: 2.5527\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.3428 - mae: 2.0929 - val_loss: 17.7417 - val_mae: 2.4578\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2842 - mae: 2.0875 - val_loss: 17.6914 - val_mae: 2.4624\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1998 - mae: 2.0712 - val_loss: 17.5884 - val_mae: 2.4741\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1279 - mae: 2.0387 - val_loss: 17.5055 - val_mae: 2.5144\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2983 - mae: 2.0742 - val_loss: 18.9100 - val_mae: 2.7489\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4675 - mae: 2.1430 - val_loss: 18.8977 - val_mae: 2.7488\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2357 - mae: 2.0939 - val_loss: 18.1536 - val_mae: 2.6253\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2790 - mae: 2.1017 - val_loss: 17.9184 - val_mae: 2.5693\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.2503 - mae: 2.1007 - val_loss: 17.8274 - val_mae: 2.5234\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9701 - mae: 2.0609 - val_loss: 17.9781 - val_mae: 2.5663\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8584 - mae: 2.0336 - val_loss: 18.2521 - val_mae: 2.6119\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8252 - mae: 2.0405 - val_loss: 18.5320 - val_mae: 2.6616\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.1597 - mae: 2.1177 - val_loss: 19.3237 - val_mae: 2.7872\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0843 - mae: 2.1014 - val_loss: 18.6993 - val_mae: 2.7085\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8609 - mae: 2.0367 - val_loss: 17.8110 - val_mae: 2.5583\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7040 - mae: 2.0005 - val_loss: 17.5824 - val_mae: 2.5185\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7280 - mae: 1.9936 - val_loss: 17.7049 - val_mae: 2.5544\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6195 - mae: 2.0016 - val_loss: 18.0305 - val_mae: 2.5664\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6708 - mae: 2.0199 - val_loss: 18.3043 - val_mae: 2.6035\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.6633 - mae: 2.0033 - val_loss: 18.4956 - val_mae: 2.6227\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6342 - mae: 2.0120 - val_loss: 18.5679 - val_mae: 2.6050\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5723 - mae: 2.0041 - val_loss: 18.0783 - val_mae: 2.5220\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5042 - mae: 1.9620 - val_loss: 18.3926 - val_mae: 2.5925\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8461 - mae: 2.0330 - val_loss: 19.3200 - val_mae: 2.7561\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5871 - mae: 2.0185 - val_loss: 18.5531 - val_mae: 2.6575\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4238 - mae: 2.0191 - val_loss: 17.9660 - val_mae: 2.5790\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4346 - mae: 2.0157 - val_loss: 17.8400 - val_mae: 2.5668\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3747 - mae: 1.9981 - val_loss: 17.9283 - val_mae: 2.5766\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3185 - mae: 1.9864 - val_loss: 17.7006 - val_mae: 2.5177\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3739 - mae: 1.9860 - val_loss: 17.4784 - val_mae: 2.5063\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2690 - mae: 1.9560 - val_loss: 17.5914 - val_mae: 2.5152\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2489 - mae: 1.9635 - val_loss: 17.8654 - val_mae: 2.5749\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3191 - mae: 1.9570 - val_loss: 17.6198 - val_mae: 2.5548\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2323 - mae: 1.9376 - val_loss: 17.5685 - val_mae: 2.5519\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1567 - mae: 1.9306 - val_loss: 17.4666 - val_mae: 2.5315\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.1719 - mae: 1.9445 - val_loss: 17.1581 - val_mae: 2.4744\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.3008 - mae: 1.9489 - val_loss: 16.8164 - val_mae: 2.4196\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5309 - mae: 1.9616 - val_loss: 17.0498 - val_mae: 2.4754\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3861 - mae: 1.9274 - val_loss: 16.9902 - val_mae: 2.4696\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2328 - mae: 1.9405 - val_loss: 17.2876 - val_mae: 2.5170\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.2333 - mae: 1.9769 - val_loss: 17.7984 - val_mae: 2.6124\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1052 - mae: 1.9485 - val_loss: 17.8367 - val_mae: 2.6145\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.0747 - mae: 1.9099 - val_loss: 17.6168 - val_mae: 2.5696\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1260 - mae: 1.8924 - val_loss: 17.1679 - val_mae: 2.4728\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9825 - mae: 1.8895 - val_loss: 17.4592 - val_mae: 2.5165\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9800 - mae: 1.9120 - val_loss: 18.7173 - val_mae: 2.6561\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.1710 - mae: 1.9986 - val_loss: 19.2883 - val_mae: 2.6855\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2306 - mae: 2.0154 - val_loss: 18.6147 - val_mae: 2.5989\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2314 - mae: 2.0086 - val_loss: 17.9679 - val_mae: 2.5115\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9206 - mae: 1.8975 - val_loss: 17.2301 - val_mae: 2.4354\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0725 - mae: 1.8883 - val_loss: 17.2123 - val_mae: 2.4543\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9081 - mae: 1.8673 - val_loss: 17.2953 - val_mae: 2.4901\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8161 - mae: 1.8672 - val_loss: 17.6374 - val_mae: 2.5735\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7939 - mae: 1.8851 - val_loss: 17.6315 - val_mae: 2.5585\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7642 - mae: 1.8499 - val_loss: 17.1255 - val_mae: 2.4275\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0541 - mae: 1.8622 - val_loss: 17.2215 - val_mae: 2.4384\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.0380 - mae: 1.8663 - val_loss: 17.5029 - val_mae: 2.5060\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7728 - mae: 1.8590 - val_loss: 17.4526 - val_mae: 2.4999\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7655 - mae: 1.8910 - val_loss: 17.5704 - val_mae: 2.5107\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7424 - mae: 1.8665 - val_loss: 17.2214 - val_mae: 2.4425\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7894 - mae: 1.8641 - val_loss: 17.3324 - val_mae: 2.4671\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7090 - mae: 1.8748 - val_loss: 17.7412 - val_mae: 2.5269\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6482 - mae: 1.8846 - val_loss: 17.8653 - val_mae: 2.5340\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6270 - mae: 1.8946 - val_loss: 18.0042 - val_mae: 2.5467\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6348 - mae: 1.8804 - val_loss: 17.7513 - val_mae: 2.5195\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5207 - mae: 1.8421 - val_loss: 17.5489 - val_mae: 2.4984\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5052 - mae: 1.8277 - val_loss: 17.5580 - val_mae: 2.4994\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4810 - mae: 1.8236 - val_loss: 17.5779 - val_mae: 2.4934\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4419 - mae: 1.8295 - val_loss: 17.8148 - val_mae: 2.5438\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4188 - mae: 1.8213 - val_loss: 17.6058 - val_mae: 2.5248\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4970 - mae: 1.8246 - val_loss: 17.5770 - val_mae: 2.4834\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4213 - mae: 1.8148 - val_loss: 17.8563 - val_mae: 2.5182\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4690 - mae: 1.8203 - val_loss: 17.8235 - val_mae: 2.4875\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4493 - mae: 1.8300 - val_loss: 17.8953 - val_mae: 2.5227\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5657 - mae: 1.8392 - val_loss: 18.5041 - val_mae: 2.6751\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5460 - mae: 1.8374 - val_loss: 18.1169 - val_mae: 2.6343\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3360 - mae: 1.8164 - val_loss: 17.9328 - val_mae: 2.5850\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.3104 - mae: 1.8187 - val_loss: 18.1016 - val_mae: 2.6086\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3690 - mae: 1.8444 - val_loss: 18.6856 - val_mae: 2.7124\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.3939 - mae: 1.8414 - val_loss: 18.1173 - val_mae: 2.6396\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.2532 - mae: 1.8065 - val_loss: 17.7080 - val_mae: 2.5463\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1889 - mae: 1.7935 - val_loss: 17.9596 - val_mae: 2.5759\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.5466 - mae: 1.8518 - val_loss: 20.1327 - val_mae: 2.8394\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8524 - mae: 1.9172 - val_loss: 19.4008 - val_mae: 2.7778\n",
      "Epoch 248/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6218 - mae: 1.8514 - val_loss: 18.4108 - val_mae: 2.6567\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4485 - mae: 1.8307 - val_loss: 17.8462 - val_mae: 2.5454\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.2172 - mae: 1.8296 - val_loss: 19.3364 - val_mae: 2.7415\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6163 - mae: 1.8982 - val_loss: 19.7774 - val_mae: 2.7789\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.4750 - mae: 1.8699 - val_loss: 18.8945 - val_mae: 2.6334\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1599 - mae: 1.8193 - val_loss: 18.2575 - val_mae: 2.5209\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.4415 - mae: 1.8323 - val_loss: 18.4867 - val_mae: 2.6120\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.3563 - mae: 1.8251 - val_loss: 18.2919 - val_mae: 2.6177\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.1358 - mae: 1.8007 - val_loss: 18.2720 - val_mae: 2.6366\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.1030 - mae: 1.7988 - val_loss: 17.8456 - val_mae: 2.5728\n",
      "Epoch 258/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0529 - mae: 1.8005 - val_loss: 17.9817 - val_mae: 2.6086\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0070 - mae: 1.7797 - val_loss: 17.8714 - val_mae: 2.5804\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9836 - mae: 1.7656 - val_loss: 17.7602 - val_mae: 2.5520\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.0745 - mae: 1.7839 - val_loss: 17.9130 - val_mae: 2.5586\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9790 - mae: 1.7692 - val_loss: 18.0967 - val_mae: 2.5779\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9776 - mae: 1.7499 - val_loss: 18.4790 - val_mae: 2.6413\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0519 - mae: 1.7536 - val_loss: 18.2880 - val_mae: 2.6462\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8939 - mae: 1.7429 - val_loss: 18.0260 - val_mae: 2.6292\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 6.0272 - mae: 1.7892 - val_loss: 17.8941 - val_mae: 2.6065\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.9544 - mae: 1.7816 - val_loss: 17.8296 - val_mae: 2.5761\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.9686 - mae: 1.7965 - val_loss: 18.2584 - val_mae: 2.6239\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.8967 - mae: 1.7652 - val_loss: 18.2720 - val_mae: 2.6380\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8434 - mae: 1.7389 - val_loss: 17.8087 - val_mae: 2.5595\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8300 - mae: 1.7311 - val_loss: 17.7273 - val_mae: 2.5504\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.8809 - mae: 1.7452 - val_loss: 17.7968 - val_mae: 2.5557\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8284 - mae: 1.7299 - val_loss: 17.9331 - val_mae: 2.5925\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7856 - mae: 1.7221 - val_loss: 18.1966 - val_mae: 2.6299\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8095 - mae: 1.7253 - val_loss: 18.2916 - val_mae: 2.6297\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7786 - mae: 1.7160 - val_loss: 18.0922 - val_mae: 2.5746\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.7741 - mae: 1.7396 - val_loss: 18.1994 - val_mae: 2.5689\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.7070 - mae: 1.7204 - val_loss: 18.2460 - val_mae: 2.5986\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7885 - mae: 1.7165 - val_loss: 18.3074 - val_mae: 2.6111\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7181 - mae: 1.7170 - val_loss: 18.0962 - val_mae: 2.5724\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7424 - mae: 1.7414 - val_loss: 18.2821 - val_mae: 2.6244\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.7096 - mae: 1.7363 - val_loss: 17.9321 - val_mae: 2.5854\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5.6880 - mae: 1.7127 - val_loss: 17.7903 - val_mae: 2.5148\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.8597 - mae: 1.7497 - val_loss: 17.8556 - val_mae: 2.4976\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6954 - mae: 1.7022 - val_loss: 17.9484 - val_mae: 2.5477\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6632 - mae: 1.6944 - val_loss: 18.1633 - val_mae: 2.5874\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.6611 - mae: 1.7064 - val_loss: 18.1426 - val_mae: 2.5581\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5901 - mae: 1.6937 - val_loss: 18.0732 - val_mae: 2.5498\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5863 - mae: 1.6808 - val_loss: 18.1262 - val_mae: 2.5771\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5405 - mae: 1.6667 - val_loss: 17.9010 - val_mae: 2.5398\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5066 - mae: 1.6674 - val_loss: 18.0026 - val_mae: 2.5523\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5914 - mae: 1.6919 - val_loss: 18.0560 - val_mae: 2.5752\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5482 - mae: 1.6913 - val_loss: 18.0972 - val_mae: 2.5770\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5222 - mae: 1.6701 - val_loss: 17.7802 - val_mae: 2.5338\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5157 - mae: 1.6627 - val_loss: 17.9677 - val_mae: 2.5776\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4817 - mae: 1.6739 - val_loss: 18.1280 - val_mae: 2.5732\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.4470 - mae: 1.6624 - val_loss: 17.9570 - val_mae: 2.5689\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.5378 - mae: 1.6645 - val_loss: 17.4766 - val_mae: 2.5071\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7747 - mae: 1.7171 - val_loss: 17.6829 - val_mae: 2.5246\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 5.7544 - mae: 1.7279 - val_loss: 17.8171 - val_mae: 2.5233\n",
      "6/6 [==============================] - 0s 499us/step - loss: 7.9626 - mae: 2.0925\n",
      "훈련셋 인덱스: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225]\n",
      "검증셋 인덱스: [226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243\n",
      " 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261\n",
      " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
      " 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297\n",
      " 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315\n",
      " 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333\n",
      " 334 335 336 337 338]\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 584.1354 - mae: 22.4029 - val_loss: 591.2020 - val_mae: 22.3348\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 561.9612 - mae: 21.8844 - val_loss: 569.4901 - val_mae: 21.8406\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 540.0161 - mae: 21.3796 - val_loss: 547.2084 - val_mae: 21.3222\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 516.8474 - mae: 20.8435 - val_loss: 522.1476 - val_mae: 20.7229\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 490.5011 - mae: 20.2002 - val_loss: 492.9949 - val_mae: 19.9973\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 458.9771 - mae: 19.4139 - val_loss: 457.5011 - val_mae: 19.0887\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 419.3784 - mae: 18.4365 - val_loss: 414.5840 - val_mae: 17.9760\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 372.6456 - mae: 17.2020 - val_loss: 363.9335 - val_mae: 16.5988\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 319.5247 - mae: 15.7025 - val_loss: 308.5481 - val_mae: 14.9951\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 261.1390 - mae: 13.9200 - val_loss: 253.4379 - val_mae: 13.2613\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 205.5524 - mae: 12.0124 - val_loss: 202.9915 - val_mae: 11.4604\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 155.7242 - mae: 10.0657 - val_loss: 158.6231 - val_mae: 9.6971\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 115.2620 - mae: 8.2232 - val_loss: 123.7276 - val_mae: 8.1283\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 87.6237 - mae: 6.8533 - val_loss: 102.9653 - val_mae: 7.2998\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 72.8121 - mae: 6.1692 - val_loss: 90.5058 - val_mae: 6.8602\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 63.6582 - mae: 5.7265 - val_loss: 80.2145 - val_mae: 6.4442\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 57.0076 - mae: 5.3481 - val_loss: 72.4819 - val_mae: 6.0774\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 51.5928 - mae: 5.0330 - val_loss: 65.6932 - val_mae: 5.7262\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 46.9244 - mae: 4.7646 - val_loss: 59.6175 - val_mae: 5.3915\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 42.8574 - mae: 4.5415 - val_loss: 54.9534 - val_mae: 5.1273\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 40.0507 - mae: 4.3741 - val_loss: 51.8843 - val_mae: 4.8800\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 37.9976 - mae: 4.2230 - val_loss: 48.8481 - val_mae: 4.6694\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 36.1760 - mae: 4.1152 - val_loss: 45.8058 - val_mae: 4.5105\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 34.2934 - mae: 4.0315 - val_loss: 43.3743 - val_mae: 4.4049\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 32.8476 - mae: 3.9709 - val_loss: 41.5817 - val_mae: 4.3574\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 31.8180 - mae: 3.9468 - val_loss: 39.9141 - val_mae: 4.3065\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 30.7766 - mae: 3.9089 - val_loss: 38.5683 - val_mae: 4.2449\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 30.0601 - mae: 3.8672 - val_loss: 37.1501 - val_mae: 4.1593\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 29.1006 - mae: 3.7889 - val_loss: 35.9000 - val_mae: 4.0712\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 28.3548 - mae: 3.7227 - val_loss: 34.7787 - val_mae: 3.9911\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 27.8267 - mae: 3.6854 - val_loss: 34.0185 - val_mae: 3.9386\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 27.2495 - mae: 3.6327 - val_loss: 33.0812 - val_mae: 3.8718\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 26.5256 - mae: 3.5781 - val_loss: 32.0331 - val_mae: 3.8195\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 25.8760 - mae: 3.5399 - val_loss: 31.1626 - val_mae: 3.7822\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 25.3817 - mae: 3.5199 - val_loss: 30.3186 - val_mae: 3.7232\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 25.0450 - mae: 3.4774 - val_loss: 29.7294 - val_mae: 3.6727\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 24.5734 - mae: 3.4374 - val_loss: 28.9737 - val_mae: 3.6312\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.9362 - mae: 3.3862 - val_loss: 28.2897 - val_mae: 3.5860\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 23.4245 - mae: 3.3512 - val_loss: 27.6003 - val_mae: 3.5524\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 23.0066 - mae: 3.3148 - val_loss: 26.9042 - val_mae: 3.5068\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 22.7227 - mae: 3.2812 - val_loss: 26.2314 - val_mae: 3.4452\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 22.1748 - mae: 3.2476 - val_loss: 25.7116 - val_mae: 3.4201\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.8172 - mae: 3.2376 - val_loss: 25.2959 - val_mae: 3.4327\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.4251 - mae: 3.3007 - val_loss: 25.5931 - val_mae: 3.6638\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.5382 - mae: 3.4541 - val_loss: 26.4189 - val_mae: 3.8346\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 22.6124 - mae: 3.4754 - val_loss: 25.1851 - val_mae: 3.6863\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 21.4336 - mae: 3.3080 - val_loss: 23.3224 - val_mae: 3.3998\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.1469 - mae: 3.1378 - val_loss: 22.7725 - val_mae: 3.2602\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.8148 - mae: 3.0598 - val_loss: 23.9095 - val_mae: 3.2237\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.6145 - mae: 3.0777 - val_loss: 24.5746 - val_mae: 3.2523\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.4320 - mae: 3.0634 - val_loss: 24.0975 - val_mae: 3.2343\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.5076 - mae: 3.0288 - val_loss: 23.7863 - val_mae: 3.2951\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.7705 - mae: 3.1413 - val_loss: 24.3079 - val_mae: 3.5378\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 20.0317 - mae: 3.2079 - val_loss: 23.8148 - val_mae: 3.4838\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 19.3592 - mae: 3.1234 - val_loss: 22.7379 - val_mae: 3.3141\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.7572 - mae: 3.0213 - val_loss: 22.1470 - val_mae: 3.2367\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.2671 - mae: 2.9656 - val_loss: 21.9330 - val_mae: 3.2408\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.4911 - mae: 3.0170 - val_loss: 22.4088 - val_mae: 3.3851\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.8401 - mae: 3.0682 - val_loss: 21.9011 - val_mae: 3.3168\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 18.1138 - mae: 2.9656 - val_loss: 21.0678 - val_mae: 3.1981\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.6734 - mae: 2.9111 - val_loss: 20.9033 - val_mae: 3.1591\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.5462 - mae: 2.8837 - val_loss: 20.7628 - val_mae: 3.1292\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.4298 - mae: 2.8868 - val_loss: 20.7190 - val_mae: 3.1501\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.2154 - mae: 2.8703 - val_loss: 20.5685 - val_mae: 3.1317\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0589 - mae: 2.8296 - val_loss: 20.3982 - val_mae: 3.0819\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 17.0787 - mae: 2.7910 - val_loss: 20.3960 - val_mae: 3.0701\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.9832 - mae: 2.7786 - val_loss: 20.2535 - val_mae: 3.0721\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.7806 - mae: 2.7676 - val_loss: 20.1924 - val_mae: 3.0736\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.6505 - mae: 2.7708 - val_loss: 20.0922 - val_mae: 3.0761\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.5472 - mae: 2.7736 - val_loss: 20.0319 - val_mae: 3.0795\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.4101 - mae: 2.7734 - val_loss: 19.9857 - val_mae: 3.0806\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.2772 - mae: 2.7603 - val_loss: 19.9331 - val_mae: 3.0783\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.2170 - mae: 2.7646 - val_loss: 19.8767 - val_mae: 3.0697\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 16.0980 - mae: 2.7486 - val_loss: 19.7565 - val_mae: 3.0758\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.9750 - mae: 2.7480 - val_loss: 19.6433 - val_mae: 3.0731\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.8770 - mae: 2.7428 - val_loss: 19.4734 - val_mae: 3.0660\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.8955 - mae: 2.7157 - val_loss: 19.4594 - val_mae: 3.0517\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.7946 - mae: 2.6798 - val_loss: 19.4729 - val_mae: 3.0543\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.7272 - mae: 2.6670 - val_loss: 19.5086 - val_mae: 3.0622\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.5459 - mae: 2.6609 - val_loss: 19.5378 - val_mae: 3.0807\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.4247 - mae: 2.6796 - val_loss: 19.6384 - val_mae: 3.1078\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.3359 - mae: 2.6808 - val_loss: 19.4996 - val_mae: 3.0936\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.1946 - mae: 2.6626 - val_loss: 19.3459 - val_mae: 3.0734\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.1129 - mae: 2.6581 - val_loss: 19.1902 - val_mae: 3.0520\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.9827 - mae: 2.6439 - val_loss: 19.0123 - val_mae: 3.0311\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.9701 - mae: 2.6324 - val_loss: 18.8804 - val_mae: 3.0233\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.9015 - mae: 2.6157 - val_loss: 18.8365 - val_mae: 3.0172\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.9378 - mae: 2.6013 - val_loss: 18.9045 - val_mae: 3.0189\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.9054 - mae: 2.5829 - val_loss: 18.9562 - val_mae: 3.0262\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.7887 - mae: 2.6161 - val_loss: 18.7967 - val_mae: 3.0400\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 14.6171 - mae: 2.6364 - val_loss: 18.7323 - val_mae: 3.0435\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.5694 - mae: 2.6411 - val_loss: 18.6325 - val_mae: 3.0328\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.4328 - mae: 2.6210 - val_loss: 18.4951 - val_mae: 3.0088\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.3939 - mae: 2.6036 - val_loss: 18.3687 - val_mae: 2.9893\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.2795 - mae: 2.5865 - val_loss: 18.4383 - val_mae: 3.0071\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.2461 - mae: 2.5838 - val_loss: 18.4082 - val_mae: 3.0168\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.1298 - mae: 2.5768 - val_loss: 18.1827 - val_mae: 3.0033\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.1375 - mae: 2.6092 - val_loss: 18.1972 - val_mae: 3.0666\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.7592 - mae: 2.7776 - val_loss: 19.9068 - val_mae: 3.3368\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 16.0679 - mae: 2.9426 - val_loss: 19.1714 - val_mae: 3.2325\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 14.8176 - mae: 2.7423 - val_loss: 17.5200 - val_mae: 3.0008\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.7575 - mae: 2.5755 - val_loss: 17.3728 - val_mae: 3.0029\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.8591 - mae: 2.5385 - val_loss: 17.4445 - val_mae: 3.0143\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.8138 - mae: 2.5437 - val_loss: 17.4500 - val_mae: 3.0231\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.6582 - mae: 2.5263 - val_loss: 17.3952 - val_mae: 3.0148\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.5649 - mae: 2.5337 - val_loss: 17.3358 - val_mae: 2.9997\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.4727 - mae: 2.5356 - val_loss: 17.3745 - val_mae: 2.9998\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.3890 - mae: 2.5308 - val_loss: 17.4959 - val_mae: 3.0132\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.3358 - mae: 2.5262 - val_loss: 17.3777 - val_mae: 2.9849\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.2186 - mae: 2.5092 - val_loss: 17.2631 - val_mae: 2.9595\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.1379 - mae: 2.4876 - val_loss: 17.2124 - val_mae: 2.9501\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.1207 - mae: 2.4934 - val_loss: 17.2691 - val_mae: 2.9633\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.1397 - mae: 2.5295 - val_loss: 17.4168 - val_mae: 2.9890\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.0976 - mae: 2.5389 - val_loss: 17.3442 - val_mae: 2.9766\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 13.0053 - mae: 2.5184 - val_loss: 17.1291 - val_mae: 2.9346\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.8526 - mae: 2.4682 - val_loss: 17.0276 - val_mae: 2.9155\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 12.8848 - mae: 2.4486 - val_loss: 16.9512 - val_mae: 2.9241\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7869 - mae: 2.4417 - val_loss: 17.0270 - val_mae: 2.9419\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0722 - mae: 2.5783 - val_loss: 19.0197 - val_mae: 3.2330\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.6423 - mae: 2.9511 - val_loss: 21.4575 - val_mae: 3.4876\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 15.6956 - mae: 2.9187 - val_loss: 18.7762 - val_mae: 3.1759\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.7361 - mae: 2.6562 - val_loss: 17.2169 - val_mae: 3.0393\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.9386 - mae: 2.5431 - val_loss: 16.8971 - val_mae: 3.0102\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.0395 - mae: 2.5739 - val_loss: 17.9483 - val_mae: 3.1170\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.4946 - mae: 2.6837 - val_loss: 18.2246 - val_mae: 3.1713\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 13.2897 - mae: 2.6729 - val_loss: 17.5898 - val_mae: 3.1001\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.7279 - mae: 2.5822 - val_loss: 16.9260 - val_mae: 3.0186\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4462 - mae: 2.5196 - val_loss: 16.7120 - val_mae: 2.9992\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.5481 - mae: 2.5113 - val_loss: 16.8037 - val_mae: 3.0316\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.4375 - mae: 2.4908 - val_loss: 16.5415 - val_mae: 2.9862\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1658 - mae: 2.4621 - val_loss: 16.4889 - val_mae: 2.9587\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9759 - mae: 2.4517 - val_loss: 16.4408 - val_mae: 2.9321\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9839 - mae: 2.4554 - val_loss: 16.2464 - val_mae: 2.9153\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.9070 - mae: 2.3998 - val_loss: 16.1274 - val_mae: 2.9278\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.1443 - mae: 2.4189 - val_loss: 16.0752 - val_mae: 2.9250\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 12.0446 - mae: 2.3857 - val_loss: 15.8579 - val_mae: 2.8587\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.8500 - mae: 2.3689 - val_loss: 15.7049 - val_mae: 2.8347\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7068 - mae: 2.3730 - val_loss: 15.6056 - val_mae: 2.8238\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.6599 - mae: 2.3857 - val_loss: 15.4890 - val_mae: 2.8087\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6497 - mae: 2.3641 - val_loss: 15.4443 - val_mae: 2.8328\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.7867 - mae: 2.3950 - val_loss: 15.6917 - val_mae: 2.9029\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.6009 - mae: 2.3823 - val_loss: 15.4119 - val_mae: 2.8336\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.5226 - mae: 2.3615 - val_loss: 15.1962 - val_mae: 2.7999\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.3793 - mae: 2.3374 - val_loss: 15.2354 - val_mae: 2.8125\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.3023 - mae: 2.3367 - val_loss: 15.2462 - val_mae: 2.8175\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2728 - mae: 2.3201 - val_loss: 15.2560 - val_mae: 2.8259\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2280 - mae: 2.2985 - val_loss: 15.2737 - val_mae: 2.8133\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1986 - mae: 2.3090 - val_loss: 15.3958 - val_mae: 2.8289\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2596 - mae: 2.3574 - val_loss: 15.3878 - val_mae: 2.8173\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0894 - mae: 2.3384 - val_loss: 15.2709 - val_mae: 2.8044\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.0439 - mae: 2.3298 - val_loss: 15.3253 - val_mae: 2.8034\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.2936 - mae: 2.3937 - val_loss: 15.5860 - val_mae: 2.8480\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.0195 - mae: 2.3475 - val_loss: 15.1596 - val_mae: 2.8532\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.9739 - mae: 2.2955 - val_loss: 15.0758 - val_mae: 2.8643\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 11.1038 - mae: 2.2908 - val_loss: 14.8842 - val_mae: 2.8330\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.9348 - mae: 2.2704 - val_loss: 14.8661 - val_mae: 2.7940\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8362 - mae: 2.2736 - val_loss: 14.9858 - val_mae: 2.8133\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8545 - mae: 2.2927 - val_loss: 15.4645 - val_mae: 2.9126\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.8200 - mae: 2.2814 - val_loss: 15.2862 - val_mae: 2.8791\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7323 - mae: 2.2648 - val_loss: 15.1660 - val_mae: 2.8328\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.7970 - mae: 2.2511 - val_loss: 15.1315 - val_mae: 2.8407\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.6418 - mae: 2.2403 - val_loss: 15.0916 - val_mae: 2.8442\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5149 - mae: 2.2407 - val_loss: 14.8761 - val_mae: 2.8021\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.5206 - mae: 2.2570 - val_loss: 14.8895 - val_mae: 2.8043\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.5356 - mae: 2.2375 - val_loss: 14.9401 - val_mae: 2.8406\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5174 - mae: 2.2128 - val_loss: 14.9803 - val_mae: 2.8365\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.4247 - mae: 2.2077 - val_loss: 14.9157 - val_mae: 2.8003\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.4071 - mae: 2.2273 - val_loss: 14.9852 - val_mae: 2.8074\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.3633 - mae: 2.2326 - val_loss: 14.7446 - val_mae: 2.7687\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.3124 - mae: 2.2316 - val_loss: 14.6385 - val_mae: 2.7615\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.2560 - mae: 2.2200 - val_loss: 14.6207 - val_mae: 2.7731\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.2341 - mae: 2.1979 - val_loss: 14.5808 - val_mae: 2.7839\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.2038 - mae: 2.1926 - val_loss: 14.5786 - val_mae: 2.7686\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1948 - mae: 2.2017 - val_loss: 14.6895 - val_mae: 2.7730\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.5754 - mae: 2.2088 - val_loss: 15.1175 - val_mae: 2.8469\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.8396 - mae: 2.2250 - val_loss: 15.9087 - val_mae: 2.8662\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 11.0845 - mae: 2.2235 - val_loss: 15.8985 - val_mae: 2.8511\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.6285 - mae: 2.2201 - val_loss: 15.6732 - val_mae: 2.8451\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.3150 - mae: 2.2627 - val_loss: 15.7059 - val_mae: 2.8593\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.4438 - mae: 2.2957 - val_loss: 15.4335 - val_mae: 2.8396\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.1035 - mae: 2.2298 - val_loss: 15.1138 - val_mae: 2.7809\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.0158 - mae: 2.1805 - val_loss: 14.9362 - val_mae: 2.7716\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.0066 - mae: 2.1706 - val_loss: 14.8497 - val_mae: 2.7761\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8493 - mae: 2.1675 - val_loss: 14.8461 - val_mae: 2.7903\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8082 - mae: 2.1762 - val_loss: 14.7018 - val_mae: 2.7973\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8929 - mae: 2.1616 - val_loss: 14.6177 - val_mae: 2.8116\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.9542 - mae: 2.1701 - val_loss: 14.3177 - val_mae: 2.7758\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.8148 - mae: 2.1739 - val_loss: 14.1644 - val_mae: 2.7445\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.7165 - mae: 2.2128 - val_loss: 14.6189 - val_mae: 2.7935\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 10.0662 - mae: 2.3024 - val_loss: 14.5767 - val_mae: 2.7613\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.7100 - mae: 2.2372 - val_loss: 14.2810 - val_mae: 2.7225\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5827 - mae: 2.1845 - val_loss: 14.3759 - val_mae: 2.7374\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.5528 - mae: 2.1610 - val_loss: 14.3913 - val_mae: 2.7580\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5161 - mae: 2.1524 - val_loss: 14.3437 - val_mae: 2.7596\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4832 - mae: 2.1370 - val_loss: 14.4618 - val_mae: 2.7988\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6922 - mae: 2.1340 - val_loss: 14.7844 - val_mae: 2.8549\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.2719 - mae: 2.1947 - val_loss: 14.8683 - val_mae: 2.8430\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 10.1430 - mae: 2.1535 - val_loss: 13.7839 - val_mae: 2.7218\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.7141 - mae: 2.1443 - val_loss: 13.4244 - val_mae: 2.6661\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.5019 - mae: 2.1732 - val_loss: 13.3454 - val_mae: 2.6618\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.6312 - mae: 2.2222 - val_loss: 13.4098 - val_mae: 2.6663\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4150 - mae: 2.1836 - val_loss: 13.2735 - val_mae: 2.6587\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3236 - mae: 2.1466 - val_loss: 13.5739 - val_mae: 2.7305\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.2270 - mae: 2.1281 - val_loss: 13.4171 - val_mae: 2.6943\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1195 - mae: 2.1020 - val_loss: 13.4791 - val_mae: 2.6939\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 9.1517 - mae: 2.1227 - val_loss: 13.4830 - val_mae: 2.6997\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.3650 - mae: 2.1063 - val_loss: 13.7795 - val_mae: 2.7689\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.4357 - mae: 2.1099 - val_loss: 13.6922 - val_mae: 2.7508\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1541 - mae: 2.0887 - val_loss: 13.6103 - val_mae: 2.7224\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1137 - mae: 2.1248 - val_loss: 13.7294 - val_mae: 2.7366\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1285 - mae: 2.1580 - val_loss: 13.6117 - val_mae: 2.7199\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9565 - mae: 2.1170 - val_loss: 13.5838 - val_mae: 2.7303\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8902 - mae: 2.0937 - val_loss: 13.6120 - val_mae: 2.7114\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9046 - mae: 2.1506 - val_loss: 14.2116 - val_mae: 2.7363\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2035 - mae: 2.2228 - val_loss: 14.0463 - val_mae: 2.7307\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.9677 - mae: 2.1216 - val_loss: 14.1280 - val_mae: 2.8038\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.2215 - mae: 2.1309 - val_loss: 14.4281 - val_mae: 2.8528\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.1589 - mae: 2.1104 - val_loss: 13.9030 - val_mae: 2.7818\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7255 - mae: 2.0723 - val_loss: 13.5966 - val_mae: 2.7039\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7255 - mae: 2.1163 - val_loss: 13.6225 - val_mae: 2.6879\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7068 - mae: 2.0953 - val_loss: 13.4199 - val_mae: 2.6819\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6526 - mae: 2.1145 - val_loss: 13.2803 - val_mae: 2.6640\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8838 - mae: 2.1683 - val_loss: 13.3734 - val_mae: 2.6945\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8067 - mae: 2.1366 - val_loss: 13.2926 - val_mae: 2.6926\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5937 - mae: 2.0783 - val_loss: 13.3047 - val_mae: 2.6786\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0489 - mae: 2.0721 - val_loss: 14.0589 - val_mae: 2.7183\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8858 - mae: 2.0421 - val_loss: 14.0436 - val_mae: 2.6921\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8374 - mae: 2.0825 - val_loss: 13.9910 - val_mae: 2.6960\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7863 - mae: 2.0995 - val_loss: 13.7164 - val_mae: 2.6992\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5792 - mae: 2.0374 - val_loss: 13.8324 - val_mae: 2.7502\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6204 - mae: 2.1284 - val_loss: 14.7486 - val_mae: 2.8004\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7718 - mae: 2.1867 - val_loss: 14.6322 - val_mae: 2.7514\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 9.0783 - mae: 2.1937 - val_loss: 15.2477 - val_mae: 2.8030\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.8628 - mae: 2.1433 - val_loss: 15.0500 - val_mae: 2.8313\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.6907 - mae: 2.1121 - val_loss: 14.6381 - val_mae: 2.8339\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.4660 - mae: 2.0893 - val_loss: 14.3376 - val_mae: 2.8381\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5078 - mae: 2.0734 - val_loss: 13.7859 - val_mae: 2.7756\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1746 - mae: 2.0318 - val_loss: 13.3392 - val_mae: 2.7124\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1096 - mae: 2.0341 - val_loss: 13.2035 - val_mae: 2.6803\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0383 - mae: 2.0280 - val_loss: 13.2757 - val_mae: 2.6954\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0403 - mae: 2.0010 - val_loss: 13.6860 - val_mae: 2.7664\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.1723 - mae: 2.0131 - val_loss: 13.7036 - val_mae: 2.7399\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.0888 - mae: 2.0090 - val_loss: 13.7090 - val_mae: 2.7493\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 8.1611 - mae: 2.0161 - val_loss: 13.9165 - val_mae: 2.8093\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.2803 - mae: 2.0340 - val_loss: 13.6434 - val_mae: 2.7825\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9219 - mae: 2.0039 - val_loss: 13.2384 - val_mae: 2.6981\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8223 - mae: 2.0053 - val_loss: 13.1849 - val_mae: 2.6696\n",
      "Epoch 248/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8571 - mae: 2.0218 - val_loss: 13.2184 - val_mae: 2.6797\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8925 - mae: 2.0248 - val_loss: 13.2656 - val_mae: 2.7144\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7544 - mae: 1.9965 - val_loss: 13.3973 - val_mae: 2.7075\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7388 - mae: 2.0051 - val_loss: 13.5046 - val_mae: 2.6920\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7043 - mae: 2.0138 - val_loss: 13.3782 - val_mae: 2.6726\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7642 - mae: 2.0096 - val_loss: 13.5624 - val_mae: 2.6834\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8855 - mae: 2.0183 - val_loss: 13.8018 - val_mae: 2.7433\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7933 - mae: 2.0064 - val_loss: 13.5243 - val_mae: 2.6887\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.7637 - mae: 1.9994 - val_loss: 13.3648 - val_mae: 2.6898\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6632 - mae: 1.9893 - val_loss: 13.4500 - val_mae: 2.7181\n",
      "Epoch 258/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.6659 - mae: 1.9947 - val_loss: 13.7384 - val_mae: 2.7647\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4858 - mae: 1.9640 - val_loss: 13.6971 - val_mae: 2.7025\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8460 - mae: 2.0184 - val_loss: 14.0989 - val_mae: 2.7284\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.8299 - mae: 2.0183 - val_loss: 13.9849 - val_mae: 2.7422\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5421 - mae: 1.9759 - val_loss: 14.3563 - val_mae: 2.8721\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.8469 - mae: 2.0158 - val_loss: 14.2407 - val_mae: 2.8677\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5220 - mae: 1.9693 - val_loss: 13.6333 - val_mae: 2.7527\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5398 - mae: 2.0090 - val_loss: 13.7685 - val_mae: 2.7336\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4214 - mae: 1.9956 - val_loss: 13.5226 - val_mae: 2.7401\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.5451 - mae: 1.9714 - val_loss: 13.5566 - val_mae: 2.7949\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5735 - mae: 1.9352 - val_loss: 13.0980 - val_mae: 2.7075\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.2705 - mae: 1.9195 - val_loss: 13.1421 - val_mae: 2.6823\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3450 - mae: 1.9404 - val_loss: 13.3889 - val_mae: 2.6801\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4444 - mae: 1.9567 - val_loss: 13.4799 - val_mae: 2.6635\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 7.4671 - mae: 1.9606 - val_loss: 13.6043 - val_mae: 2.7039\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.7527 - mae: 2.0929 - val_loss: 16.0626 - val_mae: 2.9406\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.5826 - mae: 2.2135 - val_loss: 15.0372 - val_mae: 2.8314\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6450 - mae: 2.0519 - val_loss: 14.5518 - val_mae: 2.8449\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.6429 - mae: 2.0353 - val_loss: 14.3937 - val_mae: 2.7765\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3510 - mae: 2.0136 - val_loss: 13.9637 - val_mae: 2.6905\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.4276 - mae: 2.0390 - val_loss: 14.1022 - val_mae: 2.7405\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9552 - mae: 2.1229 - val_loss: 15.5423 - val_mae: 2.8590\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.9399 - mae: 2.1125 - val_loss: 14.5505 - val_mae: 2.7765\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.5275 - mae: 2.0370 - val_loss: 14.5310 - val_mae: 2.8297\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 8.0844 - mae: 2.1447 - val_loss: 16.9344 - val_mae: 2.9802\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 8.7779 - mae: 2.2190 - val_loss: 15.7924 - val_mae: 2.8737\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3437 - mae: 2.0513 - val_loss: 14.5460 - val_mae: 2.8253\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 7.3635 - mae: 2.0129 - val_loss: 14.4593 - val_mae: 2.8484\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9084 - mae: 1.9661 - val_loss: 14.3581 - val_mae: 2.7751\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 7.1411 - mae: 2.0197 - val_loss: 14.3462 - val_mae: 2.7354\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.8697 - mae: 1.9661 - val_loss: 13.9912 - val_mae: 2.7521\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9064 - mae: 1.9297 - val_loss: 13.9034 - val_mae: 2.7762\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8592 - mae: 1.9296 - val_loss: 13.8835 - val_mae: 2.7499\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7292 - mae: 1.9168 - val_loss: 13.8492 - val_mae: 2.7364\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7080 - mae: 1.9250 - val_loss: 13.6300 - val_mae: 2.7126\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6889 - mae: 1.9191 - val_loss: 13.5093 - val_mae: 2.7042\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8390 - mae: 1.9158 - val_loss: 13.6092 - val_mae: 2.7621\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8799 - mae: 1.9303 - val_loss: 13.7066 - val_mae: 2.7754\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.7309 - mae: 1.9142 - val_loss: 13.0877 - val_mae: 2.6786\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9277 - mae: 1.9228 - val_loss: 12.8005 - val_mae: 2.6590\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.9301 - mae: 1.9219 - val_loss: 12.7456 - val_mae: 2.6568\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.8278 - mae: 1.9028 - val_loss: 12.7301 - val_mae: 2.6300\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 6.6928 - mae: 1.8959 - val_loss: 12.7930 - val_mae: 2.6203\n",
      "6/6 [==============================] - 0s 665us/step - loss: 10.6774 - mae: 2.3302\n"
     ]
    }
   ],
   "source": [
    "# 표준화한 데이터를 분할\n",
    "split_x_train = kfold.split(std_x_train)\n",
    "split_x_train\n",
    "\n",
    "# 분할된 데이터 수만큼 반복 수행\n",
    "\n",
    "# 데이터셋을 평가한 후 결과 mae를 담을 리스트\n",
    "mae_list = []\n",
    "\n",
    "# k번 진행 - 각각 훈련셋 인덱스와 검증셋 인덱스 세트가 달라진다\n",
    "for train_index, val_index in split_x_train:\n",
    "    print(f'훈련셋 인덱스: {train_index}')\n",
    "    print(f'검증셋 인덱스: {val_index}')\n",
    "    \n",
    "    # 해당 인덱스는 무작위로 생성된다.\n",
    "    # 무작위로 생성하는 것이 과대적합을 피할 수 있는 좋은 방법이다.\n",
    "    x_train_fold, x_val_fold = std_x_train[train_index], std_x_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 13차원의 데이터를 입력으로 받고, 64개의 출력을 가지는 첫 번째 Dense층\n",
    "    model.add(Dense(64, activation = 'relu', input_shape = (13, )))\n",
    "\n",
    "    # 32개의 출력을 가지는 Dense층\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "    # 하나의 값을 출력\n",
    "    # -> 정답의 범위가 정해지지 않기 때문에 활성화 함수는 linear\n",
    "    # -> linear는 기본값이므로 생략 가능함\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "    result = model.fit(x_train_fold, y_train_fold,\n",
    "                      epochs = 300,\n",
    "                      validation_data = (x_val_fold, y_val_fold))\n",
    "\n",
    "    # 모델 평가\n",
    "    tmp, test_mae = model.evaluate(std_x_test, y_test)\n",
    "\n",
    "    # 평가 결과를 list에 추가함\n",
    "    mae_list.append(test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 결과(오차 리스트): [2.2741539478302, 2.0924527645111084, 2.330174446105957]\n",
      "오차들의 평균값을 최종 결과로 사용: 2.2322603861490884\n"
     ]
    }
   ],
   "source": [
    "print(f'전체 결과(오차 리스트): {mae_list}')\n",
    "print(f'오차들의 평균값을 최종 결과로 사용: {np.mean(mae_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내용 정리\n",
    "\n",
    "- 회귀는 분류에서 사용했던 것과는 다른 손실 함수를 사용한다.\n",
    "    - 평균 제곱 오차(MSE)는 회귀에서 자주 사용되는 손실 함구\n",
    "- 회귀에서 사용되는 평가 지표도 분류와 다르다.\n",
    "    - 정확도 개념은 회귀에 적용되지 않는다.\n",
    "    - 일반적인 회귀 지표는 평균 절대 오차(MAE)이다.\n",
    "- 입력 데이터의 특성이 서로 다른 범위를 가지면 전처리 단계에서 각 특성을 개별적으로 스케일 조정해야 한다.\n",
    "    - 표준화\n",
    "- 가용한 데이터가 적다면 'K-폴드 검증'을 사용하는 것이 신뢰할 수 있는 모델 평가 방법이다.\n",
    "- 가용한 훈련 데이터가 적다면 과대적합을 피하기 위해 은닉 층의 수를 줄인 모델이 좋다.(일반적으로 1개 또는 2개)\n",
    "    - model.add()를 적게 하라는 뜻"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
