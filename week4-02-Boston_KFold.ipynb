{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보스턴 주택 가격 예측하기\n",
    "\n",
    "- 회귀(regression): 연속적인 값의 예측\n",
    "- 보스턴 주택 가격 데이터셋 -> 1970년대 보스턴 지역의 범죄율, 토지 지역의 비율, 방의 개수 등 총 14개의 변수가 포함됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 참조 및 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.boston_housing import load_data\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KFold는 하나의 데이터셋을 내가 원하는 등분으로 나누어 학습데이터, 검증데이터를 계속해서 바꿔가며 분석을 진행하는 방법이다.\n",
    "- 다시 말해, 1-5까지 5등분을 했으면, 첫 번째에는 1-4까지가 학습데이터, 5가 검증데이터, 두 번째에는 1-3 + 5가 학습데이터, 4가 검증데이터 등등이 된다.\n",
    "> KFold가 필요한 이유? 만약 보스턴 집값 데이터가 집값이 가장 저렴한 집 ~ 가장 비싼 집 순으로 입력되어 있다면 그 데이터셋 그대로 7:3으로 나누었을 때 정확한 분석 결과를 도출하기 어렵기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 3us/step\n"
     ]
    }
   ],
   "source": [
    "# 데이터 다운받기\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path = 'boston_housing.npz', \n",
    "                                                test_split = 0.33, seed = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 크기 확인하기\n",
    "# 학습데이터와 학습 label의 크기\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 13) (167,)\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터와 검증 label의 크기\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>339 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0    22.5\n",
       "1     8.3\n",
       "2    17.2\n",
       "3    25.0\n",
       "4    28.5\n",
       "..    ...\n",
       "334  21.9\n",
       "335  21.8\n",
       "336  34.9\n",
       "337  25.0\n",
       "338  34.7\n",
       "\n",
       "[339 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주택 가격의 중간 가격($1,000 단위)를 의미함.\n",
    "DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터 확인\n",
    "- 총 13개의 특성을 갖는 데이터셋\n",
    "- 각 특성마다 데이터의 범위(스케일)가 다르다.\n",
    "    - 범죄율 : 0 ~ 1\n",
    "    - 방의 개수 : 3 ~ 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20608</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>5.593</td>\n",
       "      <td>76.5</td>\n",
       "      <td>7.9549</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>372.49</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01951</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>7.104</td>\n",
       "      <td>59.5</td>\n",
       "      <td>9.2229</td>\n",
       "      <td>3.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>393.24</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.67822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>5.362</td>\n",
       "      <td>96.2</td>\n",
       "      <td>2.1036</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>380.79</td>\n",
       "      <td>10.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04294</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>6.249</td>\n",
       "      <td>77.3</td>\n",
       "      <td>3.6150</td>\n",
       "      <td>4.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.07151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>6.121</td>\n",
       "      <td>56.8</td>\n",
       "      <td>3.7476</td>\n",
       "      <td>3.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>395.15</td>\n",
       "      <td>8.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>12.04820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>5.648</td>\n",
       "      <td>87.6</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>291.55</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.10328</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4530</td>\n",
       "      <td>5.927</td>\n",
       "      <td>47.2</td>\n",
       "      <td>6.9320</td>\n",
       "      <td>8.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>6.982</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5.4917</td>\n",
       "      <td>7.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>390.43</td>\n",
       "      <td>4.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.21161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>6.137</td>\n",
       "      <td>87.4</td>\n",
       "      <td>2.7147</td>\n",
       "      <td>5.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>394.47</td>\n",
       "      <td>13.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1      2    3       4      5     6       7     8      9   \\\n",
       "0     0.20608  22.0   5.86  0.0  0.4310  5.593  76.5  7.9549   7.0  330.0   \n",
       "1     0.01951  17.5   1.38  0.0  0.4161  7.104  59.5  9.2229   3.0  216.0   \n",
       "2     3.67822   0.0  18.10  0.0  0.7700  5.362  96.2  2.1036  24.0  666.0   \n",
       "3     0.63796   0.0   8.14  0.0  0.5380  6.096  84.5  4.4619   4.0  307.0   \n",
       "4     0.04294  28.0  15.04  0.0  0.4640  6.249  77.3  3.6150   4.0  270.0   \n",
       "..        ...   ...    ...  ...     ...    ...   ...     ...   ...    ...   \n",
       "162   0.07151   0.0   4.49  0.0  0.4490  6.121  56.8  3.7476   3.0  247.0   \n",
       "163  12.04820   0.0  18.10  0.0  0.6140  5.648  87.6  1.9512  24.0  666.0   \n",
       "164   0.10328  25.0   5.13  0.0  0.4530  5.927  47.2  6.9320   8.0  284.0   \n",
       "165   0.10000  34.0   6.09  0.0  0.4330  6.982  17.7  5.4917   7.0  329.0   \n",
       "166   0.21161   0.0   8.56  0.0  0.5200  6.137  87.4  2.7147   5.0  384.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    19.1  372.49  12.50  \n",
       "1    18.6  393.24   8.05  \n",
       "2    20.2  380.79  10.19  \n",
       "3    21.0  380.02  10.26  \n",
       "4    18.2  396.90  10.59  \n",
       "..    ...     ...    ...  \n",
       "162  18.5  395.15   8.44  \n",
       "163  20.2  291.55  14.10  \n",
       "164  19.7  396.90   9.22  \n",
       "165  16.1  390.43   4.86  \n",
       "166  20.9  394.47  13.44  \n",
       "\n",
       "[167 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 표준화 수행\n",
    "- 스케일이 서로 다를 경우 신경망의 성능에 큰 영향을 주기 때문에 표준화를 수행해야 한다.\n",
    "- 표준화 -> 각 값에 대해 특성(DF의 컬럼)의 평균을 때고 표준편차로 나누는 처리\n",
    "- 표준화는 특성의 평균을 0으로, 표준편차를 1로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 구하기\n",
    "mean = np.mean(x_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준편차 구하기\n",
    "std = np.std(x_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 표준화\n",
    "std_x_train = (x_train - mean) / std\n",
    "\n",
    "# 검증 데이터 표준화\n",
    "std_x_test = (x_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.398157</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.114676</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.572882</td>\n",
       "      <td>-0.714780</td>\n",
       "      <td>0.135436</td>\n",
       "      <td>0.258581</td>\n",
       "      <td>-0.637411</td>\n",
       "      <td>-0.789973</td>\n",
       "      <td>0.107780</td>\n",
       "      <td>0.384607</td>\n",
       "      <td>0.703446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.352052</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>0.994262</td>\n",
       "      <td>-0.554323</td>\n",
       "      <td>0.939305</td>\n",
       "      <td>-0.871380</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>-3.607039</td>\n",
       "      <td>1.563816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.403808</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>0.317915</td>\n",
       "      <td>-0.950494</td>\n",
       "      <td>1.027837</td>\n",
       "      <td>-1.081587</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>-0.397411</td>\n",
       "      <td>1.837015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.412229</td>\n",
       "      <td>0.556941</td>\n",
       "      <td>-0.903942</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.869815</td>\n",
       "      <td>0.675368</td>\n",
       "      <td>-0.902158</td>\n",
       "      <td>1.934138</td>\n",
       "      <td>-0.181541</td>\n",
       "      <td>-0.748424</td>\n",
       "      <td>0.607672</td>\n",
       "      <td>0.448912</td>\n",
       "      <td>-0.460025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.422487</td>\n",
       "      <td>2.866843</td>\n",
       "      <td>-0.929962</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.216236</td>\n",
       "      <td>0.815945</td>\n",
       "      <td>-1.451056</td>\n",
       "      <td>0.610605</td>\n",
       "      <td>-0.637411</td>\n",
       "      <td>-0.979912</td>\n",
       "      <td>0.380448</td>\n",
       "      <td>0.462715</td>\n",
       "      <td>-1.298649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>-0.421010</td>\n",
       "      <td>2.866843</td>\n",
       "      <td>-1.119328</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.372951</td>\n",
       "      <td>-0.253290</td>\n",
       "      <td>-1.305864</td>\n",
       "      <td>2.506859</td>\n",
       "      <td>-0.979314</td>\n",
       "      <td>-0.564422</td>\n",
       "      <td>-0.892003</td>\n",
       "      <td>0.420785</td>\n",
       "      <td>-0.858270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>-0.110380</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.218213</td>\n",
       "      <td>-0.744599</td>\n",
       "      <td>-1.011938</td>\n",
       "      <td>0.140008</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>0.421099</td>\n",
       "      <td>-0.334980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-0.422647</td>\n",
       "      <td>2.656852</td>\n",
       "      <td>-1.219071</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.076018</td>\n",
       "      <td>1.047399</td>\n",
       "      <td>-1.879550</td>\n",
       "      <td>0.742025</td>\n",
       "      <td>-0.751379</td>\n",
       "      <td>-0.938363</td>\n",
       "      <td>-0.028554</td>\n",
       "      <td>0.449331</td>\n",
       "      <td>-1.482140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.216240</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.218213</td>\n",
       "      <td>1.099938</td>\n",
       "      <td>0.287710</td>\n",
       "      <td>-0.177775</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>0.445776</td>\n",
       "      <td>-0.798465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>-0.423353</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.623507</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.737845</td>\n",
       "      <td>1.276014</td>\n",
       "      <td>-0.275352</td>\n",
       "      <td>0.541476</td>\n",
       "      <td>-0.865346</td>\n",
       "      <td>-0.997718</td>\n",
       "      <td>-0.255777</td>\n",
       "      <td>0.420158</td>\n",
       "      <td>-1.203505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>339 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.398157 -0.493015 -0.114676 -0.311588 -0.572882 -0.714780  0.135436   \n",
       "1    1.352052 -0.493015  0.970928 -0.311588  0.994262 -0.554323  0.939305   \n",
       "2    0.403808 -0.493015  0.970928 -0.311588  0.317915 -0.950494  1.027837   \n",
       "3   -0.412229  0.556941 -0.903942 -0.311588 -0.869815  0.675368 -0.902158   \n",
       "4   -0.422487  2.866843 -0.929962 -0.311588 -1.216236  0.815945 -1.451056   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "334 -0.421010  2.866843 -1.119328 -0.311588 -1.372951 -0.253290 -1.305864   \n",
       "335 -0.110380 -0.493015  0.970928 -0.311588 -0.218213 -0.744599 -1.011938   \n",
       "336 -0.422647  2.656852 -1.219071 -0.311588 -1.076018  1.047399 -1.879550   \n",
       "337  0.216240 -0.493015  0.970928 -0.311588 -0.218213  1.099938  0.287710   \n",
       "338 -0.423353 -0.493015 -0.623507 -0.311588 -0.737845  1.276014 -0.275352   \n",
       "\n",
       "           7         8         9         10        11        12  \n",
       "0    0.258581 -0.637411 -0.789973  0.107780  0.384607  0.703446  \n",
       "1   -0.871380  1.641939  1.518963  0.834895 -3.607039  1.563816  \n",
       "2   -1.081587  1.641939  1.518963  0.834895 -0.397411  1.837015  \n",
       "3    1.934138 -0.181541 -0.748424  0.607672  0.448912 -0.460025  \n",
       "4    0.610605 -0.637411 -0.979912  0.380448  0.462715 -1.298649  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "334  2.506859 -0.979314 -0.564422 -0.892003  0.420785 -0.858270  \n",
       "335  0.140008  1.641939  1.518963  0.834895  0.421099 -0.334980  \n",
       "336  0.742025 -0.751379 -0.938363 -0.028554  0.449331 -1.482140  \n",
       "337 -0.177775  1.641939  1.518963  0.834895  0.445776 -0.798465  \n",
       "338  0.541476 -0.865346 -0.997718 -0.255777  0.420158 -1.203505  \n",
       "\n",
       "[339 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습데이터 표준화 확인\n",
    "DataFrame(std_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.403305</td>\n",
       "      <td>0.430946</td>\n",
       "      <td>-0.798418</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.051274</td>\n",
       "      <td>-0.984573</td>\n",
       "      <td>0.270004</td>\n",
       "      <td>1.922124</td>\n",
       "      <td>-0.295509</td>\n",
       "      <td>-0.475388</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.207479</td>\n",
       "      <td>-0.052267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.424226</td>\n",
       "      <td>0.241954</td>\n",
       "      <td>-1.446021</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.174171</td>\n",
       "      <td>1.160997</td>\n",
       "      <td>-0.332013</td>\n",
       "      <td>2.508060</td>\n",
       "      <td>-0.751379</td>\n",
       "      <td>-1.152043</td>\n",
       "      <td>0.107780</td>\n",
       "      <td>0.424445</td>\n",
       "      <td>-0.657109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013963</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>1.744841</td>\n",
       "      <td>-1.312586</td>\n",
       "      <td>0.967635</td>\n",
       "      <td>-0.781733</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>0.294266</td>\n",
       "      <td>-0.366241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.354877</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.468834</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.168724</td>\n",
       "      <td>-0.270330</td>\n",
       "      <td>0.553306</td>\n",
       "      <td>0.308026</td>\n",
       "      <td>-0.637411</td>\n",
       "      <td>-0.611906</td>\n",
       "      <td>1.198453</td>\n",
       "      <td>0.286214</td>\n",
       "      <td>-0.356727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.421599</td>\n",
       "      <td>0.682936</td>\n",
       "      <td>0.528591</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.779085</td>\n",
       "      <td>-0.053075</td>\n",
       "      <td>0.298334</td>\n",
       "      <td>-0.083323</td>\n",
       "      <td>-0.637411</td>\n",
       "      <td>-0.831522</td>\n",
       "      <td>-0.073998</td>\n",
       "      <td>0.462715</td>\n",
       "      <td>-0.311873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-0.418395</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.996457</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.902807</td>\n",
       "      <td>-0.234831</td>\n",
       "      <td>-0.427627</td>\n",
       "      <td>-0.022049</td>\n",
       "      <td>-0.751379</td>\n",
       "      <td>-0.968040</td>\n",
       "      <td>0.062336</td>\n",
       "      <td>0.444416</td>\n",
       "      <td>-0.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.924589</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>0.970928</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>-0.906475</td>\n",
       "      <td>0.663086</td>\n",
       "      <td>-0.852157</td>\n",
       "      <td>1.641939</td>\n",
       "      <td>1.518963</td>\n",
       "      <td>0.834895</td>\n",
       "      <td>-0.638844</td>\n",
       "      <td>0.165204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.414832</td>\n",
       "      <td>0.556941</td>\n",
       "      <td>-0.903942</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.869815</td>\n",
       "      <td>-0.510304</td>\n",
       "      <td>-0.767590</td>\n",
       "      <td>1.449447</td>\n",
       "      <td>-0.181541</td>\n",
       "      <td>-0.748424</td>\n",
       "      <td>0.607672</td>\n",
       "      <td>0.462715</td>\n",
       "      <td>-0.498083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-0.415200</td>\n",
       "      <td>0.934925</td>\n",
       "      <td>-0.765170</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-1.034777</td>\n",
       "      <td>0.987761</td>\n",
       "      <td>-1.812266</td>\n",
       "      <td>0.783891</td>\n",
       "      <td>-0.295509</td>\n",
       "      <td>-0.481324</td>\n",
       "      <td>-1.028337</td>\n",
       "      <td>0.395063</td>\n",
       "      <td>-1.090692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>-0.402685</td>\n",
       "      <td>-0.493015</td>\n",
       "      <td>-0.408121</td>\n",
       "      <td>-0.311588</td>\n",
       "      <td>-0.317190</td>\n",
       "      <td>-0.212111</td>\n",
       "      <td>0.656003</td>\n",
       "      <td>-0.499347</td>\n",
       "      <td>-0.523444</td>\n",
       "      <td>-0.154868</td>\n",
       "      <td>1.153008</td>\n",
       "      <td>0.437306</td>\n",
       "      <td>0.075497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.403305  0.430946 -0.798418 -0.311588 -1.051274 -0.984573  0.270004   \n",
       "1   -0.424226  0.241954 -1.446021 -0.311588 -1.174171  1.160997 -0.332013   \n",
       "2   -0.013963 -0.493015  0.970928 -0.311588  1.744841 -1.312586  0.967635   \n",
       "3   -0.354877 -0.493015 -0.468834 -0.311588 -0.168724 -0.270330  0.553306   \n",
       "4   -0.421599  0.682936  0.528591 -0.311588 -0.779085 -0.053075  0.298334   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "162 -0.418395 -0.493015 -0.996457 -0.311588 -0.902807 -0.234831 -0.427627   \n",
       "163  0.924589 -0.493015  0.970928 -0.311588  0.458134 -0.906475  0.663086   \n",
       "164 -0.414832  0.556941 -0.903942 -0.311588 -0.869815 -0.510304 -0.767590   \n",
       "165 -0.415200  0.934925 -0.765170 -0.311588 -1.034777  0.987761 -1.812266   \n",
       "166 -0.402685 -0.493015 -0.408121 -0.311588 -0.317190 -0.212111  0.656003   \n",
       "\n",
       "           7         8         9         10        11        12  \n",
       "0    1.922124 -0.295509 -0.475388  0.335004  0.207479 -0.052267  \n",
       "1    2.508060 -0.751379 -1.152043  0.107780  0.424445 -0.657109  \n",
       "2   -0.781733  1.641939  1.518963  0.834895  0.294266 -0.366241  \n",
       "3    0.308026 -0.637411 -0.611906  1.198453  0.286214 -0.356727  \n",
       "4   -0.083323 -0.637411 -0.831522 -0.073998  0.462715 -0.311873  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "162 -0.022049 -0.751379 -0.968040  0.062336  0.444416 -0.604100  \n",
       "163 -0.852157  1.641939  1.518963  0.834895 -0.638844  0.165204  \n",
       "164  1.449447 -0.181541 -0.748424  0.607672  0.462715 -0.498083  \n",
       "165  0.783891 -0.295509 -0.481324 -1.028337  0.395063 -1.090692  \n",
       "166 -0.499347 -0.523444 -0.154868  1.153008  0.437306  0.075497  \n",
       "\n",
       "[167 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증 데이터 표준화 확인\n",
    "DataFrame(std_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할할 폴드 수\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\youjin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 주어진 데이터셋을 k만큼 등분\n",
    "# 여기서는 3이므로 훈련 데이터셋(404개)를 3등분하여 \n",
    "# 1개는 검증셋으로, 나머지는 훈련셋으로 활용\n",
    "kfold = KFold(n_splits = k, random_state = 777)\n",
    "\n",
    "# 흠.. 워닝에 shuffle을 True로 해야 random_state가 의미있다고 하는데.. 그럼 굳이 k-fold를 할 필요가 있나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련셋 인덱스: [113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n",
      " 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
      " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
      " 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328\n",
      " 329 330 331 332 333 334 335 336 337 338]\n",
      "검증셋 인덱스: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112]\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 601.4412 - mae: 22.5437 - val_loss: 566.1795 - val_mae: 21.9389\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 576.6249 - mae: 21.9639 - val_loss: 545.2610 - val_mae: 21.4173\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 552.9473 - mae: 21.3807 - val_loss: 523.5261 - val_mae: 20.8665\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 527.4464 - mae: 20.7413 - val_loss: 498.6050 - val_mae: 20.2151\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 497.9426 - mae: 19.9678 - val_loss: 468.6682 - val_mae: 19.4304\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 464.2434 - mae: 19.0760 - val_loss: 433.8127 - val_mae: 18.5026\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 423.9059 - mae: 18.0209 - val_loss: 393.9465 - val_mae: 17.4085\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 379.7135 - mae: 16.8410 - val_loss: 350.1576 - val_mae: 16.1424\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 331.5971 - mae: 15.5419 - val_loss: 303.4774 - val_mae: 14.8052\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 280.6696 - mae: 14.1463 - val_loss: 253.3433 - val_mae: 13.3759\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 229.4972 - mae: 12.6482 - val_loss: 203.8116 - val_mae: 11.8762\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 176.7746 - mae: 10.9986 - val_loss: 156.4877 - val_mae: 10.2477\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 131.9989 - mae: 9.2302 - val_loss: 112.3869 - val_mae: 8.4724\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 93.4973 - mae: 7.5074 - val_loss: 81.3960 - val_mae: 6.9843\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 69.3832 - mae: 6.1873 - val_loss: 61.5916 - val_mae: 5.9073\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 55.0776 - mae: 5.4277 - val_loss: 50.1936 - val_mae: 5.1599\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 47.6200 - mae: 5.1352 - val_loss: 42.3756 - val_mae: 4.6272\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 42.2945 - mae: 4.9346 - val_loss: 37.3137 - val_mae: 4.4269\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 37.9645 - mae: 4.6784 - val_loss: 34.1741 - val_mae: 4.2814\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 34.6179 - mae: 4.4095 - val_loss: 31.8246 - val_mae: 4.1571\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 31.9010 - mae: 4.1601 - val_loss: 30.4188 - val_mae: 4.0740\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 29.9855 - mae: 3.9463 - val_loss: 29.5295 - val_mae: 3.9989\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 28.3976 - mae: 3.7596 - val_loss: 28.6311 - val_mae: 3.9332\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 27.1506 - mae: 3.6346 - val_loss: 27.6604 - val_mae: 3.8750\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 25.8596 - mae: 3.5692 - val_loss: 26.6592 - val_mae: 3.8367\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 24.8184 - mae: 3.5034 - val_loss: 25.9836 - val_mae: 3.8079\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23.9462 - mae: 3.4419 - val_loss: 25.4886 - val_mae: 3.8060\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23.1074 - mae: 3.3924 - val_loss: 25.2000 - val_mae: 3.7781\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 22.3374 - mae: 3.3116 - val_loss: 25.0069 - val_mae: 3.7474\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21.8308 - mae: 3.2480 - val_loss: 24.8125 - val_mae: 3.7113\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21.2496 - mae: 3.1724 - val_loss: 24.6288 - val_mae: 3.6796\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 20.7068 - mae: 3.1189 - val_loss: 24.3962 - val_mae: 3.6489\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 20.2865 - mae: 3.0902 - val_loss: 24.0630 - val_mae: 3.6173\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 19.8353 - mae: 3.0591 - val_loss: 23.8770 - val_mae: 3.6028\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 19.4706 - mae: 3.0229 - val_loss: 23.6597 - val_mae: 3.5879\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 19.1985 - mae: 2.9975 - val_loss: 23.3628 - val_mae: 3.5751\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 18.7732 - mae: 2.9661 - val_loss: 23.1928 - val_mae: 3.5639\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 18.4370 - mae: 2.9286 - val_loss: 23.1197 - val_mae: 3.5575\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 18.1746 - mae: 2.8893 - val_loss: 23.0769 - val_mae: 3.5459\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.8969 - mae: 2.8515 - val_loss: 23.0681 - val_mae: 3.5306\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.7387 - mae: 2.8217 - val_loss: 23.0569 - val_mae: 3.5165\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.5031 - mae: 2.8291 - val_loss: 22.9946 - val_mae: 3.5209\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 17.3280 - mae: 2.8401 - val_loss: 22.8813 - val_mae: 3.5162\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 17.0538 - mae: 2.8153 - val_loss: 22.5703 - val_mae: 3.4893\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16.7102 - mae: 2.7691 - val_loss: 22.3257 - val_mae: 3.4763\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16.5080 - mae: 2.7469 - val_loss: 22.1797 - val_mae: 3.4588\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16.2739 - mae: 2.7177 - val_loss: 22.0773 - val_mae: 3.4376\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 16.1233 - mae: 2.7041 - val_loss: 21.9343 - val_mae: 3.4273\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.9758 - mae: 2.6748 - val_loss: 22.1323 - val_mae: 3.4433\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.8165 - mae: 2.6493 - val_loss: 22.0929 - val_mae: 3.4400\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.6227 - mae: 2.6359 - val_loss: 21.8501 - val_mae: 3.4284\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.3589 - mae: 2.6302 - val_loss: 21.6465 - val_mae: 3.4184\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.1994 - mae: 2.6290 - val_loss: 21.5223 - val_mae: 3.4082\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15.1058 - mae: 2.6307 - val_loss: 21.6391 - val_mae: 3.4199\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15.0035 - mae: 2.6359 - val_loss: 21.4205 - val_mae: 3.3978\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.7790 - mae: 2.5887 - val_loss: 21.1618 - val_mae: 3.3652\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14.5397 - mae: 2.5581 - val_loss: 21.1585 - val_mae: 3.3564\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.4345 - mae: 2.5550 - val_loss: 21.0042 - val_mae: 3.3612\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.5310 - mae: 2.5917 - val_loss: 21.3163 - val_mae: 3.3970\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.5468 - mae: 2.6267 - val_loss: 21.3470 - val_mae: 3.3915\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.5043 - mae: 2.6150 - val_loss: 21.3974 - val_mae: 3.3883\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.3378 - mae: 2.5882 - val_loss: 21.1692 - val_mae: 3.3664\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.0771 - mae: 2.5580 - val_loss: 20.9127 - val_mae: 3.3321\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 13.9006 - mae: 2.5380 - val_loss: 20.4510 - val_mae: 3.3233\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - ETA: 0s - loss: 7.4215 - mae: 2.315 - 0s 6ms/step - loss: 13.7022 - mae: 2.5919 - val_loss: 20.5654 - val_mae: 3.4423\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 15.3334 - mae: 2.8088 - val_loss: 20.9520 - val_mae: 3.5219\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15.1080 - mae: 2.7838 - val_loss: 19.8121 - val_mae: 3.3804\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.4187 - mae: 2.5830 - val_loss: 19.4596 - val_mae: 3.3214\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.7828 - mae: 2.5001 - val_loss: 19.5314 - val_mae: 3.2964\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.7220 - mae: 2.4953 - val_loss: 19.6562 - val_mae: 3.2945\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.8583 - mae: 2.5181 - val_loss: 19.7191 - val_mae: 3.3178\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 12.5735 - mae: 2.4916 - val_loss: 19.4772 - val_mae: 3.3171\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12.3899 - mae: 2.4640 - val_loss: 19.3684 - val_mae: 3.3178\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 12.2695 - mae: 2.4493 - val_loss: 19.2051 - val_mae: 3.2990\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 12.5332 - mae: 2.4626 - val_loss: 19.0467 - val_mae: 3.2498\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12.3929 - mae: 2.4550 - val_loss: 18.6631 - val_mae: 3.2350\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12.1059 - mae: 2.4360 - val_loss: 18.3793 - val_mae: 3.2379\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11.9210 - mae: 2.4240 - val_loss: 18.3089 - val_mae: 3.2260\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11.8263 - mae: 2.4076 - val_loss: 18.2935 - val_mae: 3.2192\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11.7331 - mae: 2.3940 - val_loss: 18.4154 - val_mae: 3.1967\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11.6721 - mae: 2.3725 - val_loss: 18.7767 - val_mae: 3.1732\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11.8317 - mae: 2.3680 - val_loss: 18.6490 - val_mae: 3.1884\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 11.6079 - mae: 2.3554 - val_loss: 18.4461 - val_mae: 3.2229\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 11.4252 - mae: 2.3553 - val_loss: 18.2832 - val_mae: 3.2383\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11.3429 - mae: 2.3520 - val_loss: 18.2092 - val_mae: 3.2345\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 11.3414 - mae: 2.3516 - val_loss: 18.1923 - val_mae: 3.2315\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.2767 - mae: 2.3381 - val_loss: 18.3935 - val_mae: 3.2611\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 11.1007 - mae: 2.3275 - val_loss: 18.4510 - val_mae: 3.2427\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 11.0809 - mae: 2.3236 - val_loss: 18.4367 - val_mae: 3.2380\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 11.0241 - mae: 2.3257 - val_loss: 18.2794 - val_mae: 3.2476\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.8793 - mae: 2.3080 - val_loss: 18.0314 - val_mae: 3.2243\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 10.8819 - mae: 2.2987 - val_loss: 17.8278 - val_mae: 3.2124\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 10.7883 - mae: 2.2998 - val_loss: 17.8134 - val_mae: 3.2284\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.8620 - mae: 2.3284 - val_loss: 17.7937 - val_mae: 3.2171\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.7272 - mae: 2.2880 - val_loss: 17.6995 - val_mae: 3.1625\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.6171 - mae: 2.2670 - val_loss: 17.8202 - val_mae: 3.1472\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 10.6642 - mae: 2.2627 - val_loss: 17.7137 - val_mae: 3.1627\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 10.6027 - mae: 2.2800 - val_loss: 17.7456 - val_mae: 3.2739\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 11.0883 - mae: 2.3582 - val_loss: 17.6296 - val_mae: 3.2802\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.8961 - mae: 2.3641 - val_loss: 17.7970 - val_mae: 3.2781\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.5482 - mae: 2.3117 - val_loss: 17.6253 - val_mae: 3.1881\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.3173 - mae: 2.2749 - val_loss: 17.6586 - val_mae: 3.1686\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.2386 - mae: 2.2638 - val_loss: 17.6141 - val_mae: 3.1664\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.1485 - mae: 2.2557 - val_loss: 17.4648 - val_mae: 3.1691\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.0060 - mae: 2.2395 - val_loss: 17.3861 - val_mae: 3.1719\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.8889 - mae: 2.2252 - val_loss: 17.4159 - val_mae: 3.2011\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.8058 - mae: 2.2161 - val_loss: 17.4076 - val_mae: 3.2051\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.7551 - mae: 2.2121 - val_loss: 17.4738 - val_mae: 3.1942\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.9729 - mae: 2.2205 - val_loss: 17.8018 - val_mae: 3.1861\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.0155 - mae: 2.2113 - val_loss: 17.6723 - val_mae: 3.1922\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.8281 - mae: 2.1946 - val_loss: 17.4395 - val_mae: 3.1902\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.7420 - mae: 2.1839 - val_loss: 17.3478 - val_mae: 3.1947\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.6677 - mae: 2.1949 - val_loss: 17.4064 - val_mae: 3.2160\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.6006 - mae: 2.2094 - val_loss: 17.4187 - val_mae: 3.2030\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.5663 - mae: 2.2107 - val_loss: 17.2943 - val_mae: 3.2010\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.6331 - mae: 2.2050 - val_loss: 17.5562 - val_mae: 3.1893\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.6424 - mae: 2.1944 - val_loss: 17.3692 - val_mae: 3.2169\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.4156 - mae: 2.1820 - val_loss: 17.2948 - val_mae: 3.2536\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.4443 - mae: 2.3222 - val_loss: 17.9926 - val_mae: 3.3621\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.5163 - mae: 2.2955 - val_loss: 17.3404 - val_mae: 3.2699\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.8709 - mae: 2.2449 - val_loss: 17.4433 - val_mae: 3.2249\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.4012 - mae: 2.1992 - val_loss: 17.7681 - val_mae: 3.2759\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.5710 - mae: 2.2417 - val_loss: 17.7212 - val_mae: 3.2835\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.3047 - mae: 2.2009 - val_loss: 17.4531 - val_mae: 3.2337\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 9.0866 - mae: 2.1712 - val_loss: 17.1667 - val_mae: 3.2006\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.9577 - mae: 2.1439 - val_loss: 17.1011 - val_mae: 3.1876\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.9910 - mae: 2.1432 - val_loss: 17.0930 - val_mae: 3.1919\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 8.8541 - mae: 2.1253 - val_loss: 17.1065 - val_mae: 3.2237\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.8040 - mae: 2.1230 - val_loss: 17.2156 - val_mae: 3.2617\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.9339 - mae: 2.1571 - val_loss: 17.1601 - val_mae: 3.2525\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.8013 - mae: 2.1274 - val_loss: 16.9571 - val_mae: 3.1984\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.6568 - mae: 2.0941 - val_loss: 16.9425 - val_mae: 3.1781\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.6587 - mae: 2.0961 - val_loss: 17.3211 - val_mae: 3.2316\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.6774 - mae: 2.0938 - val_loss: 17.2783 - val_mae: 3.2262\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.5522 - mae: 2.0788 - val_loss: 17.3272 - val_mae: 3.2411\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.6406 - mae: 2.0964 - val_loss: 17.2668 - val_mae: 3.2324\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 8.5872 - mae: 2.0891 - val_loss: 17.1773 - val_mae: 3.2372\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 8.5059 - mae: 2.0701 - val_loss: 17.0407 - val_mae: 3.2080\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.3571 - mae: 2.0554 - val_loss: 16.9303 - val_mae: 3.1878\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.3054 - mae: 2.0511 - val_loss: 16.8426 - val_mae: 3.1900\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.2381 - mae: 2.0481 - val_loss: 16.7355 - val_mae: 3.1675\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.3050 - mae: 2.0565 - val_loss: 16.6649 - val_mae: 3.1529\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.2427 - mae: 2.0535 - val_loss: 16.5094 - val_mae: 3.1534\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.1550 - mae: 2.0436 - val_loss: 16.5506 - val_mae: 3.1726\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.3063 - mae: 2.1001 - val_loss: 16.6751 - val_mae: 3.2087\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.1933 - mae: 2.0659 - val_loss: 16.3822 - val_mae: 3.1240\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 8.7990 - mae: 2.0708 - val_loss: 16.9256 - val_mae: 3.0654\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.5335 - mae: 2.1424 - val_loss: 16.9924 - val_mae: 3.0942\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.5551 - mae: 2.1505 - val_loss: 16.8819 - val_mae: 3.1501\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.2746 - mae: 2.1635 - val_loss: 16.9114 - val_mae: 3.2006\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.7893 - mae: 2.1459 - val_loss: 16.8087 - val_mae: 3.1979\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.4485 - mae: 2.1017 - val_loss: 16.6690 - val_mae: 3.1831\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.1635 - mae: 2.0603 - val_loss: 16.6260 - val_mae: 3.1671\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.0064 - mae: 2.0146 - val_loss: 16.6106 - val_mae: 3.1461\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.8410 - mae: 2.0140 - val_loss: 16.5591 - val_mae: 3.1712\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.8312 - mae: 2.0277 - val_loss: 16.5736 - val_mae: 3.2091\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.6643 - mae: 2.0026 - val_loss: 16.4756 - val_mae: 3.1730\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.6666 - mae: 2.0060 - val_loss: 16.4317 - val_mae: 3.1929\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.6281 - mae: 2.0015 - val_loss: 16.3713 - val_mae: 3.2054\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.5104 - mae: 1.9865 - val_loss: 16.3352 - val_mae: 3.1951\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.4718 - mae: 1.9810 - val_loss: 16.3227 - val_mae: 3.1921\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.4064 - mae: 1.9628 - val_loss: 16.2888 - val_mae: 3.1664\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.4673 - mae: 1.9805 - val_loss: 16.2630 - val_mae: 3.1767\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.4693 - mae: 1.9846 - val_loss: 16.1810 - val_mae: 3.1744\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.3047 - mae: 1.9551 - val_loss: 16.2650 - val_mae: 3.1848\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.3390 - mae: 1.9618 - val_loss: 16.1493 - val_mae: 3.1846\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.3155 - mae: 1.9710 - val_loss: 16.0704 - val_mae: 3.1883\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.3485 - mae: 1.9685 - val_loss: 15.9589 - val_mae: 3.1403\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2896 - mae: 1.9554 - val_loss: 15.8648 - val_mae: 3.1329\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.1891 - mae: 1.9291 - val_loss: 15.8525 - val_mae: 3.1363\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.1792 - mae: 1.9483 - val_loss: 15.9463 - val_mae: 3.1500\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - ETA: 0s - loss: 5.1250 - mae: 1.890 - 0s 6ms/step - loss: 7.1767 - mae: 1.9490 - val_loss: 15.8305 - val_mae: 3.1252\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.0967 - mae: 1.9225 - val_loss: 15.7910 - val_mae: 3.1068\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2317 - mae: 1.9445 - val_loss: 15.7818 - val_mae: 3.0794\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2287 - mae: 1.9484 - val_loss: 15.6994 - val_mae: 3.0854\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2394 - mae: 1.9623 - val_loss: 15.8731 - val_mae: 3.1154\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2187 - mae: 1.9620 - val_loss: 15.7641 - val_mae: 3.1271\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.8830 - mae: 1.9116 - val_loss: 15.4978 - val_mae: 3.1189\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.0777 - mae: 1.9351 - val_loss: 15.5534 - val_mae: 3.1429\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.0938 - mae: 1.9379 - val_loss: 15.5666 - val_mae: 3.1500\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.0766 - mae: 1.9460 - val_loss: 15.5555 - val_mae: 3.1161\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.8558 - mae: 1.9025 - val_loss: 15.6287 - val_mae: 3.1429\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8187 - mae: 1.9027 - val_loss: 15.6362 - val_mae: 3.1456\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 6.7780 - mae: 1.8897 - val_loss: 15.6509 - val_mae: 3.1223\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.9342 - mae: 1.8923 - val_loss: 15.8137 - val_mae: 3.0972\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.8605 - mae: 1.9102 - val_loss: 15.5755 - val_mae: 3.0952\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.0166 - mae: 1.9318 - val_loss: 15.6429 - val_mae: 3.0732\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9684 - mae: 1.9216 - val_loss: 15.6020 - val_mae: 3.1045\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.0014 - mae: 1.9829 - val_loss: 16.2102 - val_mae: 3.2169\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.9762 - mae: 1.9942 - val_loss: 15.8760 - val_mae: 3.1843\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7290 - mae: 1.8993 - val_loss: 15.6703 - val_mae: 3.1035\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.6620 - mae: 1.8823 - val_loss: 15.6025 - val_mae: 3.1004\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.5602 - mae: 1.8421 - val_loss: 15.7023 - val_mae: 3.0944\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7601 - mae: 1.8792 - val_loss: 15.8198 - val_mae: 3.1059\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.5494 - mae: 1.8769 - val_loss: 15.8997 - val_mae: 3.1512\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.5950 - mae: 1.9296 - val_loss: 16.1919 - val_mae: 3.1948\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.7647 - mae: 1.9623 - val_loss: 16.1703 - val_mae: 3.1880\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.5349 - mae: 1.9002 - val_loss: 15.8846 - val_mae: 3.1296\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.4743 - mae: 1.8766 - val_loss: 15.8386 - val_mae: 3.1023\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.4154 - mae: 1.8560 - val_loss: 15.7496 - val_mae: 3.1013\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.5864 - mae: 1.8902 - val_loss: 15.6681 - val_mae: 3.0844\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.5295 - mae: 1.8827 - val_loss: 15.6013 - val_mae: 3.1259\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.4171 - mae: 1.8675 - val_loss: 15.6106 - val_mae: 3.1458\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.3208 - mae: 1.8610 - val_loss: 15.5167 - val_mae: 3.1080\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.2193 - mae: 1.8345 - val_loss: 15.5249 - val_mae: 3.1008\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.1720 - mae: 1.8259 - val_loss: 15.5435 - val_mae: 3.1158\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.0885 - mae: 1.8153 - val_loss: 15.4899 - val_mae: 3.1225\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.0986 - mae: 1.8173 - val_loss: 15.4755 - val_mae: 3.1223\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0897 - mae: 1.8190 - val_loss: 15.5085 - val_mae: 3.1245\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0374 - mae: 1.8065 - val_loss: 15.5669 - val_mae: 3.0854\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2614 - mae: 1.8447 - val_loss: 15.6676 - val_mae: 3.0956\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.1557 - mae: 1.8449 - val_loss: 15.6182 - val_mae: 3.1165\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.0001 - mae: 1.8146 - val_loss: 15.7114 - val_mae: 3.1600\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0360 - mae: 1.8237 - val_loss: 15.8010 - val_mae: 3.1716\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0110 - mae: 1.8023 - val_loss: 15.6666 - val_mae: 3.1127\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0757 - mae: 1.7960 - val_loss: 15.6537 - val_mae: 3.0902\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.9619 - mae: 1.7798 - val_loss: 15.5836 - val_mae: 3.0995\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.9021 - mae: 1.7769 - val_loss: 15.5596 - val_mae: 3.1213\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.8808 - mae: 1.7842 - val_loss: 15.5515 - val_mae: 3.1185\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.9448 - mae: 1.7922 - val_loss: 15.4392 - val_mae: 3.1188\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.8277 - mae: 1.7768 - val_loss: 15.4229 - val_mae: 3.1153\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.7628 - mae: 1.7699 - val_loss: 15.5337 - val_mae: 3.1261\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.0574 - mae: 1.8135 - val_loss: 15.6750 - val_mae: 3.0769\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.2799 - mae: 1.8084 - val_loss: 15.5335 - val_mae: 3.0647\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0006 - mae: 1.7895 - val_loss: 15.6333 - val_mae: 3.1228\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.9078 - mae: 1.8071 - val_loss: 15.7836 - val_mae: 3.1735\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.8755 - mae: 1.7945 - val_loss: 15.7420 - val_mae: 3.1655\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.7009 - mae: 1.7613 - val_loss: 15.6303 - val_mae: 3.1412\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - ETA: 0s - loss: 6.0528 - mae: 1.794 - 0s 4ms/step - loss: 5.6644 - mae: 1.7337 - val_loss: 15.5244 - val_mae: 3.1054\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6324 - mae: 1.7268 - val_loss: 15.4837 - val_mae: 3.1027\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6065 - mae: 1.7213 - val_loss: 15.5156 - val_mae: 3.0977\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.5889 - mae: 1.7221 - val_loss: 15.4943 - val_mae: 3.1048\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5333 - mae: 1.7140 - val_loss: 15.3916 - val_mae: 3.0849\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.5414 - mae: 1.7097 - val_loss: 15.3664 - val_mae: 3.0977\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6383 - mae: 1.7434 - val_loss: 15.5053 - val_mae: 3.1295\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.5632 - mae: 1.7291 - val_loss: 15.4282 - val_mae: 3.0894\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.5053 - mae: 1.7090 - val_loss: 15.2920 - val_mae: 3.0665\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.4931 - mae: 1.6886 - val_loss: 15.1616 - val_mae: 3.0505\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.5621 - mae: 1.7145 - val_loss: 15.1931 - val_mae: 3.0982\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.6004 - mae: 1.7444 - val_loss: 15.2551 - val_mae: 3.1339\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.5823 - mae: 1.7510 - val_loss: 15.1020 - val_mae: 3.0972\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.3748 - mae: 1.6872 - val_loss: 15.0640 - val_mae: 3.0407\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.4385 - mae: 1.6911 - val_loss: 15.0798 - val_mae: 3.0344\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.3547 - mae: 1.6887 - val_loss: 15.1708 - val_mae: 3.0631\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.2904 - mae: 1.6710 - val_loss: 15.1646 - val_mae: 3.0462\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.3260 - mae: 1.6836 - val_loss: 15.1368 - val_mae: 3.0598\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.2797 - mae: 1.6805 - val_loss: 15.1705 - val_mae: 3.0813\n",
      "Epoch 248/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.2391 - mae: 1.6754 - val_loss: 15.1027 - val_mae: 3.0691\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.2469 - mae: 1.6678 - val_loss: 15.0710 - val_mae: 3.0796\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.3091 - mae: 1.6802 - val_loss: 15.1790 - val_mae: 3.0851\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.4888 - mae: 1.7235 - val_loss: 15.4673 - val_mae: 3.0851\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.6645 - mae: 1.7483 - val_loss: 15.2663 - val_mae: 3.0954\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5.4356 - mae: 1.7267 - val_loss: 15.2013 - val_mae: 3.1148\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.3899 - mae: 1.6933 - val_loss: 15.1858 - val_mae: 3.1142\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 5.3305 - mae: 1.7074 - val_loss: 15.0879 - val_mae: 3.0870\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.1987 - mae: 1.6656 - val_loss: 14.8111 - val_mae: 3.0254\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.1429 - mae: 1.6502 - val_loss: 14.8270 - val_mae: 2.9962\n",
      "Epoch 258/300\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 5.1413 - mae: 1.6640 - val_loss: 15.0465 - val_mae: 3.0324\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.2569 - mae: 1.7024 - val_loss: 14.9839 - val_mae: 3.0265\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.1514 - mae: 1.6724 - val_loss: 14.8094 - val_mae: 3.0125\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 5.0793 - mae: 1.6515 - val_loss: 14.9518 - val_mae: 3.0503\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 5.1311 - mae: 1.6669 - val_loss: 15.0321 - val_mae: 3.0741\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.0513 - mae: 1.6549 - val_loss: 14.9857 - val_mae: 3.0503\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.9962 - mae: 1.6345 - val_loss: 15.0083 - val_mae: 3.0504\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.9954 - mae: 1.6317 - val_loss: 15.0375 - val_mae: 3.0702\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 4.9800 - mae: 1.6324 - val_loss: 14.9668 - val_mae: 3.0451\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.0426 - mae: 1.6372 - val_loss: 14.8554 - val_mae: 3.0315\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.0099 - mae: 1.6516 - val_loss: 15.0600 - val_mae: 3.0785\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.0544 - mae: 1.6511 - val_loss: 14.9243 - val_mae: 3.0651\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.9542 - mae: 1.6339 - val_loss: 14.9794 - val_mae: 3.0415\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.0743 - mae: 1.6469 - val_loss: 14.8884 - val_mae: 3.0476\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.8391 - mae: 1.6096 - val_loss: 15.0825 - val_mae: 3.0886\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9323 - mae: 1.6392 - val_loss: 14.8781 - val_mae: 3.0521\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.8579 - mae: 1.6099 - val_loss: 14.8966 - val_mae: 3.0215\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.9854 - mae: 1.6135 - val_loss: 14.9076 - val_mae: 3.0121\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7776 - mae: 1.6076 - val_loss: 15.0198 - val_mae: 3.0757\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.9678 - mae: 1.6419 - val_loss: 14.7181 - val_mae: 3.0443\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.0587 - mae: 1.6417 - val_loss: 14.6980 - val_mae: 2.9887\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.1098 - mae: 1.6480 - val_loss: 14.4565 - val_mae: 2.9849\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.8649 - mae: 1.6273 - val_loss: 14.7151 - val_mae: 3.0537\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.1051 - mae: 1.6886 - val_loss: 14.5896 - val_mae: 3.0385\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.8016 - mae: 1.5893 - val_loss: 14.4131 - val_mae: 3.0049\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.7550 - mae: 1.5664 - val_loss: 14.4503 - val_mae: 3.0143\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7028 - mae: 1.5672 - val_loss: 14.5074 - val_mae: 2.9983\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7495 - mae: 1.5809 - val_loss: 14.7100 - val_mae: 3.0248\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9676 - mae: 1.6434 - val_loss: 15.4311 - val_mae: 3.1051\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9347 - mae: 1.6513 - val_loss: 14.9784 - val_mae: 3.0167\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7240 - mae: 1.5932 - val_loss: 14.9072 - val_mae: 3.0184\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.7237 - mae: 1.5925 - val_loss: 15.0348 - val_mae: 3.0509\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7029 - mae: 1.5824 - val_loss: 14.8180 - val_mae: 3.0084\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.6119 - mae: 1.5551 - val_loss: 14.7041 - val_mae: 2.9828\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.7437 - mae: 1.5747 - val_loss: 14.7060 - val_mae: 2.9693\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5.3798 - mae: 1.7052 - val_loss: 15.1173 - val_mae: 3.0721\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.2063 - mae: 1.6685 - val_loss: 14.6512 - val_mae: 2.9726\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.7874 - mae: 1.5938 - val_loss: 14.5972 - val_mae: 2.9930\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.6573 - mae: 1.5813 - val_loss: 14.7925 - val_mae: 3.0261\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.5810 - mae: 1.5657 - val_loss: 14.7673 - val_mae: 3.0223\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.5542 - mae: 1.5482 - val_loss: 14.6744 - val_mae: 2.9931\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.5775 - mae: 1.5472 - val_loss: 14.6378 - val_mae: 2.9818\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.5600 - mae: 1.5242 - val_loss: 14.6003 - val_mae: 2.9964\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 10.0155 - mae: 2.1615\n",
      "훈련셋 인덱스: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
      " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
      " 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328\n",
      " 329 330 331 332 333 334 335 336 337 338]\n",
      "검증셋 인덱스: [113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n",
      " 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n",
      " 221 222 223 224 225]\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 613.5052 - mae: 22.9411 - val_loss: 600.3727 - val_mae: 22.7316\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 599.5109 - mae: 22.6410 - val_loss: 588.6329 - val_mae: 22.4840\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 589.0323 - mae: 22.4190 - val_loss: 579.1410 - val_mae: 22.2795\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 579.5026 - mae: 22.2103 - val_loss: 569.0974 - val_mae: 22.0624\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 568.6995 - mae: 21.9758 - val_loss: 556.3362 - val_mae: 21.7860\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 555.4527 - mae: 21.6860 - val_loss: 540.4731 - val_mae: 21.4357\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 538.9661 - mae: 21.3132 - val_loss: 519.3005 - val_mae: 20.9654\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 517.2967 - mae: 20.8178 - val_loss: 491.9997 - val_mae: 20.3419\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 489.9555 - mae: 20.1688 - val_loss: 457.7119 - val_mae: 19.5427\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 455.9896 - mae: 19.3584 - val_loss: 416.2731 - val_mae: 18.5365\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 414.4685 - mae: 18.3124 - val_loss: 367.3576 - val_mae: 17.2922\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 367.3125 - mae: 17.0625 - val_loss: 314.4221 - val_mae: 15.8340\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 316.1077 - mae: 15.6042 - val_loss: 256.6216 - val_mae: 14.0990\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 258.7475 - mae: 13.8656 - val_loss: 200.4075 - val_mae: 12.1462\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.1757 - mae: 11.9995 - val_loss: 153.1512 - val_mae: 10.2697\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 160.7117 - mae: 10.1579 - val_loss: 114.8282 - val_mae: 8.5095\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 124.5043 - mae: 8.6244 - val_loss: 89.0626 - val_mae: 7.2147\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 96.8775 - mae: 7.4455 - val_loss: 73.4783 - val_mae: 6.4640\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 79.1727 - mae: 6.6132 - val_loss: 64.6289 - val_mae: 6.0345\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 66.9251 - mae: 6.0307 - val_loss: 58.4216 - val_mae: 5.7342\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 58.3971 - mae: 5.5554 - val_loss: 52.0373 - val_mae: 5.3967\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 50.5412 - mae: 5.1310 - val_loss: 46.5068 - val_mae: 5.1225\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 45.0323 - mae: 4.8599 - val_loss: 41.9751 - val_mae: 4.8985\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 41.0482 - mae: 4.7066 - val_loss: 37.8393 - val_mae: 4.5951\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 37.0382 - mae: 4.4782 - val_loss: 34.1960 - val_mae: 4.2687\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34.2773 - mae: 4.3181 - val_loss: 31.9047 - val_mae: 4.1277\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 32.0433 - mae: 4.2122 - val_loss: 30.3600 - val_mae: 3.9964\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 30.6884 - mae: 4.1614 - val_loss: 29.6548 - val_mae: 3.9925\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 29.6584 - mae: 4.1308 - val_loss: 28.6895 - val_mae: 3.9233\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 28.5311 - mae: 4.0373 - val_loss: 27.7993 - val_mae: 3.8345\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 27.8074 - mae: 3.9930 - val_loss: 27.4215 - val_mae: 3.8234\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 27.2596 - mae: 3.9673 - val_loss: 26.7916 - val_mae: 3.7630\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 26.4464 - mae: 3.8935 - val_loss: 26.0451 - val_mae: 3.6892\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 25.7534 - mae: 3.7964 - val_loss: 25.1967 - val_mae: 3.5836\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 25.0428 - mae: 3.6846 - val_loss: 24.7454 - val_mae: 3.4850\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 24.5378 - mae: 3.5765 - val_loss: 24.4779 - val_mae: 3.4207\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 24.2812 - mae: 3.5274 - val_loss: 24.2637 - val_mae: 3.3789\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 23.9201 - mae: 3.5046 - val_loss: 23.8925 - val_mae: 3.3605\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 23.5265 - mae: 3.4837 - val_loss: 23.6605 - val_mae: 3.3410\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 23.2753 - mae: 3.4696 - val_loss: 23.4381 - val_mae: 3.3355\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 22.9342 - mae: 3.4559 - val_loss: 23.2712 - val_mae: 3.3310\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 22.7076 - mae: 3.4499 - val_loss: 23.1333 - val_mae: 3.3342\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 22.5291 - mae: 3.4535 - val_loss: 23.0584 - val_mae: 3.3455\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 22.3671 - mae: 3.4448 - val_loss: 22.8595 - val_mae: 3.3225\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 22.0775 - mae: 3.4151 - val_loss: 22.5954 - val_mae: 3.2773\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 21.7883 - mae: 3.3797 - val_loss: 22.4048 - val_mae: 3.2411\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21.5647 - mae: 3.3502 - val_loss: 22.2897 - val_mae: 3.2110\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 21.3123 - mae: 3.3174 - val_loss: 22.1153 - val_mae: 3.1899\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 21.0381 - mae: 3.3010 - val_loss: 21.8791 - val_mae: 3.1873\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 20.7071 - mae: 3.3258 - val_loss: 21.8729 - val_mae: 3.2278\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 20.6072 - mae: 3.3539 - val_loss: 21.8580 - val_mae: 3.2374\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 20.4651 - mae: 3.3525 - val_loss: 21.7655 - val_mae: 3.2304\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 20.0577 - mae: 3.3098 - val_loss: 21.5548 - val_mae: 3.1911\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 19.7616 - mae: 3.2513 - val_loss: 21.4819 - val_mae: 3.1569\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 19.6259 - mae: 3.2163 - val_loss: 21.5541 - val_mae: 3.1641\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 19.5022 - mae: 3.1837 - val_loss: 21.3458 - val_mae: 3.1272\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 19.1332 - mae: 3.1648 - val_loss: 20.9568 - val_mae: 3.1277\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 19.2304 - mae: 3.1936 - val_loss: 20.9090 - val_mae: 3.1442\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 18.8415 - mae: 3.2102 - val_loss: 20.7708 - val_mae: 3.1707\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 18.8592 - mae: 3.2408 - val_loss: 20.7432 - val_mae: 3.1606\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - ETA: 0s - loss: 27.5055 - mae: 3.50 - 0s 8ms/step - loss: 18.5475 - mae: 3.2214 - val_loss: 20.5112 - val_mae: 3.1216\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 18.2365 - mae: 3.1682 - val_loss: 20.1884 - val_mae: 3.0604\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 17.8842 - mae: 3.0938 - val_loss: 19.8853 - val_mae: 3.0076\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 17.6110 - mae: 3.0550 - val_loss: 19.5632 - val_mae: 2.9759\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 17.4387 - mae: 3.0418 - val_loss: 19.3946 - val_mae: 2.9591\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 17.3356 - mae: 3.0116 - val_loss: 19.2213 - val_mae: 2.9347\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 17.1434 - mae: 2.9710 - val_loss: 19.1393 - val_mae: 2.9239\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 17.0070 - mae: 2.9356 - val_loss: 19.2132 - val_mae: 2.9296\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 17.0812 - mae: 2.9227 - val_loss: 19.1983 - val_mae: 2.9293\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16.9250 - mae: 2.9071 - val_loss: 19.1183 - val_mae: 2.9316\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 16.6137 - mae: 2.9050 - val_loss: 18.9158 - val_mae: 2.9210\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 16.3420 - mae: 2.8891 - val_loss: 18.8591 - val_mae: 2.9148\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 16.1801 - mae: 2.8861 - val_loss: 18.6926 - val_mae: 2.9037\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 15.8606 - mae: 2.8832 - val_loss: 18.5807 - val_mae: 2.8999\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 15.6324 - mae: 2.8698 - val_loss: 18.5130 - val_mae: 2.8890\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 15.4409 - mae: 2.8520 - val_loss: 18.4967 - val_mae: 2.8855\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - ETA: 0s - loss: 12.7204 - mae: 2.70 - 0s 8ms/step - loss: 15.2364 - mae: 2.8699 - val_loss: 18.7607 - val_mae: 2.9316\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 15.1745 - mae: 2.8947 - val_loss: 18.7884 - val_mae: 2.9406\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 15.0715 - mae: 2.8701 - val_loss: 18.4181 - val_mae: 2.8890\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 14.6558 - mae: 2.8161 - val_loss: 18.1173 - val_mae: 2.8699\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14.9727 - mae: 2.8514 - val_loss: 18.2628 - val_mae: 2.9028\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 14.9866 - mae: 2.8581 - val_loss: 18.0483 - val_mae: 2.8883\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 14.5837 - mae: 2.8111 - val_loss: 17.7373 - val_mae: 2.8215\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 14.2147 - mae: 2.7608 - val_loss: 17.6198 - val_mae: 2.7892\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14.0408 - mae: 2.7402 - val_loss: 17.5435 - val_mae: 2.7885\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 13.9705 - mae: 2.7296 - val_loss: 17.3659 - val_mae: 2.7469\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 13.8944 - mae: 2.7178 - val_loss: 17.2848 - val_mae: 2.7306\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 13.7508 - mae: 2.7032 - val_loss: 17.2371 - val_mae: 2.7280\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13.6027 - mae: 2.6780 - val_loss: 17.2088 - val_mae: 2.7164\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 13.4747 - mae: 2.6570 - val_loss: 17.1374 - val_mae: 2.7172\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 13.3623 - mae: 2.6658 - val_loss: 17.2131 - val_mae: 2.7529\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 13.4694 - mae: 2.7526 - val_loss: 18.8500 - val_mae: 2.9923\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 13.9602 - mae: 2.8434 - val_loss: 18.8827 - val_mae: 2.9807\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.6204 - mae: 2.7772 - val_loss: 17.9063 - val_mae: 2.8577\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 12.8938 - mae: 2.6880 - val_loss: 16.9302 - val_mae: 2.7055\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.9143 - mae: 2.6515 - val_loss: 16.6541 - val_mae: 2.6651\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 12.8245 - mae: 2.6451 - val_loss: 16.6132 - val_mae: 2.6578\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.6493 - mae: 2.6283 - val_loss: 16.6862 - val_mae: 2.6626\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.5040 - mae: 2.5937 - val_loss: 17.0843 - val_mae: 2.7094\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 12.4023 - mae: 2.5695 - val_loss: 17.0114 - val_mae: 2.7010\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.2225 - mae: 2.5538 - val_loss: 16.8418 - val_mae: 2.6914\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 12.0543 - mae: 2.5474 - val_loss: 16.6068 - val_mae: 2.6701\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.0419 - mae: 2.5713 - val_loss: 16.5997 - val_mae: 2.6739\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.9228 - mae: 2.5608 - val_loss: 16.5839 - val_mae: 2.6779\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.7822 - mae: 2.5417 - val_loss: 16.5129 - val_mae: 2.6633\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.6702 - mae: 2.5245 - val_loss: 16.5459 - val_mae: 2.6837\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.5917 - mae: 2.5157 - val_loss: 16.5248 - val_mae: 2.6823\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.4865 - mae: 2.4968 - val_loss: 16.3541 - val_mae: 2.6471\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.4477 - mae: 2.4795 - val_loss: 16.1406 - val_mae: 2.5803\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 11.3428 - mae: 2.4497 - val_loss: 16.1875 - val_mae: 2.5702\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.3217 - mae: 2.4406 - val_loss: 16.1978 - val_mae: 2.5824\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.1127 - mae: 2.4412 - val_loss: 16.1865 - val_mae: 2.6221\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 11.0573 - mae: 2.4454 - val_loss: 16.2316 - val_mae: 2.6365\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.9597 - mae: 2.4419 - val_loss: 16.2274 - val_mae: 2.6355\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.8785 - mae: 2.4356 - val_loss: 16.2755 - val_mae: 2.6511\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.7580 - mae: 2.4146 - val_loss: 16.2515 - val_mae: 2.6387\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.6517 - mae: 2.4008 - val_loss: 16.2818 - val_mae: 2.6403\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.6076 - mae: 2.3967 - val_loss: 16.3980 - val_mae: 2.6455\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.5646 - mae: 2.3782 - val_loss: 16.2788 - val_mae: 2.6129\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.6035 - mae: 2.3651 - val_loss: 16.2886 - val_mae: 2.6247\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.4539 - mae: 2.3514 - val_loss: 16.1780 - val_mae: 2.5966\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.3278 - mae: 2.3480 - val_loss: 16.1728 - val_mae: 2.5841\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.3009 - mae: 2.3411 - val_loss: 16.0767 - val_mae: 2.5582\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.2245 - mae: 2.3340 - val_loss: 16.0216 - val_mae: 2.5599\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.0988 - mae: 2.3219 - val_loss: 16.0327 - val_mae: 2.5641\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.0628 - mae: 2.3275 - val_loss: 16.0699 - val_mae: 2.5828\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.9746 - mae: 2.3220 - val_loss: 16.0893 - val_mae: 2.5871\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.9589 - mae: 2.3041 - val_loss: 16.0514 - val_mae: 2.5498\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.9344 - mae: 2.2948 - val_loss: 16.1006 - val_mae: 2.5719\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.8821 - mae: 2.2940 - val_loss: 16.2232 - val_mae: 2.5891\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.7669 - mae: 2.2943 - val_loss: 16.2271 - val_mae: 2.5872\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.7431 - mae: 2.3019 - val_loss: 16.1695 - val_mae: 2.5839\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.6933 - mae: 2.2851 - val_loss: 16.2016 - val_mae: 2.5974\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.5610 - mae: 2.2614 - val_loss: 16.1209 - val_mae: 2.5673\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.4886 - mae: 2.2559 - val_loss: 16.1906 - val_mae: 2.5946\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.4441 - mae: 2.2518 - val_loss: 16.3018 - val_mae: 2.6288\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.4703 - mae: 2.2480 - val_loss: 16.3301 - val_mae: 2.6261\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.3974 - mae: 2.2257 - val_loss: 16.0785 - val_mae: 2.5571\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.3674 - mae: 2.2217 - val_loss: 15.9602 - val_mae: 2.5313\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.2456 - mae: 2.2237 - val_loss: 15.8825 - val_mae: 2.5200\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.1451 - mae: 2.2153 - val_loss: 15.9774 - val_mae: 2.5702\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.0996 - mae: 2.2044 - val_loss: 16.0582 - val_mae: 2.5869\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.0366 - mae: 2.2001 - val_loss: 16.0040 - val_mae: 2.5629\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.9669 - mae: 2.1810 - val_loss: 15.9860 - val_mae: 2.5507\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.9442 - mae: 2.1746 - val_loss: 15.9943 - val_mae: 2.5455\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.9560 - mae: 2.1802 - val_loss: 16.2891 - val_mae: 2.6238\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.9486 - mae: 2.1794 - val_loss: 16.4811 - val_mae: 2.6638\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.8215 - mae: 2.1723 - val_loss: 16.1059 - val_mae: 2.5682\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.7956 - mae: 2.1808 - val_loss: 15.9761 - val_mae: 2.5182\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.7529 - mae: 2.1847 - val_loss: 15.9969 - val_mae: 2.5281\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.7046 - mae: 2.1698 - val_loss: 16.0515 - val_mae: 2.5420\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.5315 - mae: 2.1365 - val_loss: 16.4042 - val_mae: 2.6343\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.6463 - mae: 2.1556 - val_loss: 16.4821 - val_mae: 2.6466\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.6382 - mae: 2.1451 - val_loss: 16.1676 - val_mae: 2.5543\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5366 - mae: 2.1277 - val_loss: 16.0993 - val_mae: 2.5313\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.4521 - mae: 2.1259 - val_loss: 16.1941 - val_mae: 2.5599\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.3779 - mae: 2.1233 - val_loss: 16.2196 - val_mae: 2.5834\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.3239 - mae: 2.1140 - val_loss: 16.1309 - val_mae: 2.5656\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.2800 - mae: 2.1079 - val_loss: 16.0164 - val_mae: 2.5446\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.2649 - mae: 2.1080 - val_loss: 16.2032 - val_mae: 2.5873\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.7207 - mae: 2.2090 - val_loss: 19.1077 - val_mae: 2.9183\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.5998 - mae: 2.3084 - val_loss: 18.1095 - val_mae: 2.8085\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.6323 - mae: 2.1955 - val_loss: 16.2702 - val_mae: 2.5391\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.4277 - mae: 2.1665 - val_loss: 16.0579 - val_mae: 2.5213\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.3222 - mae: 2.1406 - val_loss: 15.9994 - val_mae: 2.5152\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.1366 - mae: 2.1114 - val_loss: 15.9397 - val_mae: 2.5226\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.0128 - mae: 2.0840 - val_loss: 16.0363 - val_mae: 2.5522\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.9393 - mae: 2.0770 - val_loss: 16.3005 - val_mae: 2.5912\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.9389 - mae: 2.0754 - val_loss: 16.2780 - val_mae: 2.6052\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.8944 - mae: 2.0659 - val_loss: 16.1294 - val_mae: 2.5878\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.8452 - mae: 2.0567 - val_loss: 16.0272 - val_mae: 2.5604\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.7992 - mae: 2.0674 - val_loss: 15.9956 - val_mae: 2.5322\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.8203 - mae: 2.0733 - val_loss: 16.5986 - val_mae: 2.6122\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.9854 - mae: 2.0952 - val_loss: 17.9561 - val_mae: 2.7877\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.3648 - mae: 2.1614 - val_loss: 17.4559 - val_mae: 2.6666\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.9368 - mae: 2.1025 - val_loss: 16.7459 - val_mae: 2.6182\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.7948 - mae: 2.0690 - val_loss: 16.4394 - val_mae: 2.5812\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.7039 - mae: 2.0480 - val_loss: 16.1954 - val_mae: 2.5405\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.7467 - mae: 2.0778 - val_loss: 16.2326 - val_mae: 2.5351\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.6259 - mae: 2.0401 - val_loss: 16.0089 - val_mae: 2.4874\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.5867 - mae: 1.9924 - val_loss: 16.0599 - val_mae: 2.5071\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.5236 - mae: 1.9869 - val_loss: 16.2943 - val_mae: 2.5549\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.4437 - mae: 1.9822 - val_loss: 16.2719 - val_mae: 2.5334\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.4056 - mae: 1.9881 - val_loss: 16.2404 - val_mae: 2.5080\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.3487 - mae: 1.9889 - val_loss: 16.1274 - val_mae: 2.5230\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.2855 - mae: 1.9687 - val_loss: 15.8885 - val_mae: 2.4890\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.3720 - mae: 1.9633 - val_loss: 15.5995 - val_mae: 2.4382\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.3829 - mae: 1.9725 - val_loss: 15.6681 - val_mae: 2.4719\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.2831 - mae: 1.9779 - val_loss: 15.9513 - val_mae: 2.5366\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.2612 - mae: 1.9731 - val_loss: 16.0175 - val_mae: 2.5571\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 7.2985 - mae: 1.9604 - val_loss: 16.0747 - val_mae: 2.5673\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.1121 - mae: 1.9333 - val_loss: 16.0542 - val_mae: 2.5230\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.1077 - mae: 1.9454 - val_loss: 16.1954 - val_mae: 2.5193\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.1123 - mae: 1.9524 - val_loss: 16.3949 - val_mae: 2.5491\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.0769 - mae: 1.9370 - val_loss: 16.3072 - val_mae: 2.5289\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9977 - mae: 1.9173 - val_loss: 16.2234 - val_mae: 2.4988\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.1374 - mae: 1.9236 - val_loss: 16.6747 - val_mae: 2.6290\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.1353 - mae: 1.9332 - val_loss: 16.3650 - val_mae: 2.5622\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9722 - mae: 1.9186 - val_loss: 16.3021 - val_mae: 2.5255\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8953 - mae: 1.9078 - val_loss: 16.4181 - val_mae: 2.5679\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9281 - mae: 1.9263 - val_loss: 16.4727 - val_mae: 2.5594\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8485 - mae: 1.9161 - val_loss: 16.1089 - val_mae: 2.4915\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8981 - mae: 1.9165 - val_loss: 16.0585 - val_mae: 2.4947\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8997 - mae: 1.9331 - val_loss: 16.3183 - val_mae: 2.5538\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.1787 - mae: 1.9930 - val_loss: 17.4731 - val_mae: 2.7119\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.1241 - mae: 1.9637 - val_loss: 16.6403 - val_mae: 2.6068\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8008 - mae: 1.9054 - val_loss: 16.0436 - val_mae: 2.5098\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9509 - mae: 1.9231 - val_loss: 16.0555 - val_mae: 2.4951\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7137 - mae: 1.8923 - val_loss: 16.3145 - val_mae: 2.5328\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.6458 - mae: 1.8720 - val_loss: 16.3331 - val_mae: 2.5444\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7410 - mae: 1.8958 - val_loss: 16.0800 - val_mae: 2.5048\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.6112 - mae: 1.8882 - val_loss: 16.0559 - val_mae: 2.5094\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.6804 - mae: 1.8762 - val_loss: 16.0486 - val_mae: 2.5682\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7830 - mae: 1.8797 - val_loss: 15.9939 - val_mae: 2.5427\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.6104 - mae: 1.8826 - val_loss: 15.9470 - val_mae: 2.4847\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.6231 - mae: 1.9081 - val_loss: 15.8541 - val_mae: 2.4632\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.4676 - mae: 1.8611 - val_loss: 15.7636 - val_mae: 2.4796\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.5428 - mae: 1.8367 - val_loss: 15.6824 - val_mae: 2.4726\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.5544 - mae: 1.8291 - val_loss: 15.7546 - val_mae: 2.4733\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.4934 - mae: 1.8248 - val_loss: 15.7740 - val_mae: 2.4902\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.3898 - mae: 1.8228 - val_loss: 15.7871 - val_mae: 2.4802\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.3898 - mae: 1.8483 - val_loss: 15.7747 - val_mae: 2.4687\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.3676 - mae: 1.8466 - val_loss: 15.9346 - val_mae: 2.4950\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.3082 - mae: 1.8129 - val_loss: 15.9654 - val_mae: 2.5127\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2661 - mae: 1.7960 - val_loss: 15.9803 - val_mae: 2.4805\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.2280 - mae: 1.8023 - val_loss: 16.0560 - val_mae: 2.4623\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2201 - mae: 1.8221 - val_loss: 15.9882 - val_mae: 2.4397\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.1993 - mae: 1.8092 - val_loss: 16.1143 - val_mae: 2.4812\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.1902 - mae: 1.7893 - val_loss: 16.1060 - val_mae: 2.4834\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.1856 - mae: 1.7807 - val_loss: 16.0568 - val_mae: 2.4428\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2111 - mae: 1.7866 - val_loss: 16.2215 - val_mae: 2.4614\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.1675 - mae: 1.7935 - val_loss: 16.4411 - val_mae: 2.5127\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.1640 - mae: 1.7786 - val_loss: 16.8437 - val_mae: 2.6139\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2153 - mae: 1.7858 - val_loss: 16.5147 - val_mae: 2.5426\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.1123 - mae: 1.7737 - val_loss: 16.1391 - val_mae: 2.4415\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0408 - mae: 1.7886 - val_loss: 16.1347 - val_mae: 2.4503\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.0506 - mae: 1.7846 - val_loss: 16.0587 - val_mae: 2.4628\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.9662 - mae: 1.7574 - val_loss: 16.8318 - val_mae: 2.6083\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.2505 - mae: 1.9585 - val_loss: 19.8933 - val_mae: 2.8755\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.4338 - mae: 1.9844 - val_loss: 17.6700 - val_mae: 2.6398\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.4294 - mae: 1.8584 - val_loss: 16.4011 - val_mae: 2.4935\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.5367 - mae: 1.8772 - val_loss: 16.0256 - val_mae: 2.4692\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.3000 - mae: 1.8388 - val_loss: 15.9572 - val_mae: 2.4546\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.2658 - mae: 1.8617 - val_loss: 16.5219 - val_mae: 2.5560\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.1020 - mae: 1.8052 - val_loss: 16.4274 - val_mae: 2.5061\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.0050 - mae: 1.7478 - val_loss: 16.2005 - val_mae: 2.4563\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.9503 - mae: 1.7455 - val_loss: 16.3284 - val_mae: 2.4587\n",
      "Epoch 248/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.8816 - mae: 1.7399 - val_loss: 16.4617 - val_mae: 2.5207\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.8583 - mae: 1.7411 - val_loss: 16.3178 - val_mae: 2.5225\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.7871 - mae: 1.7302 - val_loss: 15.9217 - val_mae: 2.4736\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.8231 - mae: 1.7173 - val_loss: 15.8327 - val_mae: 2.4802\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.7253 - mae: 1.7153 - val_loss: 15.7730 - val_mae: 2.4558\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6621 - mae: 1.7162 - val_loss: 16.3473 - val_mae: 2.5474\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.0691 - mae: 1.7811 - val_loss: 17.6869 - val_mae: 2.6903\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.2557 - mae: 1.8549 - val_loss: 17.3636 - val_mae: 2.6950\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.0126 - mae: 1.8581 - val_loss: 16.1214 - val_mae: 2.4569\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.7563 - mae: 1.7211 - val_loss: 16.0972 - val_mae: 2.5205\n",
      "Epoch 258/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.8532 - mae: 1.7283 - val_loss: 15.7396 - val_mae: 2.4707\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6975 - mae: 1.7008 - val_loss: 15.7214 - val_mae: 2.4379\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.6314 - mae: 1.7161 - val_loss: 15.6989 - val_mae: 2.4359\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5773 - mae: 1.7049 - val_loss: 16.0377 - val_mae: 2.4918\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6099 - mae: 1.7045 - val_loss: 16.6384 - val_mae: 2.5411\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.5606 - mae: 1.7027 - val_loss: 16.5584 - val_mae: 2.5157\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.6305 - mae: 1.6752 - val_loss: 16.3144 - val_mae: 2.4983\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.6091 - mae: 1.6755 - val_loss: 16.2500 - val_mae: 2.5234\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5753 - mae: 1.6682 - val_loss: 16.1162 - val_mae: 2.4936\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.7288 - mae: 1.7237 - val_loss: 16.3399 - val_mae: 2.5538\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5955 - mae: 1.7232 - val_loss: 15.8843 - val_mae: 2.4747\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.4313 - mae: 1.6962 - val_loss: 15.7962 - val_mae: 2.4593\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.4259 - mae: 1.6650 - val_loss: 15.9232 - val_mae: 2.4581\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.7280 - mae: 1.6861 - val_loss: 15.6927 - val_mae: 2.4259\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.5304 - mae: 1.6760 - val_loss: 15.7433 - val_mae: 2.4218\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.3568 - mae: 1.6442 - val_loss: 16.2674 - val_mae: 2.5243\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.3685 - mae: 1.6517 - val_loss: 16.1005 - val_mae: 2.4980\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.2848 - mae: 1.6462 - val_loss: 15.7446 - val_mae: 2.4334\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.2627 - mae: 1.6529 - val_loss: 15.8070 - val_mae: 2.4345\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.2755 - mae: 1.6450 - val_loss: 16.1570 - val_mae: 2.4895\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.2352 - mae: 1.6393 - val_loss: 16.0196 - val_mae: 2.4762\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.2802 - mae: 1.6461 - val_loss: 15.7486 - val_mae: 2.4297\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.2016 - mae: 1.6405 - val_loss: 15.8258 - val_mae: 2.4366\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.2111 - mae: 1.6522 - val_loss: 15.7766 - val_mae: 2.4232\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.2313 - mae: 1.6371 - val_loss: 15.9616 - val_mae: 2.4624\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 5.1826 - mae: 1.6329 - val_loss: 16.1495 - val_mae: 2.4833\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.1879 - mae: 1.6575 - val_loss: 16.2505 - val_mae: 2.4850\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.1674 - mae: 1.6585 - val_loss: 15.9272 - val_mae: 2.4342\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.1360 - mae: 1.6431 - val_loss: 15.9142 - val_mae: 2.4391\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.2349 - mae: 1.6301 - val_loss: 15.3689 - val_mae: 2.4056\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.3048 - mae: 1.6654 - val_loss: 15.6665 - val_mae: 2.4599\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.4383 - mae: 1.7277 - val_loss: 15.6437 - val_mae: 2.4284\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.1943 - mae: 1.6179 - val_loss: 16.3857 - val_mae: 2.6353\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.6196 - mae: 1.6767 - val_loss: 15.9550 - val_mae: 2.5281\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.1933 - mae: 1.6396 - val_loss: 16.0683 - val_mae: 2.4979\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 5.2785 - mae: 1.7122 - val_loss: 16.1912 - val_mae: 2.5079\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.1905 - mae: 1.6790 - val_loss: 15.8262 - val_mae: 2.4258\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.0030 - mae: 1.6014 - val_loss: 15.9041 - val_mae: 2.4567\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 5.0570 - mae: 1.5891 - val_loss: 15.7104 - val_mae: 2.4158\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9799 - mae: 1.5841 - val_loss: 15.6704 - val_mae: 2.4132\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9395 - mae: 1.5861 - val_loss: 15.7864 - val_mae: 2.4451\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.9363 - mae: 1.5789 - val_loss: 15.8663 - val_mae: 2.4644\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 4.9420 - mae: 1.5731 - val_loss: 15.7340 - val_mae: 2.4315\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 9.4206 - mae: 2.2192\n",
      "훈련셋 인덱스: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225]\n",
      "검증셋 인덱스: [226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243\n",
      " 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261\n",
      " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
      " 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297\n",
      " 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315\n",
      " 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333\n",
      " 334 335 336 337 338]\n",
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 592.2022 - mae: 22.5296 - val_loss: 599.2910 - val_mae: 22.4472\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 569.2728 - mae: 22.0113 - val_loss: 578.2015 - val_mae: 21.9659\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 548.6128 - mae: 21.5299 - val_loss: 557.3557 - val_mae: 21.4832\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 527.5021 - mae: 21.0370 - val_loss: 535.0291 - val_mae: 20.9517\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 504.3304 - mae: 20.4838 - val_loss: 509.1514 - val_mae: 20.3295\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 476.8889 - mae: 19.8206 - val_loss: 478.1455 - val_mae: 19.5881\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 443.1768 - mae: 19.0078 - val_loss: 439.9772 - val_mae: 18.6761\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 401.9410 - mae: 17.9769 - val_loss: 396.1849 - val_mae: 17.5758\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 353.8527 - mae: 16.7113 - val_loss: 345.5960 - val_mae: 16.2132\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 301.1994 - mae: 15.2550 - val_loss: 293.1076 - val_mae: 14.6896\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 248.4622 - mae: 13.6481 - val_loss: 242.2635 - val_mae: 13.0823\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 196.5073 - mae: 11.8811 - val_loss: 193.4458 - val_mae: 11.3223\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 150.5004 - mae: 10.0479 - val_loss: 151.5937 - val_mae: 9.7642\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 114.8672 - mae: 8.5595 - val_loss: 123.3744 - val_mae: 8.8092\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 93.2656 - mae: 7.5922 - val_loss: 104.4296 - val_mae: 8.0883\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 78.7943 - mae: 6.8815 - val_loss: 91.1718 - val_mae: 7.5443\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 69.2285 - mae: 6.3864 - val_loss: 80.7915 - val_mae: 7.0711\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 61.9427 - mae: 5.9618 - val_loss: 71.7337 - val_mae: 6.6115\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 55.2635 - mae: 5.5324 - val_loss: 63.5251 - val_mae: 6.1337\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 48.6879 - mae: 5.1276 - val_loss: 56.9021 - val_mae: 5.7215\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 43.5635 - mae: 4.8155 - val_loss: 51.2460 - val_mae: 5.3476\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 39.5468 - mae: 4.5508 - val_loss: 46.5265 - val_mae: 5.0210\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 36.1922 - mae: 4.3448 - val_loss: 42.2799 - val_mae: 4.7076\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 32.8737 - mae: 4.1773 - val_loss: 38.9620 - val_mae: 4.5068\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 31.2356 - mae: 4.0933 - val_loss: 36.3453 - val_mae: 4.3366\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 29.4577 - mae: 3.9714 - val_loss: 34.1778 - val_mae: 4.1677\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 28.0086 - mae: 3.8465 - val_loss: 32.3882 - val_mae: 4.0193\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 26.9027 - mae: 3.7374 - val_loss: 30.6702 - val_mae: 3.8733\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 25.7934 - mae: 3.6292 - val_loss: 29.5505 - val_mae: 3.8015\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 25.0714 - mae: 3.5717 - val_loss: 28.4885 - val_mae: 3.7564\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 24.2890 - mae: 3.5177 - val_loss: 27.5429 - val_mae: 3.6878\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23.7389 - mae: 3.4740 - val_loss: 26.7515 - val_mae: 3.6391\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 23.1754 - mae: 3.4215 - val_loss: 26.0419 - val_mae: 3.5733\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 22.7030 - mae: 3.3718 - val_loss: 25.4886 - val_mae: 3.5164\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 22.3478 - mae: 3.3339 - val_loss: 24.9368 - val_mae: 3.4834\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21.8610 - mae: 3.2967 - val_loss: 24.6604 - val_mae: 3.5030\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 21.6004 - mae: 3.3004 - val_loss: 24.3256 - val_mae: 3.4944\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 21.1852 - mae: 3.2645 - val_loss: 23.8438 - val_mae: 3.4355\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 20.7765 - mae: 3.2215 - val_loss: 23.4343 - val_mae: 3.3804\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 20.3643 - mae: 3.1683 - val_loss: 23.0384 - val_mae: 3.3177\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 20.1407 - mae: 3.1076 - val_loss: 22.6396 - val_mae: 3.2423\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 19.8971 - mae: 3.0667 - val_loss: 22.2661 - val_mae: 3.2188\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 19.6057 - mae: 3.0570 - val_loss: 21.8918 - val_mae: 3.2183\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 19.1868 - mae: 3.0515 - val_loss: 21.6727 - val_mae: 3.2192\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 18.8773 - mae: 3.0243 - val_loss: 21.4584 - val_mae: 3.1841\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 18.6703 - mae: 2.9951 - val_loss: 21.2771 - val_mae: 3.1756\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 18.3998 - mae: 2.9738 - val_loss: 21.1028 - val_mae: 3.1527\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 18.2980 - mae: 2.9539 - val_loss: 21.0065 - val_mae: 3.1486\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 18.2347 - mae: 2.9796 - val_loss: 20.7258 - val_mae: 3.2279\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 17.8067 - mae: 2.9623 - val_loss: 20.2778 - val_mae: 3.1658\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.5027 - mae: 2.9089 - val_loss: 19.9996 - val_mae: 3.0936\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.3831 - mae: 2.8659 - val_loss: 19.8447 - val_mae: 3.0685\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.1454 - mae: 2.8396 - val_loss: 19.7186 - val_mae: 3.0689\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.9112 - mae: 2.8249 - val_loss: 19.5639 - val_mae: 3.0649\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.7081 - mae: 2.8141 - val_loss: 19.4616 - val_mae: 3.0676\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 16.5103 - mae: 2.7989 - val_loss: 19.3640 - val_mae: 3.0610\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.3511 - mae: 2.7852 - val_loss: 19.2944 - val_mae: 3.0647\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 16.1253 - mae: 2.7632 - val_loss: 19.1477 - val_mae: 3.0487\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.9609 - mae: 2.7509 - val_loss: 18.9915 - val_mae: 3.0416\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.9975 - mae: 2.7443 - val_loss: 18.8733 - val_mae: 3.0125\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.7255 - mae: 2.7195 - val_loss: 18.7277 - val_mae: 3.0153\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.5838 - mae: 2.7063 - val_loss: 18.7661 - val_mae: 3.0245\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.4964 - mae: 2.6968 - val_loss: 18.6913 - val_mae: 3.0260\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.3747 - mae: 2.6861 - val_loss: 18.7036 - val_mae: 3.0162\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.2964 - mae: 2.6715 - val_loss: 18.6635 - val_mae: 3.0163\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.1559 - mae: 2.6562 - val_loss: 18.6675 - val_mae: 3.0410\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.0477 - mae: 2.6601 - val_loss: 18.5530 - val_mae: 3.0489\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.9063 - mae: 2.6584 - val_loss: 18.4197 - val_mae: 3.0420\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.7692 - mae: 2.6466 - val_loss: 18.2009 - val_mae: 3.0124\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.6391 - mae: 2.6237 - val_loss: 17.9506 - val_mae: 2.9903\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.5400 - mae: 2.6066 - val_loss: 17.7894 - val_mae: 2.9813\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.4930 - mae: 2.5991 - val_loss: 17.6407 - val_mae: 2.9692\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 14.5298 - mae: 2.5863 - val_loss: 17.6745 - val_mae: 2.9573\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 14.4478 - mae: 2.5650 - val_loss: 17.5829 - val_mae: 2.9577\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14.1237 - mae: 2.5425 - val_loss: 17.6773 - val_mae: 3.0031\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.1368 - mae: 2.5676 - val_loss: 17.7142 - val_mae: 3.0122\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 14.0499 - mae: 2.5517 - val_loss: 17.5979 - val_mae: 2.9874\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.9112 - mae: 2.5289 - val_loss: 17.4255 - val_mae: 2.9755\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.7787 - mae: 2.5187 - val_loss: 17.1688 - val_mae: 2.9600\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.8102 - mae: 2.5240 - val_loss: 17.0230 - val_mae: 2.9617\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.6544 - mae: 2.5221 - val_loss: 16.7925 - val_mae: 2.9227\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.5405 - mae: 2.5072 - val_loss: 16.6966 - val_mae: 2.9066\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.4758 - mae: 2.5030 - val_loss: 16.6346 - val_mae: 2.9062\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.3758 - mae: 2.4869 - val_loss: 16.6038 - val_mae: 2.9156\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.2779 - mae: 2.4895 - val_loss: 16.7067 - val_mae: 2.9598\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.9009 - mae: 2.5190 - val_loss: 17.4740 - val_mae: 3.0828\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.6974 - mae: 2.6045 - val_loss: 17.5274 - val_mae: 3.0800\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 13.5380 - mae: 2.5779 - val_loss: 17.0594 - val_mae: 3.0221\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.1583 - mae: 2.5115 - val_loss: 16.4956 - val_mae: 2.9599\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.0891 - mae: 2.5567 - val_loss: 17.4216 - val_mae: 3.1486\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.9528 - mae: 2.6800 - val_loss: 17.5245 - val_mae: 3.1777\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 13.5614 - mae: 2.6475 - val_loss: 16.8618 - val_mae: 3.0930\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.8998 - mae: 2.5645 - val_loss: 16.2859 - val_mae: 3.0128\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.6817 - mae: 2.5165 - val_loss: 16.1003 - val_mae: 2.9502\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.7055 - mae: 2.4824 - val_loss: 16.0604 - val_mae: 2.9501\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.5738 - mae: 2.4651 - val_loss: 16.1094 - val_mae: 2.9858\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.5818 - mae: 2.4726 - val_loss: 15.9705 - val_mae: 2.9910\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.3272 - mae: 2.4414 - val_loss: 15.5803 - val_mae: 2.8836\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 12.4312 - mae: 2.4417 - val_loss: 15.5651 - val_mae: 2.8536\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.4375 - mae: 2.4540 - val_loss: 15.5105 - val_mae: 2.8613\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.2348 - mae: 2.4599 - val_loss: 15.6895 - val_mae: 2.9167\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.7754 - mae: 2.5508 - val_loss: 16.9280 - val_mae: 3.0719\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.9905 - mae: 2.6029 - val_loss: 16.5784 - val_mae: 3.0169\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.6267 - mae: 2.5369 - val_loss: 16.1886 - val_mae: 2.9659\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.1875 - mae: 2.4409 - val_loss: 16.0555 - val_mae: 2.9418\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 12.0877 - mae: 2.4174 - val_loss: 15.9725 - val_mae: 2.9330\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.9675 - mae: 2.3935 - val_loss: 15.9138 - val_mae: 2.9333\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.8330 - mae: 2.3857 - val_loss: 15.9452 - val_mae: 2.9709\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.8963 - mae: 2.4154 - val_loss: 16.2242 - val_mae: 3.0429\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.8896 - mae: 2.4384 - val_loss: 15.7715 - val_mae: 2.9426\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.6516 - mae: 2.3844 - val_loss: 15.5462 - val_mae: 2.8773\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.6117 - mae: 2.3510 - val_loss: 15.6210 - val_mae: 2.8827\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.5841 - mae: 2.3326 - val_loss: 15.7561 - val_mae: 2.9230\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.5942 - mae: 2.3609 - val_loss: 15.8139 - val_mae: 2.9307\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.6491 - mae: 2.3997 - val_loss: 15.6227 - val_mae: 2.9066\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.5492 - mae: 2.3812 - val_loss: 15.3139 - val_mae: 2.8739\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.3954 - mae: 2.3317 - val_loss: 15.0596 - val_mae: 2.8364\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.4056 - mae: 2.3187 - val_loss: 15.0568 - val_mae: 2.8438\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.2982 - mae: 2.3052 - val_loss: 15.0431 - val_mae: 2.8543\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.2425 - mae: 2.3124 - val_loss: 15.0643 - val_mae: 2.8670\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.2123 - mae: 2.3153 - val_loss: 15.0365 - val_mae: 2.8643\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.1470 - mae: 2.3099 - val_loss: 14.9744 - val_mae: 2.8452\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 11.1388 - mae: 2.3086 - val_loss: 14.9967 - val_mae: 2.8563\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.0789 - mae: 2.3072 - val_loss: 15.0653 - val_mae: 2.9000\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.1079 - mae: 2.3152 - val_loss: 15.0723 - val_mae: 2.9084\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.0380 - mae: 2.3171 - val_loss: 14.9730 - val_mae: 2.8760\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.9467 - mae: 2.3158 - val_loss: 14.9372 - val_mae: 2.8387\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.9602 - mae: 2.3110 - val_loss: 15.0098 - val_mae: 2.8227\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.9567 - mae: 2.3088 - val_loss: 14.9975 - val_mae: 2.8391\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.9087 - mae: 2.3141 - val_loss: 14.9492 - val_mae: 2.8799\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.8452 - mae: 2.3035 - val_loss: 14.9044 - val_mae: 2.8637\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.7971 - mae: 2.2852 - val_loss: 14.8018 - val_mae: 2.8361\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.7760 - mae: 2.2735 - val_loss: 14.8880 - val_mae: 2.8653\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.7484 - mae: 2.2670 - val_loss: 14.8136 - val_mae: 2.8358\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.8078 - mae: 2.2550 - val_loss: 14.9887 - val_mae: 2.8140\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.9189 - mae: 2.2726 - val_loss: 14.9968 - val_mae: 2.8387\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.6079 - mae: 2.2462 - val_loss: 15.0416 - val_mae: 2.8659\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.5806 - mae: 2.2689 - val_loss: 15.0485 - val_mae: 2.8763\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.5398 - mae: 2.2781 - val_loss: 14.8900 - val_mae: 2.8695\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.6435 - mae: 2.2604 - val_loss: 14.7439 - val_mae: 2.8679\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.6459 - mae: 2.2475 - val_loss: 14.5007 - val_mae: 2.8246\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.5042 - mae: 2.2520 - val_loss: 14.4886 - val_mae: 2.8248\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.4606 - mae: 2.2469 - val_loss: 14.4542 - val_mae: 2.8050\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.4031 - mae: 2.2360 - val_loss: 14.4570 - val_mae: 2.8231\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.3870 - mae: 2.2296 - val_loss: 14.4668 - val_mae: 2.8278\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.3180 - mae: 2.2304 - val_loss: 14.5144 - val_mae: 2.8493\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.3908 - mae: 2.2670 - val_loss: 14.4886 - val_mae: 2.8313\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.2878 - mae: 2.2575 - val_loss: 14.3049 - val_mae: 2.7376\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 10.4046 - mae: 2.2723 - val_loss: 14.2339 - val_mae: 2.7515\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.2911 - mae: 2.2553 - val_loss: 14.6383 - val_mae: 2.8844\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.3539 - mae: 2.2482 - val_loss: 14.6816 - val_mae: 2.8883\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.2542 - mae: 2.2106 - val_loss: 14.5157 - val_mae: 2.8422\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.2313 - mae: 2.2088 - val_loss: 14.3617 - val_mae: 2.7920\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.1080 - mae: 2.1821 - val_loss: 14.5796 - val_mae: 2.8553\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.5294 - mae: 2.1971 - val_loss: 14.4391 - val_mae: 2.8334\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.1250 - mae: 2.1961 - val_loss: 14.2721 - val_mae: 2.7914\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 10.0778 - mae: 2.2080 - val_loss: 14.3865 - val_mae: 2.8265\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 10.0397 - mae: 2.1973 - val_loss: 14.5410 - val_mae: 2.8746\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.9655 - mae: 2.2098 - val_loss: 14.5408 - val_mae: 2.8463\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.9247 - mae: 2.2126 - val_loss: 14.4666 - val_mae: 2.8145\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.8696 - mae: 2.1946 - val_loss: 14.4541 - val_mae: 2.8211\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.8055 - mae: 2.1707 - val_loss: 14.4524 - val_mae: 2.8312\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.7717 - mae: 2.1590 - val_loss: 14.4287 - val_mae: 2.8399\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.7625 - mae: 2.1443 - val_loss: 14.4641 - val_mae: 2.8526\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.7426 - mae: 2.1522 - val_loss: 14.4282 - val_mae: 2.8259\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.7784 - mae: 2.2099 - val_loss: 14.7401 - val_mae: 2.8267\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.9051 - mae: 2.2595 - val_loss: 14.8237 - val_mae: 2.8566\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.7835 - mae: 2.2512 - val_loss: 14.5876 - val_mae: 2.8673\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.6629 - mae: 2.1938 - val_loss: 14.3947 - val_mae: 2.8483\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.6809 - mae: 2.1682 - val_loss: 14.3546 - val_mae: 2.8280\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.5432 - mae: 2.1584 - val_loss: 14.3452 - val_mae: 2.8256\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.5947 - mae: 2.2014 - val_loss: 14.4596 - val_mae: 2.8113\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.5432 - mae: 2.1980 - val_loss: 14.3047 - val_mae: 2.7859\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.4473 - mae: 2.1898 - val_loss: 14.2826 - val_mae: 2.8269\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.5265 - mae: 2.1985 - val_loss: 14.4121 - val_mae: 2.8730\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.4386 - mae: 2.1819 - val_loss: 14.1470 - val_mae: 2.8030\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.3812 - mae: 2.1746 - val_loss: 14.0095 - val_mae: 2.7880\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.2907 - mae: 2.1648 - val_loss: 13.7996 - val_mae: 2.7570\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.2574 - mae: 2.1439 - val_loss: 13.6511 - val_mae: 2.7372\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.2583 - mae: 2.1324 - val_loss: 13.7339 - val_mae: 2.7598\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 9.1756 - mae: 2.1349 - val_loss: 14.1100 - val_mae: 2.8300\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.3572 - mae: 2.1761 - val_loss: 14.2981 - val_mae: 2.8315\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.3536 - mae: 2.1621 - val_loss: 13.9798 - val_mae: 2.7933\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.1570 - mae: 2.1196 - val_loss: 13.7777 - val_mae: 2.7894\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.4863 - mae: 2.1204 - val_loss: 13.9147 - val_mae: 2.8281\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.5166 - mae: 2.1299 - val_loss: 13.7313 - val_mae: 2.8187\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.1896 - mae: 2.1268 - val_loss: 13.6234 - val_mae: 2.7771\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.1261 - mae: 2.1479 - val_loss: 13.6953 - val_mae: 2.7716\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.1704 - mae: 2.1703 - val_loss: 13.7493 - val_mae: 2.7766\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.1968 - mae: 2.1661 - val_loss: 13.7985 - val_mae: 2.8064\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.0028 - mae: 2.1255 - val_loss: 13.6226 - val_mae: 2.7521\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.9794 - mae: 2.1063 - val_loss: 13.7531 - val_mae: 2.7939\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.9195 - mae: 2.1011 - val_loss: 13.7947 - val_mae: 2.7919\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.8360 - mae: 2.0848 - val_loss: 13.6731 - val_mae: 2.7488\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.7895 - mae: 2.0883 - val_loss: 13.7339 - val_mae: 2.7448\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.7425 - mae: 2.0746 - val_loss: 13.8301 - val_mae: 2.7734\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.8068 - mae: 2.0620 - val_loss: 13.8540 - val_mae: 2.7839\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.8274 - mae: 2.0921 - val_loss: 14.1779 - val_mae: 2.8413\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.7505 - mae: 2.0958 - val_loss: 14.0765 - val_mae: 2.7698\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.6903 - mae: 2.0956 - val_loss: 14.0876 - val_mae: 2.7814\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5999 - mae: 2.0909 - val_loss: 14.0161 - val_mae: 2.8032\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5702 - mae: 2.0629 - val_loss: 13.9088 - val_mae: 2.8099\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.6979 - mae: 2.0528 - val_loss: 13.8548 - val_mae: 2.8076\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.7018 - mae: 2.0530 - val_loss: 13.6695 - val_mae: 2.7636\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5756 - mae: 2.0333 - val_loss: 13.6751 - val_mae: 2.7665\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5011 - mae: 2.0507 - val_loss: 13.8569 - val_mae: 2.8093\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.5428 - mae: 2.0867 - val_loss: 13.7884 - val_mae: 2.8075\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.4948 - mae: 2.0988 - val_loss: 13.5845 - val_mae: 2.7313\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.4100 - mae: 2.0644 - val_loss: 13.4762 - val_mae: 2.7151\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.4115 - mae: 2.0473 - val_loss: 13.5347 - val_mae: 2.7607\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.3568 - mae: 2.0280 - val_loss: 13.4921 - val_mae: 2.7664\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.3035 - mae: 2.0264 - val_loss: 13.4431 - val_mae: 2.7491\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.2784 - mae: 2.0358 - val_loss: 13.3912 - val_mae: 2.7233\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.2148 - mae: 2.0449 - val_loss: 13.5838 - val_mae: 2.7746\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.3136 - mae: 2.0729 - val_loss: 13.5741 - val_mae: 2.7793\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.2603 - mae: 2.0689 - val_loss: 13.4225 - val_mae: 2.7439\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.2723 - mae: 2.0608 - val_loss: 13.4412 - val_mae: 2.7698\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.1878 - mae: 2.0406 - val_loss: 13.3073 - val_mae: 2.7478\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.1850 - mae: 2.0127 - val_loss: 13.3001 - val_mae: 2.7514\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0867 - mae: 1.9982 - val_loss: 13.4438 - val_mae: 2.7525\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.1109 - mae: 2.0230 - val_loss: 13.4779 - val_mae: 2.7520\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 8.0996 - mae: 2.0152 - val_loss: 13.3196 - val_mae: 2.7355\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.2324 - mae: 2.0042 - val_loss: 13.4811 - val_mae: 2.7851\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.1762 - mae: 1.9921 - val_loss: 13.5781 - val_mae: 2.7998\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.1425 - mae: 2.0161 - val_loss: 13.7976 - val_mae: 2.8036\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.9531 - mae: 2.0157 - val_loss: 13.4156 - val_mae: 2.7430\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.0320 - mae: 1.9971 - val_loss: 13.4025 - val_mae: 2.7531\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.9778 - mae: 1.9795 - val_loss: 13.7255 - val_mae: 2.7417\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.8466 - mae: 2.2225 - val_loss: 16.8925 - val_mae: 3.0011\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.2997 - mae: 2.2593 - val_loss: 14.9746 - val_mae: 2.8410\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0864 - mae: 2.0683 - val_loss: 13.7739 - val_mae: 2.8526\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.3738 - mae: 2.0667 - val_loss: 13.5973 - val_mae: 2.8400\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.0627 - mae: 2.0451 - val_loss: 13.9308 - val_mae: 2.8429\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.6481 - mae: 2.2197 - val_loss: 17.3627 - val_mae: 3.0899\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.8602 - mae: 2.3507 - val_loss: 16.1700 - val_mae: 2.9530\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.4650 - mae: 2.1607 - val_loss: 14.3061 - val_mae: 2.7782\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.8298 - mae: 2.0737 - val_loss: 13.5652 - val_mae: 2.7949\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.9116 - mae: 2.0377 - val_loss: 13.3797 - val_mae: 2.7557\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.0692 - mae: 2.0490 - val_loss: 13.5615 - val_mae: 2.7703\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.7309 - mae: 2.0139 - val_loss: 13.6630 - val_mae: 2.7921\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.6085 - mae: 2.0332 - val_loss: 13.7777 - val_mae: 2.7981\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.5933 - mae: 2.0420 - val_loss: 13.5776 - val_mae: 2.7770\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.4890 - mae: 2.0122 - val_loss: 13.2845 - val_mae: 2.7378\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.3809 - mae: 1.9972 - val_loss: 13.1718 - val_mae: 2.7725\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.4281 - mae: 1.9928 - val_loss: 13.0043 - val_mae: 2.7483\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.3356 - mae: 1.9720 - val_loss: 13.1017 - val_mae: 2.6856\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.4276 - mae: 1.9831 - val_loss: 13.1288 - val_mae: 2.6881\n",
      "Epoch 248/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.3398 - mae: 1.9696 - val_loss: 13.3367 - val_mae: 2.7654\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.4679 - mae: 1.9892 - val_loss: 13.7335 - val_mae: 2.8322\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.4808 - mae: 1.9652 - val_loss: 13.5008 - val_mae: 2.7710\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.4151 - mae: 1.9571 - val_loss: 13.3109 - val_mae: 2.7270\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.3754 - mae: 1.9370 - val_loss: 13.0950 - val_mae: 2.7315\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2469 - mae: 1.9360 - val_loss: 13.1244 - val_mae: 2.7574\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2688 - mae: 1.9737 - val_loss: 13.2766 - val_mae: 2.7797\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2872 - mae: 2.0063 - val_loss: 13.2609 - val_mae: 2.7485\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.1061 - mae: 1.9674 - val_loss: 13.0579 - val_mae: 2.7112\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.1153 - mae: 1.9356 - val_loss: 13.1223 - val_mae: 2.7465\n",
      "Epoch 258/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.1150 - mae: 1.9273 - val_loss: 12.9847 - val_mae: 2.7099\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.0428 - mae: 1.9261 - val_loss: 13.0875 - val_mae: 2.7205\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.9708 - mae: 1.9227 - val_loss: 13.0309 - val_mae: 2.7174\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9608 - mae: 1.9161 - val_loss: 12.8474 - val_mae: 2.6826\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.9492 - mae: 1.8931 - val_loss: 12.6917 - val_mae: 2.6787\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.9418 - mae: 1.9035 - val_loss: 12.9339 - val_mae: 2.7193\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.8552 - mae: 1.9021 - val_loss: 12.8235 - val_mae: 2.7047\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9283 - mae: 1.8799 - val_loss: 12.8632 - val_mae: 2.7128\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.9389 - mae: 1.8712 - val_loss: 12.9142 - val_mae: 2.6879\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8797 - mae: 1.8971 - val_loss: 13.1729 - val_mae: 2.7088\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7833 - mae: 1.9184 - val_loss: 13.1876 - val_mae: 2.7404\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7185 - mae: 1.8904 - val_loss: 12.7975 - val_mae: 2.6885\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8582 - mae: 1.8611 - val_loss: 12.6871 - val_mae: 2.6542\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.8971 - mae: 1.8705 - val_loss: 12.6897 - val_mae: 2.6665\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.6853 - mae: 1.8675 - val_loss: 12.7332 - val_mae: 2.6763\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.6407 - mae: 1.8834 - val_loss: 12.9250 - val_mae: 2.6817\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7928 - mae: 1.9288 - val_loss: 13.2105 - val_mae: 2.6914\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.6937 - mae: 1.9018 - val_loss: 12.8445 - val_mae: 2.6534\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.7372 - mae: 1.8647 - val_loss: 12.6725 - val_mae: 2.6482\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.7549 - mae: 1.8660 - val_loss: 12.7187 - val_mae: 2.6755\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.5627 - mae: 1.8670 - val_loss: 13.0275 - val_mae: 2.6836\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.1130 - mae: 1.9706 - val_loss: 14.2879 - val_mae: 2.7741\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.9332 - mae: 1.8761 - val_loss: 12.8426 - val_mae: 2.6896\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.7844 - mae: 1.8361 - val_loss: 12.9866 - val_mae: 2.7021\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2820 - mae: 1.9505 - val_loss: 16.5252 - val_mae: 2.9717\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 8.2868 - mae: 2.1314 - val_loss: 15.5743 - val_mae: 2.8598\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.2028 - mae: 1.9556 - val_loss: 13.4906 - val_mae: 2.7441\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.0747 - mae: 1.9219 - val_loss: 13.0960 - val_mae: 2.7268\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.8245 - mae: 1.9069 - val_loss: 12.9869 - val_mae: 2.7088\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.4725 - mae: 1.8610 - val_loss: 13.0115 - val_mae: 2.6712\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.4133 - mae: 1.8569 - val_loss: 13.1945 - val_mae: 2.6608\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.4202 - mae: 1.8744 - val_loss: 13.2181 - val_mae: 2.6736\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.3528 - mae: 1.8653 - val_loss: 12.9823 - val_mae: 2.6909\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2547 - mae: 1.8305 - val_loss: 12.6742 - val_mae: 2.6522\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2487 - mae: 1.8199 - val_loss: 12.6101 - val_mae: 2.6363\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.2860 - mae: 1.8169 - val_loss: 12.5308 - val_mae: 2.6319\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.2078 - mae: 1.8123 - val_loss: 12.7405 - val_mae: 2.6833\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.1732 - mae: 1.8304 - val_loss: 12.9133 - val_mae: 2.6809\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2642 - mae: 1.8731 - val_loss: 12.8433 - val_mae: 2.6614\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.6798 - mae: 1.8815 - val_loss: 12.4080 - val_mae: 2.6739\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.5437 - mae: 1.8608 - val_loss: 12.4689 - val_mae: 2.6950\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 6.3680 - mae: 1.8498 - val_loss: 12.3580 - val_mae: 2.6599\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.2564 - mae: 1.8460 - val_loss: 12.5160 - val_mae: 2.6821\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 9.9684 - mae: 2.2820\n"
     ]
    }
   ],
   "source": [
    "# 표준화한 데이터를 분할\n",
    "split_x_train = kfold.split(std_x_train)\n",
    "split_x_train\n",
    "\n",
    "# 분할된 데이터 수만큼 반복 수행\n",
    "\n",
    "# 데이터셋을 평가한 후 결과 mae를 담을 리스트\n",
    "mae_list = []\n",
    "\n",
    "# k번 진행 - 각각 훈련셋 인덱스와 검증셋 인덱스 세트가 달라진다\n",
    "for train_index, val_index in split_x_train:\n",
    "    print(f'훈련셋 인덱스: {train_index}')\n",
    "    print(f'검증셋 인덱스: {val_index}')\n",
    "    \n",
    "    # 해당 인덱스는 무작위로 생성된다.\n",
    "    # 무작위로 생성하는 것이 과대적합을 피할 수 있는 좋은 방법이다.\n",
    "    x_train_fold, x_val_fold = std_x_train[train_index], std_x_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 13차원의 데이터를 입력으로 받고, 64개의 출력을 가지는 첫 번째 Dense층\n",
    "    model.add(Dense(64, activation = 'relu', input_shape = (13, )))\n",
    "\n",
    "    # 32개의 출력을 가지는 Dense층\n",
    "    model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "    # 하나의 값을 출력\n",
    "    # -> 정답의 범위가 정해지지 않기 때문에 활성화 함수는 linear\n",
    "    # -> linear는 기본값이므로 생략 가능함\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "    result = model.fit(x_train_fold, y_train_fold,\n",
    "                      epochs = 300,\n",
    "                      validation_data = (x_val_fold, y_val_fold))\n",
    "\n",
    "    # 모델 평가\n",
    "    tmp, test_mae = model.evaluate(std_x_test, y_test)\n",
    "\n",
    "    # 평가 결과를 list에 추가함\n",
    "    mae_list.append(test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 결과(오차 리스트): [2.161458969116211, 2.219230890274048, 2.2820093631744385]\n",
      "오차들의 평균값을 최종 결과로 사용: 2.220899740854899\n"
     ]
    }
   ],
   "source": [
    "print(f'전체 결과(오차 리스트): {mae_list}')\n",
    "print(f'오차들의 평균값을 최종 결과로 사용: {np.mean(mae_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내용 정리\n",
    "\n",
    "- 회귀는 분류에서 사용했던 것과는 다른 손실 함수를 사용한다.\n",
    "    - 평균 제곱 오차(MSE)는 회귀에서 자주 사용되는 손실 함구\n",
    "- 회귀에서 사용되는 평가 지표도 분류와 다르다.\n",
    "    - 정확도 개념은 회귀에 적용되지 않는다.\n",
    "    - 일반적인 회귀 지표는 평균 절대 오차(MAE)이다.\n",
    "- 입력 데이터의 특성이 서로 다른 범위를 가지면 전처리 단계에서 각 특성을 개별적으로 스케일 조정해야 한다.\n",
    "    - 표준화\n",
    "- 가용한 데이터가 적다면 'K-폴드 검증'을 사용하는 것이 신뢰할 수 있는 모델 평가 방법이다.\n",
    "- 가용한 훈련 데이터가 적다면 과대적합을 피하기 위해 은닉 층의 수를 줄인 모델이 좋다.(일반적으로 1개 또는 2개)\n",
    "    - model.add()를 적게 하라는 뜻"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
